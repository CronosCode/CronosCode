---
title: Probability and Computing
author: Michael Mitzenmacher, Eli Upfal
status: reading
tags: [probability, statistics, theory]
layout: book
---



Probability and Computing

Randomization and probabilistic techniques play an important role in modern computer
science, with applications ranging from combinatorial optimization and machine learning
to communication networks and secure protocols.
This textbook provides an indispensable teaching tool to accompany a one- or two-
semester course for advanced undergraduate or beginning graduate students in computer
science and applied mathematics. It offers a comprehensive introduction to the role of ran-
domization and probabilistic techniques in modern computer science, in particular to tech-
niques and paradigms used in the development and probabilistic analysis of algorithms
and for data analyses. It assumes only an elementary background in discrete mathematics
and gives a rigorous yet accessible treatment of the material, with numerous examples and
applications.
The first half of the book covers core material, including random sampling, expecta-
tions, Markov’s inequality, Chebyshev’s inequality, Chernoff bounds, balls-and-bins mod-
els, the probabilistic method, and Markov chains. In the second half, the authors delve
into more advanced topics such as continuous probability, applications of limited indepen-
dence, entropy, Markov chain Monte Carlo methods, coupling, martingales, and balanced
allocations.
This greatly expanded new edition includes several newly added chapters and sec-
tions, covering topics including normal distributions, sample complexity, VC dimension,
Rademacher complexity, power laws and related distributions, cuckoo hashing, and appli-
cations of the Lovász Local Lemma. New material relevant to machine learning and big
data analysis enables students to learn up-to-date techniques and applications. Among the
many new exercises and examples are programming-related exercises that provide students
with practical experience and training related to the theoretical concepts covered in the text.

Michael Mitzenmacher is a Professor of Computer Science in the School of Engineering
and Applied Sciences at Harvard University, where he was also the Area Dean for Com-
puter Science from 2010 to 2013. Michael has authored or co-authored over 200 confer-
ence and journal publications on a variety of topics, including algorithms for the Internet,
efficient hash-based data structures, erasure and error-correcting codes, power laws, and
compression. His work on low-density parity-check codes shared the 2002 IEEE Informa-
tion Theory Society Best Paper Award and won the 2009 ACM SIGCOMM Test of Time
Award. He is an ACM Fellow, and was elected as the Chair of the ACM Special Interest
Group on Algorithms and Computation Theory in 2015.

Eli Upfal is a Professor of Computer Science at Brown University, where he was also the
department chair from 2002 to 2007. Prior to joining Brown in 1998, he was a researcher and
project manager at the IBM Almaden Research Center, and a Professor of Applied Math-
ematics and Computer Science at the Weizmann Institute of Science. His main research
interests are randomized algorithms, probabilistic analysis of algorithms, and computa-
tional statistics, with applications ranging from combinatorial and stochastic optimization,
computational biology, and computational finance. He is a Fellow of both the IEEE and the
ACM.

Probability and Computing
Randomization and Probabilistic
Techniques in Algorithms and
Data Analysis
Second Edition
Michael Mitzenmacher Eli Upfal
University Printing House, Cambridge CB2 8BS, United Kingdom
One Liberty Plaza, 20th Floor, New York, NY 10006, USA
477 Williamstown Road, Port Melbourne, VIC 3207, Australia
4843/24, 2nd Floor, Ansari Road, Daryaganj, Delhi - 110002, India
79 Anson Road, #06-04/06, Singapore 079906
Cambridge University Press is part of the University of Cambridge.
It furthers the University’s mission by disseminating knowledge in the pursuit of
education, learning, and research at the highest international levels of excellence.
http://www.cambridge.org
Information on this title:www.cambridge.org/
10.1017/
©Michael Mitzenmacher and Eli Upfal 2017
This publication is in copyright. Subject to statutory exception
and to the provisions of relevant collective licensing agreements,
no reproduction of any part may take place without the written
permission of Cambridge University Press.
First published 2017
Printed in the United States of America by Sheridan Books, Inc.
A catalogue record for this publication is available from the British Library.
Library of Congress Cataloging in Publication Data
Names: Mitzenmacher, Michael, 1969– author. | Upfal, Eli, 1954– author.
Title: Probability and computing / Michael Mitzenmacher Eli Upfal.
Description: Second edition. | Cambridge, United Kingdom ;
New York, NY, USA : Cambridge University Press, [2017] |
Includes bibliographical references and index.
Identifiers: LCCN 2016041654 | ISBN 9781107154889
Subjects: LCSH: Algorithms. | Probabilities. | Stochastic analysis.
Classification: LCC QA274.M574 2017 | DDC 518/.1 – dc
LC record available athttps://lccn.loc.gov/
ISBN 978-1-107-15488-9 Hardback
Additional resources for this publication atwww.cambridge.org/Mitzenmacher.
Cambridge University Press has no responsibility for the persistence or accuracy
of URLs for external or third-party Internet Web sites referred to in this publication
and does not guarantee that any content on such Web sites is, or will remain,
accurate or appropriate.

To
Stephanie, Michaela, Jacqueline, and Chloe
M.M.
Liane, Tamara, and Ilan
E.U.
Contents
Preface to the Second Edition page xv

1 Events and Probability Preface to the First Edition xvii
1.1 Application: Verifying Polynomial Identities
1.2 Axioms of Probability
1.3 Application: Verifying Matrix Multiplication
1.4 Application: Naïve Bayesian Classifier
1.5 Application: A Randomized Min-Cut Algorithm
1.6 Exercises
2 Discrete Random Variables and Expectation
2.1 Random Variables and Expectation
2.1.1 Linearity of Expectations
2.1.2 Jensen’s Inequality
2.2 The Bernoulli and Binomial Random Variables
2.3 Conditional Expectation
2.4 The Geometric Distribution
2.4.1 Example: Coupon Collector’s Problem
2.5 Application: The Expected Run-Time of Quicksort
2.6 Exercises
3 Moments and Deviations
3.1 Markov’s Inequality
3.2 Variance and Moments of a Random Variable
3.2.1 Example: Variance of a Binomial Random Variable
3.3 Chebyshev’s Inequality contents
3.3.1 Example: Coupon Collector’s Problem
3.4 Median and Mean
3.5 Application: A Randomized Algorithm for Computing the Median
3.5.1 The Algorithm
3.5.2 Analysis of the Algorithm
3.6 Exercises
4 Chernoff and Hoeffding Bounds
4.1 Moment Generating Functions
4.2 Deriving and Applying Chernoff Bounds
4.2.1 Chernoff Bounds for the Sum of Poisson Trials
4.2.2 Example: Coin Flips
4.2.3 Application: Estimating a Parameter
4.3 Better Bounds for Some Special Cases
4.4 Application: Set Balancing
4.5 The Hoeffding Bound
4.6 ∗ Application: Packet Routing in Sparse Networks
4.6.1 Permutation Routing on the Hypercube
4.6.2 Permutation Routing on the Butterfly
4.7 Exercises
5 Balls, Bins, and Random Graphs
5.1 Example: The Birthday Paradox
5.2 Balls into Bins
5.2.1 The Balls-and-Bins Model
5.2.2 Application: Bucket Sort
5.3 The Poisson Distribution
5.3.1 Limit of the Binomial Distribution
5.4 The Poisson Approximation
5.4.1 ∗ Example: Coupon Collector’s Problem, Revisited
5.5 Application: Hashing
5.5.1 Chain Hashing
5.5.2 Hashing: Bit Strings
5.5.3 Bloom Filters
5.5.4 Breaking Symmetry
5.6 Random Graphs
5.6.1 Random Graph Models
5.6.2 Application: Hamiltonian Cycles in Random Graphs
5.7 Exercises
5.8 An Exploratory Assignment
6 The Probabilistic Method
6.1 The Basic Counting Argument
6.2 The Expectation Argument contents
6.2.1 Application: Finding a Large Cut
6.2.2 Application: Maximum Satisfiability
6.3 Derandomization Using Conditional Expectations
6.4 Sample and Modify
6.4.1 Application: Independent Sets
6.4.2 Application: Graphs with Large Girth
6.5 The Second Moment Method
6.5.1 Application: Threshold Behavior in Random Graphs
6.6 The Conditional Expectation Inequality
6.7 The Lovász Local Lemma
6.7.1 Application: Edge-Disjoint Paths
6.7.2 Application: Satisfiability
6.8 ∗ Explicit Constructions Using the Local Lemma
6.8.1 Application: A Satisfiability Algorithm
6.9 Lovász Local Lemma: The General Case
6.10 ∗The Algorithmic Lovász Local Lemma
6.11 Exercises
7 Markov Chains and Random Walks
7.1 Markov Chains: Definitions and Representations
7.1.1 Application: A Randomized Algorithm for 2-Satisfiability
7.1.2 Application: A Randomized Algorithm for 3-Satisfiability
7.2 Classification of States
7.2.1 Example: The Gambler’s Ruin
7.3 Stationary Distributions
7.3.1 Example: A Simple Queue
7.4 Random Walks on Undirected Graphs
7.4.1 Application: An s – t Connectivity Algorithm
7.5 Parrondo’s Paradox
7.6 Exercises
8 Continuous Distributions and the Poisson Process
8.1 Continuous Random Variables
8.1.1 Probability Distributions inR
8.1.2 Joint Distributions and Conditional Probability
8.2 The Uniform Distribution
8.2.1 Additional Properties of the Uniform Distribution
8.3 The Exponential Distribution
8.3.1 Additional Properties of the Exponential Distribution
8.3.2 ∗ Example: Balls and Bins with Feedback
8.4 The Poisson Process
8.4.1 Interarrival Distribution
8.4.2 Combining and Splitting Poisson Processes contents
8.4.3 Conditional Arrival Time Distribution
8.5 Continuous Time Markov Processes
8.6 Example: Markovian Queues
8.6.1 M / M /1 Queue in Equilibrium
8.6.2 M / M / 1 / K Queue in Equilibrium
8.6.3 The Number of Customers in an M / M /∞Queue
8.7 Exercises
9 The Normal Distribution
9.1 The Normal Distribution
9.1.1 The Standard Normal Distribution
9.1.2 The General Univariate Normal Distribution
9.1.3 The Moment Generating Function
9.2 ∗ Limit of the Binomial Distribution
9.3 The Central Limit Theorem
9.4 ∗ Multivariate Normal Distributions
9.4.1 Properties of the Multivariate Normal Distribution
9.5 Application: Generating Normally Distributed Random Values
9.6 Maximum Likelihood Point Estimates
9.7 Application: EM Algorithm For a Mixture of Gaussians
9.8 Exercises
10 Entropy, Randomness, and Information
- 10.1 The Entropy Function
- 10.2 Entropy and Binomial Coefficients
- 10.3 Entropy: A Measure of Randomness
- 10.4 Compression
- 10.5 ∗Coding: Shannon’s Theorem
- 10.6 Exercises
11 The Monte Carlo Method
- 11.1 The Monte Carlo Method
- 11.2 Application: The DNF Counting Problem
- 11.2.1 The Naïve Approach
- 11.2.2 A Fully Polynomial Randomized Scheme for DNF Counting
- 11.3 From Approximate Sampling to Approximate Counting
- 11.4 The Markov Chain Monte Carlo Method
- 11.4.1 The Metropolis Algorithm
- 11.5 Exercises
- 11.6 An Exploratory Assignment on Minimum Spanning Trees
12 Coupling of Markov Chains contents
- 12.1 Variation Distance and Mixing Time
- 12.2 Coupling
- 12.2.1 Example: Shuffling Cards
- 12.2.2 Example: Random Walks on the Hypercube
- 12.2.3 Example: Independent Sets of Fixed Size
- 12.3 Application: Variation Distance Is Nonincreasing
- 12.4 Geometric Convergence
- Colorings 12.5 Application: Approximately Sampling Proper
- 12.6 Path Coupling
- 12.7 Exercises
13 Martingales
- 13.1 Martingales
- 13.2 Stopping Times
- 13.2.1 Example: A Ballot Theorem
- 13.3 Wald’s Equation
- 13.4 Tail Inequalities for Martingales
- 13.5 Applications of the Azuma–Hoeffding Inequality
- 13.5.1 General Formalization
- 13.5.2 Application: Pattern Matching
- 13.5.3 Application: Balls and Bins
- 13.5.4 Application: Chromatic Number
- 13.6 Exercises
Complexity 14 Sample Complexity, VC Dimension, and Rademacher
14.1 The Learning Setting
14.2 VC Dimension
14.2.1 Additional Examples of VC Dimension
14.2.2 Growth Function
14.2.3 VC dimension component bounds
14.2.4  /theta-nets and /theta-samples
14.3 The /theta-net Theorem
14.4 Application: PAC Learning
14.5 The /theta-sample Theorem
14.5.1 Application: Agnostic Learning
14.5.2 Application: Data Mining
14.6 Rademacher Complexity
14.6.1 Rademacher Complexity and Sample Error
14.6.2 Estimating the Rademacher Complexity contents
14.6.3 Application: Agnostic Learning of a Binary Classification
14.7 Exercises
15 Pairwise Independence and Universal Hash Functions
15.1 Pairwise Independence
15.1.1 Example: A Construction of Pairwise Independent Bits
15.1.2 Application: Derandomizing an Algorithm for Large Cuts
aPrime 15.1.3 Example: Constructing Pairwise Independent Values Modulo
15.2 Chebyshev’s Inequality for Pairwise Independent Variables
15.2.1 Application: Sampling Using Fewer Random Bits
15.3 Universal Families of Hash Functions
15.3.1 Example: A 2-Universal Family of Hash Functions
15.3.2 Example: A Strongly 2-Universal Family of Hash Functions
15.3.3 Application: Perfect Hashing
15.4 Application: Finding Heavy Hitters in Data Streams
15.5 Exercises
16 Power Laws and Related Distributions
16.1 Power Law Distributions: Basic Definitions and Properties
16.2 Power Laws in Language
16.2.1 Zipf’s Law and Other Examples
16.2.2 Languages via Optimization
16.2.3 Monkeys Typing Randomly
16.3 Preferential Attachment
16.3.1 A Formal Version
16.4 Using the Power Law in Algorithm Analysis
16.5 Other Related Distributions
16.5.1 Lognormal Distributions
16.5.2 Power Law with Exponential Cutoff
16.6 Exercises
17 Balanced Allocations and Cuckoo Hashing
17.1 The Power of Two Choices
17.1.1 The Upper Bound
17.2 Two Choices: The Lower Bound
17.3 Applications of the Power of Two Choices
17.3.1 Hashing
17.3.2 Dynamic Resource Allocation
17.4 Cuckoo Hashing
17.5 Extending Cuckoo Hashing
17.5.1 Cuckoo Hashing with Deletions
contents
17.5.2 Handling Failures 453
17.5.3 More Choices and Bigger Bins 454
17.6 Exercises 456
Further Reading 463

Index 464

Note: Asterisks indicate advanced material for this chapter.

xiii
Preface to the Second Edition

In the ten years since the publication of the first edition of this book, probabilistic
methods have become even more central to computer science, rising with the growing
importance of massive data analysis, machine learning, and data mining. Many of the
successful applications of these areas rely on algorithms and heuristics that build on
sophisticated probabilistic and statistical insights. Judicious use of these tools requires
a thorough understanding of the underlying mathematical concepts. Most of the new
material in this second edition focuses on these concepts.
The ability in recent years to create, collect, and store massive data sets, such as
the World Wide Web, social networks, and genome data, lead to new challenges in
modeling and analyzing such structures. A good foundation for models and analysis
comes from understanding some standard distributions. Our new chapter on the nor-
mal distribution (also known as the Gaussian distribution) covers the most common
statistical distribution, as usual with an emphasis on how it is used in settings in com-
puter science, such as for tail bounds. However, an interesting phenomenon is that in
many modern data sets, including social networks and the World Wide Web, we do not
see normal distributions, but instead we see distributions with very different proper-
ties, most notably unusually heavy tails. For example, some pages in the World Wide
Web have an unusually large number of pages that link to them, orders of magnitude
larger than the average. The new chapter on power laws and related distributions covers
specific distributions that are important for modeling and understanding these kinds of
modern data sets.
Machine learning is one of the great successes of computer science in recent years,
providing efficient tools for modeling, understanding, and making predictions based on
large data sets. A question that is often overlooked in practical applications of machine
learning is the accuracy of the predictions, and in particular the relation between accu-
racy and the sample size. A rigorous introduction to approaches to these important
questions is presented in a new chapter on sample complexity, VC dimension, and
Rademacher averages.

xv
preface to the second edition
We have also used the new edition to enhance some of our previous material. For
example, we present some of the recent advances on algorithmic variations of the pow-
erful Lovász local lemma, and we have a new section covering the wonderfully named
and increasingly useful hashing approach known as cuckoo hashing. Finally, in addi-
tion to all of this new material, the new edition includes updates and corrections, and
many new exercises.
We thank the many readers who sent us corrections over the years – unfortunately,
too many to list here!

xvi
Preface to the First Edition

Why Randomness?
Why should computer scientists study and use randomness? Computers appear to
behave far too unpredictably as it is! Adding randomness would seemingly be a dis-
advantage, adding further complications to the already challenging task of efficiently
utilizing computers.
Science has learned in the last century to accept randomness as an essential com-
ponent in modeling and analyzing nature. In physics, for example, Newton’s laws led
people to believe that the universe was a deterministic place; given a big enough calcu-
lator and the appropriate initial conditions, one could determine the location of planets
years from now. The development of quantum theory suggests a rather different view;
the universe still behaves according to laws, but the backbone of these laws is proba-
bilistic. “God does not play dice with the universe” was Einstein’s anecdotal objection
to modern quantum mechanics. Nevertheless, the prevailing theory today for subparti-
cle physics is based on random behavior and statistical laws, and randomness plays a
significant role in almost every other field of science ranging from genetics and evolu-
tion in biology to modeling price fluctuations in a free-market economy.
Computer science is no exception. From the highly theoretical notion of probabilis-
tic theorem proving to the very practical design of PC Ethernet cards, randomness
and probabilistic methods play a key role in modern computer science. The last two
decades have witnessed a tremendous growth in the use of probability theory in comput-
ing. Increasingly more advanced and sophisticated probabilistic techniques have been
developed for use within broader and more challenging computer science applications.
In this book, we study the fundamental ways in which randomness comes to bear on
computer science: randomized algorithms and the probabilistic analysis of algorithms.
Randomized algorithms: Randomized algorithms are algorithms that make random
choices during their execution. In practice, a randomized program would use values
generated by a random number generator to decide the next step at several branches
of its execution. For example, the protocol implemented in an Ethernet card uses ran-
dom numbers to decide when it next tries to access the shared Ethernet communication

xvii
preface to the first edition
medium. The randomness is useful for breaking symmetry, preventing different cards
from repeatedly accessing the medium at the same time. Other commonly used applica-
tions of randomized algorithms include Monte Carlo simulations and primality testing
in cryptography. In these and many other important applications, randomized algo-
rithms are significantly more efficient than the best known deterministic solutions.
Furthermore, in most cases the randomized algorithms are also simpler and easier to
program.
These gains come at a price; the answer may have some probability of being incor-
rect, or the efficiency is guaranteed only with some probability. Although it may seem
unusual to design an algorithm that may be incorrect, if the probability of error is suf-
ficiently small then the improvement in speed or memory requirements may well be
worthwhile.
Probabilistic analysis of algorithms: Complexity theory tries to classify computa-
tion problems according to their computational complexity, in particular distinguishing
between easy and hard problems. For example, complexity theory shows that the Trav-
eling Salesman problem is NP-hard. It is therefore very unlikely that we will ever know
an algorithm that can solve any instance of the Traveling Salesman problem in time that
is subexponential in the number of cities. An embarrassing phenomenon for the clas-
sical worst-case complexity theory is that the problems it classifies as hard to compute
are often easy to solve in practice. Probabilistic analysis gives a theoretical explanation
for this phenomenon. Although these problems may be hard to solve on some set of
pathological inputs, on most inputs (in particular, those that occur in real-life applica-
tions) the problem is actually easy to solve. More precisely, if we think of the input as
being randomly selected according to some probability distribution on the collection of
all possible inputs, we are very likely to obtain a problem instance that is easy to solve,
and instances that are hard to solve appear with relatively small probability. Probabilis-
tic analysis of algorithms is the method of studying how algorithms perform when the
input is taken from a well-defined probabilistic space. As we will see, even NP-hard
problems might have algorithms that are extremely efficient on almost all inputs.

The Book
This textbook is designed to accompany one- or two-semester courses for advanced
undergraduate or beginning graduate students in computer science and applied math-
ematics. The study of randomized and probabilistic techniques in most leading uni-
versities has moved from being the subject of an advanced graduate seminar meant
for theoreticians to being a regular course geared generally to advanced undergraduate
and beginning graduate students. There are a number of excellent advanced, research-
oriented books on this subject, but there is a clear need for an introductory textbook.
We hope that our book satisfies this need.
The textbook has developed from courses on probabilistic methods in computer sci-
ence taught at Brown (CS 155) and Harvard (CS 223) in recent years. The emphasis in
these courses and in this textbook is on the probabilistic techniques and paradigms, not
on particular applications. Each chapter of the book is devoted to one such method or

xviii
preface to the first edition
technique. Techniques are clarified though examples based on analyzing randomized
algorithms or developing probabilistic analysis of algorithms on random inputs. Many
of these examples are derived from problems in networking, reflecting a prominent
trend in the networking field (and the taste of the authors).
The book contains fourteen chapters. We may view the book as being divided into
two parts, where the first part (Chapters 1 – 7 ) comprises what we believe is core mate-
rial. The book assumes only a basic familiarity with probability theory, equivalent to
what is covered in a standard course on discrete mathematics for computer scientists.
Chapters 1 – 3 review this elementary probability theory while introducing some inter-
esting applications. Topics covered include random sampling, expectation, Markov’s
inequality, variance, and Chebyshev’s inequality. If the class has sufficient background
in probability, then these chapters can be taught quickly. We do not suggest skipping
them, however, because they introduce the concepts of randomized algorithms and
probabilistic analysis of algorithms and also contain several examples that are used
throughout the text.
Chapters 4 – 7 cover more advanced topics, including Chernoff bounds, balls-and-
bins models, the probabilistic method, and Markov chains. The material in these chap-
ters is more challenging than in the initial chapters. Sections that are particularly chal-
lenging (and hence that the instructor may want to consider skipping) are marked with
an asterisk. The core material in the first seven chapters may constitute the bulk of a
quarter- or semester-long course, depending on the pace.
The second part of the book (Chapters 8 – 17 ) covers additional advanced material
that can be used either to fill out the basic course as necessary or for a more advanced
second course. These chapters are largely self-contained, so the instructor can choose
the topics best suited to the class. The chapters on continuous probability and entropy
are perhaps the most appropriate for incorporating into the basic course. Our intro-
duction to continuous probability (Chapter 8 ) focuses on uniform and exponential
distributions, including examples from queueing theory. Our examination of entropy
(Chapter 10 ) shows how randomness can be measured and how entropy arises naturally
in the context of randomness extraction, compression, and coding.
Chapters 11 and 12 cover the Monte Carlo method and coupling, respectively; these
chapters are closely related and are best taught together. Chapter 13 , on martingales,
covers important issues on dealing with dependent random variables, a theme that con-
tinues in a different vein in Chapter 15 is the development of pairwise independence
and derandomization. Finally, the chapter on balanced allocations (Chapter 17 ) covers
a topic close to the authors’ hearts and ties in nicely with Chapter 5 concerning analysis
of balls-and-bins problems.
The order of the subjects, especially in the first part of the book, corresponds to
their relative importance in the algorithmic literature. Thus, for example, the study
of Chernoff bounds precedes more fundamental probability concepts such as Markov
chains. However, instructors may choose to teach the chapters in a different order. A
course with more emphasis on general stochastic processes, for example, may teach
Markov chains (Chapter 7 ) immediately after Chapters 1 – 3 , following with the chapter
on balls, bins, and random graphs (Chapter 5 , omitting the Hamiltonian cycle exam-
ple). Chapter 6 on the probabilistic method could then be skipped, following instead

xix
preface to the first edition
with continuous probability and the Poisson process (Chapter 8 ). The material from
Chapter 4 on Chernoff bounds, however, is needed for most of the remaining material.
Most of the exercises in the book are theoretical, but we have included some pro-
gramming exercises – including two more extensive exploratory assignments that
require some programming. We have found that occasional programming exercises are
often helpful in reinforcing the book’s ideas and in adding some variety to the course.
We have decided to restrict the material in this book to methods and techniques based
on rigorous mathematical analysis; with few exceptions, all claims in this book are fol-
lowed by full proofs. Obviously, many extremely useful probabilistic methods do not
fall within this strict category. For example, in the important area of Monte Carlo meth-
ods, most practical solutions are heuristics that have been demonstrated to be effective
and efficient by experimental evaluation rather than by rigorous mathematical analy-
sis. We have taken the view that, in order to best apply and understand the strengths
and weaknesses of heuristic methods, a firm grasp of underlying probability theory and
rigorous techniques – as we present in this book – is necessary. We hope that students
will appreciate this point of view by the end of the course.

Acknowledgments
Our first thanks go to the many probabilists and computer scientists who developed
the beautiful material covered in this book. We chose not to overload the textbook
with numerous references to the original papers. Instead, we provide a reference list
that includes a number of excellent books giving background material as well as more
advanced discussion of the topics covered here.
The book owes a great deal to the comments and feedback of students and teaching
assistants who took the courses CS 155 at Brown and CS 223 at Harvard. In particular
we wish to thank Aris Anagnostopoulos, Eden Hochbaum, Rob Hunter, and Adam
Kirsch, all of whom read and commented on early drafts of the book.
Special thanks to Dick Karp, who used a draft of the book in teaching CS 174 at
Berkeley during fall 2003. His early comments and corrections were most valuable in
improving the manuscript. Peter Bartlett taught CS 174 at Berkeley in spring 2004, also
providing many corrections and useful comments.
We thank our colleagues who carefully read parts of the manuscript, pointed out
many errors, and suggested important improvements in content and presentation: Artur
Czumaj, Alan Frieze, Claire Kenyon, Joe Marks, Salil Vadhan, Eric Vigoda, and the
anonymous reviewers who read the manuscript for the publisher.
We also thank Rajeev Motwani and Prabhakar Raghavan for allowing us to use some
of the exercises in their excellent book Randomized Algorithms.
We are grateful to Lauren Cowles of Cambridge University Press for her editorial
help and advice in preparing and organizing the manuscript.
Writing of this book was supported in part by NSF ITR Grant no. CCR-0121154.

xx
chapter one

Events and Probability

This chapter introduces the notion of randomized algorithms and reviews some basic
concepts of probability theory in the context of analyzing the performance of simple
randomized algorithms for verifying algebraic identities and finding a minimum cut-set
in a graph.

1.1 Application: Verifying Polynomial Identities
Computers can sometimes make mistakes, due for example to incorrect programming
or hardware failure. It would be useful to have simple ways to double-check the results
of computations. For some problems, we can use randomness to efficiently verify the
correctness of an output.
Suppose we have a program that multiplies together monomials. Consider the prob-
lem of verifying the following identity, which might be output by our program:

( x +1)( x −2)( x +3)( x −4)( x +5)( x −6)
?
≡ x^6 − 7 x^3 + 25.
There is an easy way to verify whether the identity is correct: multiply together the
terms on the left-hand side and see if the resulting polynomial matches the right-hand
side. In this example, when we multiply all the constant terms on the left, the result
does not match the constant term on the right, so the identity cannot be valid. More
generally, given two polynomials F ( x ) and G ( x ), we can verify the identity

F ( x )≡? G ( x )
by converting the two polynomials to their canonical forms

(∑ d
i = 0 cix
i ); two polynomi-
als are equivalent if and only if all the coefficients in their canonical forms are equal.
From this point on let us assume that, as in our example, F ( x ) is given as a product
F ( x )=

∏ d
i = 1 ( x − ai ) and G ( x ) is given in its canonical form. Transforming F ( x )to
its canonical form by consecutively multiplying the i th monomial with the product of

events and probability
the first i −1 monomials requires /eta( d^2 ) multiplications of coefficients. We assume in
what follows that each multiplication can be performed in constant time, although if
the products of the coefficients grow large then it could conceivably require more than
constant time to add and multiply numbers together.
So far, we have not said anything particularly interesting. To check whether the
computer program has multiplied monomials together correctly, we have suggested
multiplying the monomials together again to check the result. Our approach for check-
ing the program is to write another program that does essentially the same thing we
expect the first program to do. This is certainly one way to double-check a program:
write a second program that does the same thing, and make sure they agree. There
are at least two problems with this approach, both stemming from the idea that there
should be a difference between checking a given answer and recomputing it. First, if
there is a bug in the program that multiplies monomials, the same bug may occur in
the checking program. (Suppose that the checking program was written by the same
person who wrote the original program!) Second, it stands to reason that we would like
to check the answer in less time than it takes to try to solve the original problem all over
again.
Let us instead utilize randomness to obtain a faster method to verify the identity. We
informally explain the algorithm and then set up the formal mathematical framework
for analyzing the algorithm.
Assume that the maximum degree, or the largest exponent of x ,in F ( x ) and G ( x )is
d. The algorithm chooses an integer r uniformly at random in the range{ 1 ,..., 100 d },
where by “uniformly at random” we mean that all integers are equally likely to be
chosen. The algorithm then computes the values F ( r ) and G ( r ). If F ( r )= G ( r ) the
algorithm decides that the two polynomials are not equivalent, and if F ( r )= G ( r ) the
algorithm decides that the two polynomials are equivalent.
Suppose that in one computation step the algorithm can generate an integer chosen
uniformly at random in the range{ 1 ,..., 100 d }. Computing the values of F ( r ) and
G ( r ) can be done in O ( d ) time, which is faster than computing the canonical form of
F ( r ). The randomized algorithm, however, may give a wrong answer.
How can the algorithm give the wrong answer?
If F ( x )≡ G ( x ), then the algorithm gives the correct answer, since it will find that
F ( r )= G ( r ) for any value of r.
If F ( x )≡ G ( x ) and F ( r )= G ( r ), then the algorithm gives the correct answer since
it has found a case where F ( x ) and G ( x ) disagree. Thus, when the algorithm decides
that the two polynomials are not the same, the answer is always correct.
If F ( x )≡ G ( x ) and F ( r )= G ( r ), the algorithm gives the wrong answer. In other
words, it is possible that the algorithm decides that the two polynomials are the
same when they are not. For this error to occur, r must be a root of the equation
F ( x )− G ( x )=0. The degree of the polynomial F ( x )− G ( x ) is no larger than d and,
by the fundamental theorem of algebra, a polynomial of degree up to d has no more
than d roots. Thus, if F ( x )≡ G ( x ), then there are no more than d values in the
range{ 1 ,..., 100 d }for which F ( r )= G ( r ). Since there are 100 d values in the range
{ 1 ,..., 100 d }, the chance that the algorithm chooses such a value and returns a wrong
answer is no more than 1/100.

1.2 Axioms of Probability
1.2. Axioms of Probability
We turn now to a formal mathematical setting for analyzing the randomized algorithm.
Any probabilistic statement must refer to the underlying probability space.
Definition 1.1: A probability space has three components:
1. a sample space  m , which is the set of all possible outcomes of the random process
modeled by the probability space;
2. a family of sets F representing the allowable events, where each set in F is a subset^1
of the sample space  m ; and
3. a probability function Pr :F→ R satisfying Definition1.2.

An element of mis called a simple or elementary event.
In the randomized algorithm for verifying polynomial identities, the sample space
is the set of integers{ 1 ,..., 100 d }. Each choice of an integer r in this range is a simple
event.
Definition 1.2: A probability function is any function Pr :F→ R that satisfies the
following conditions:
1. for any event E, 0 ≤Pr( E )≤ 1 ;
2. Pr( m)= 1 ; and
3. for any finite or countably infinite sequence of pairwise mutually disjoint events
E 1 , E 2 , E 3 ,...,

Pr
(
⋃
i ≥ 1
Ei
)
=
∑
i ≥ 1
Pr( Ei ).
In most of this book we will use discrete probability spaces. In a discrete probability
space the sample space mis finite or countably infinite, and the familyFof allow-
able events consists of all subsets of m. In a discrete probability space, the probability
function is uniquely defined by the probabilities of the simple events.
Again, in the randomized algorithm for verifying polynomial identities, each choice
of an integer r is a simple event. Since the algorithm chooses the integer uniformly at
random, all simple events have equal probability. The sample space has 100 d simple
events, and the sum of the probabilities of all simple events must be 1. Therefore each
simple event has probability 1/ 100 d.
Because events are sets, we use standard set theory notation to express combinations
of events. We write E 1 ∩ E 2 for the occurrence of both E 1 and E 2 and write E 1 ∪ E 2 for
the occurrence of either E 1 or E 2 (or both). For example, suppose we roll two dice. If
E 1 is the event that the first die is a 1 and E 2 is the event that the second die is a 1, then
E 1 ∩ E 2 denotes the event that both dice are 1 while E 1 ∪ E 2 denotes the event that at
least one of the two dice lands on 1. Similarly, we write E 1 − E 2 for the occurrence
(^1) In a discrete probability spaceF= 2  m. Otherwise, and introductory readers may skip this point, since the events
need to be measurable,Fmust include the empty set and be closed under complement and union and intersection
of countably many sets (aσ-algebra).

events and probability
of an event that is in E 1 but not in E 2. With the same dice example, E 1 − E 2 consists
of the event where the first die is a 1 and the second die is not. We use the notation E ̄
as shorthand for m− E ;for example, if E is the event that we obtain an even number
when rolling a die, then E ̄is the event that we obtain an odd number.
Definition1.2yields the following obvious lemma.

Lemma 1.1: For any two events E 1 and E 2 ,

Pr( E 1 ∪ E 2 )=Pr( E 1 )+Pr( E 2 )−Pr( E 1 ∩ E 2 ).
Proof: From the definition,

Pr( E 1 )=Pr( E 1 −( E 1 ∩ E 2 ))+Pr( E 1 ∩ E 2 ),
Pr( E 2 )=Pr( E 2 −( E 1 ∩ E 2 ))+Pr( E 1 ∩ E 2 ),
Pr( E 1 ∪ E 2 )=Pr( E 1 −( E 1 ∩ E 2 ))+Pr( E 2 −( E 1 ∩ E 2 ))+Pr( E 1 ∩ E 2 ).
The lemma easily follows.  /theta

A consequence of Definition1.2is known as the union bound. Although it is very
simple, it is tremendously useful.

Lemma 1.2: For any finite or countably infinite sequence of events E 1 , E 2 ,... ,

Pr
(
⋃
i ≥ 1
Ei
)
≤
∑
i ≥ 1
Pr( Ei ).
Notice that Lemma1.2differs from the third part of Definition1.2in that Definition
1.2is an equality and requires the events to be pairwise mutually disjoint.
Lemma1.1can be generalized to the following equality, often referred to as the
inclusion–exclusion principle.

Lemma 1.3: Let E 1 ,..., Enbe any n events. Then

Pr
( n
⋃
i = 1
Ei
)
=
∑ n
i = 1
Pr( Ei )−
∑
i < j
Pr( Ei ∩ Ej )+
∑
i < j < k
Pr( Ei ∩ Ej ∩ Ek )
− ···+(−1) n+^1
∑
i 1 < i 2 <···< i n
Pr
( n
⋂
r = 1
Eir
)
+···.
The proof of the inclusion–exclusion principle is left as Exercise1.7.
We showed before that the only case in which the algorithm may fail to give the
correct answer is when the two input polynomials F ( x ) and G ( x ) are not equivalent;
the algorithm then gives an incorrect answer if the random number it chooses is a root
of the polynomial F ( x )− G ( x ). Let E represent the event that the algorithm failed to
give the correct answer. The elements of the set corresponding to E are the roots of
the polynomial F ( x )− G ( x ) that are in the set of integers{ 1 ,..., 100 d }. Since the
polynomial has no more than d roots it follows that the event E includes no more than

1.2 axioms of probability
d simple events, and therefore

Pr(algorithm fails)=Pr( E )≤
d
100 d
=
1
100
.
It may seem unusual to have an algorithm that can return the wrong answer. It may
help to think of the correctness of an algorithm as a goal that we seek to optimize in
conjunction with other goals. In designing an algorithm, we generally seek to minimize
the number of computational steps and the memory required. Sometimes there is a
trade-off; there may be a faster algorithm that uses more memory or a slower algorithm
that uses less memory. The randomized algorithm we have presented gives a trade-off
between correctness and speed. Allowing algorithms that may give an incorrect answer
(but in a systematic way) expands the trade-off space available in designing algorithms.
Rest assured, however, that not all randomized algorithms give incorrect answers, as
we shall see.
For the algorithm just described, the algorithm gives the correct answer 99% of
the time even when the polynomials are not equivalent. Can we improve this prob-
ability? One way is to choose the random number r from a larger range of integers.
If our sample space is the set of integers{ 1 ,..., 1000 d }, then the probability of a
wrong answer is at most 1/1000. At some point, however, the range of values we
can use is limited by the precision available on the machine on which we run the
algorithm.
Another approach is to repeat the algorithm multiple times, using different random
values to test the identity. The property we use here is that the algorithm has a one-sided
error. The algorithm may be wrong only when it outputs that the two polynomials are
equivalent. If any run yields a number r such that F ( r )= G ( r ), then the polynomials are
not equivalent. Thus, if we repeat the algorithm a number of times and find F ( r )= G ( r )
in at least one round of the algorithm, we know that F ( x ) and G ( x ) are not equivalent.
The algorithm outputs that the two polynomials are equivalent only if there is equality
for all runs.
In repeating the algorithm we repeatedly choose a random number in the range
{ 1 ,..., 100 d }. Repeatedly choosing random numbers according to a given distribution
is generally referred to as sampling. In this case, we can repeatedly choose random
numbers in the range{ 1 ,..., 100 d }in two ways: we can sample either with replace-
ment or without replacement. Sampling with replacement means that we do not remem-
ber which numbers we have already tested; each time we run the algorithm, we choose
a number uniformly at random from the range{ 1 ,..., 100 d }regardless of previous
choices, so there is some chance we will choose an r that we have chosen on a previous
run. Sampling without replacement means that, once we have chosen a number r ,we
do not allow the number to be chosen on subsequent runs; the number chosen at a given
iteration is uniform over all previously unselected numbers.
Let us first consider the case where sampling is done with replacement. Assume
that we repeat the algorithm k times, and that the input polynomials are not equiva-
lent. What is the probability that in all k iterations our random sampling from the set
{ 1 ,..., 100 d }yields roots of the polynomial F ( x )− G ( x ), resulting in a wrong output
by the algorithm? If k =1, we know that this probability is at most d / 100 d = 1 /100.

events and probability
If k =2, it seems that the probability that the first iteration finds a root is at most 1/ 100
and the probability that the second iteration finds a root is at most 1/100, so the prob-
ability that both iterations find a root is at most (1/100)^2. Generalizing, for any k , the
probability of choosing roots for k iterations would be at most (1/100) k.
To formalize this, we introduce the notion of independence.

Definition 1.3: Two events E and F are independent if and only if

Pr( E ∩ F )=Pr( E )·Pr( F ).
More generally, events E 1 , E 2 ,..., Ekare mutually independent if and only if, for any
subset I ⊆[1, k ] ,

Pr
(
⋂
i ∈ I
Ei
)
=
∏
i ∈ I
Pr( Ei ).
If our algorithm samples with replacement then in each iteration the algorithm chooses
a random number uniformly at random from the set{ 1 ,..., 100 d }, and thus the choice
in one iteration is independent of the choices in previous iterations. For the case where
the polynomials are not equivalent, let Ei be the event that, on the i th run of the algo-
rithm, we choose a root ri such that F ( ri )− G ( ri )=0. The probability that the algo-
rithm returns the wrong answer is given by

Pr( E 1 ∩ E 2 ∩···∩ Ek ).
Since Pr( Ei )isatmost d / 100 d and since the events E 1 , E 2 ,..., Ek are independent,
the probability that the algorithm gives the wrong answer after k iterations is

Pr( E 1 ∩ E 2 ∩···∩ Ek )=
∏ k
i = 1
Pr( Ei )≤
∏ k
i = 1
d
100 d
=
(
1
100
) k
.
The probability of making an error is therefore at most exponentially small in the num-
ber of trials.
Now let us consider the case where sampling is done without replacement. In this
case the probability of choosing a given number is conditioned on the events of the
previous iterations.

Definition 1.4: The conditional probability that event E occurs given that event F
occurs is

Pr( E | F )=
Pr( E ∩ F )
Pr( F )
.
The conditional probability is well-defined only if Pr( F )> 0.

Intuitively, we are looking for the probability of E ∩ F within the set of events defined
by F. Because F defines our restricted sample space, we normalize the probabilities
by dividing by Pr( F ), so that the sum of the probabilities of all events is 1. When
Pr( F )>0, the definition can also be written in the useful form

Pr( E | F )Pr( F )=Pr( E ∩ F ).
1.2 axioms of probability
Notice that, when E and F are independent and Pr( F )=0, we have
Pr( E | F )=
Pr( E ∩ F )
Pr( F )
=
Pr( E )Pr( F )
Pr( F )
=Pr( E ).
This is a property that conditional probability should have; intuitively, if two events are
independent, then information about one event should not affect the probability of the
second event.
Again assume that we repeat the algorithm k times and that the input polynomials are
not equivalent. What is the probability that in all the k iterations our random sampling
from the set{ 1 ,..., 100 d }yields roots of the polynomial F ( x )− G ( x ), resulting in a
wrong output by the algorithm?
As in the analysis with replacement, we let Ei be the event that the random num-
ber ri chosen in the i th iteration of the algorithm is a root of F ( x )− G ( x ); again, the
probability that the algorithm returns the wrong answer is given by

Pr( E 1 ∩ E 2 ∩···∩ Ek ).
Applying the definition of conditional probability, we obtain

Pr( E 1 ∩ E 2 ∩···∩ Ek )=Pr( Ek | E 1 ∩ E 2 ∩···∩ Ek − 1 )·Pr( E 1 ∩ E 2 ∩···∩ Ek − 1 ),
and repeating this argument gives

Pr( E 1 ∩ E 2 ∩···∩ Ek )
=Pr( E 1 )·Pr( E 2 | E 1 )·Pr( E 3 | E 1 ∩ E 2 )···Pr( Ek | E 1 ∩ E 2 ∩···∩ Ek − 1 ).
Can we bound Pr( Ej | E 1 ∩ E 2 ∩···∩ Ej − 1 )? Recall that there are at most d values
r for which F ( r )− G ( r )=0; if trials 1 through j − 1 < d have found j −1 of them,
then when sampling without replacement there are only d −( j −1) values out of the
100 d −( j −1) remaining choices for which F ( r )− G ( r )=0. Hence

Pr( Ej | E 1 ∩ E 2 ∩···∩ Ej − 1 )≤
d −( j −1)
100 d −( j −1)
,
and the probability that the algorithm gives the wrong answer after k ≤ d iterations is
bounded by

Pr( E 1 ∩ E 2 ∩···∩ Ek )≤
∏ k
j = 1
d −( j −1)
100 d −( j −1)
≤
(
1
100
) k
.
Because ( d −( j −1))/(100 d −( j −1))< d / 100 d when j >1, our bounds on the
probability of making an error are actually slightly better without replacement. You
may also notice that, if we take d +1 samples without replacement and the two poly-
nomials are not equivalent, then we are guaranteed to find an r such that F ( r )− G ( r )=

Thus, in d +1 iterations we are guaranteed to output the correct answer. However,
computing the value of the polynomial at d +1 points takes /eta( d^2 ) time using the stan-
dard approach, which is no faster than finding the canonical form deterministically.
Since sampling without replacement appears to give better bounds on the probability
of error, why would we ever want to consider sampling with replacement? In some
cases, sampling with replacement is significantly easier to analyze, so it may be worth
events and probability
considering for theoretical reasons. In practice, sampling with replacement is often
simpler to code and the effect on the probability of making an error is almost negligible,
making it a desirable alternative.

1.3 Application: Verifying Matrix Multiplication
We now consider another example where randomness can be used to verify an equality
more quickly than the known deterministic algorithms. Suppose we are given three
n × n matrices A , B , and C. For convenience, assume we are working over the integers
modulo 2. We want to verify whether

AB = C.
One way to accomplish this is to multiply A and B and compare the result to C. The sim-
ple matrix multiplication algorithm takes /eta( n^3 ) operations. There exist more sophisti-
cated algorithms that are known to take roughly /eta( n^2.^37 ) operations.
Once again, we use a randomized algorithm that allows for faster verification – at the
expense of possibly returning a wrong answer with small probability. The algorithm is
similar in spirit to our randomized algorithm for checking polynomial identities. The
algorithm chooses a random vector ̄ r =( r 1 , r 2 ,..., rn )∈{ 0 , 1 } n. It then computes AB r ̄
by first computing B r ̄and then A ( B r ̄), and it also computes C r ̄.If A ( B r ̄)= C r ̄, then
AB = C. Otherwise, it returns that AB = C.
The algorithm requires three matrix-vector multiplications, which can be done in
time /eta( n^2 ) in the obvious way. The probability that the algorithm returns that AB = C
when they are actually not equal is bounded by the following theorem.

Theorem 1.4: If AB = C and if ̄ r is chosen uniformly at random from { 0 , 1 } n, then

Pr( AB r ̄= C r ̄)≤
1
2
.
Proof: Before beginning, we point out that the sample space for the vector ̄ r is the set
{ 0 , 1 } n and that the event under consideration is AB r ̄= C r ̄. We also make note of the
following simple but useful lemma.

Lemma 1.5: Choosingr ̄=( r 1 , r 2 ,..., rn )∈{ 0 , 1 } nuniformly at random is equiva-
lent to choosing each riindependently and uniformly from { 0 , 1 }.

Proof: If each ri is chosen independently and uniformly at random, then each of the
2 n possible vectors ̄ r is chosen with probability 2− n , giving the lemma.  /theta

Let D = AB − C =0. Then AB r ̄= C r ̄implies that D r ̄=0. Since D =0 it must have
some nonzero entry; without loss of generality, let that entry be d 11.
For D r ̄=0, it must be the case that
∑ n

j = 1
d 1 jrj = 0
1.3 application: verifying matrix multiplication
or, equivalently,

r 1 =−
∑ n
j = 2 d^1 jrj
d 11
. (1.1)
Now we introduce a helpful idea. Instead of reasoning about the vector ̄ r , suppose
that we choose the rk independently and uniformly at random from{ 0 , 1 }in order,
from rn down to r 1. Lemma1.5says that choosing the rk in this way is equivalent to
choosing a vector ̄ r uniformly at random. Now consider the situation just before r 1 is
chosen. At this point, the right-hand side of Eqn. (1.1) is determined, and there is at
most one choice for r 1 that will make that equality hold. Since there are two choices
for r 1 , the equality holds with probability at most 1/2, and hence the probability that
AB r ̄= C r ̄is at most 1/2. By considering all variables besides r 1 as having been set, we
have reduced the sample space to the set of two values{ 0 , 1 }for r 1 and have changed
the event being considered to whether Eqn. (1.1) holds.  /theta

This idea is called the principleofdeferreddecisions. When there are several random
variables, such as the ri of the vector ̄ r , it often helps to think of some of them as being
set at one point in the algorithm with the rest of them being left random – or deferred –
until some further point in the analysis. Formally, this corresponds to conditioning on
the revealed values; when some of the random variables are revealed, we must condition
on the revealed values for the rest of the analysis. We will see further examples of the
principle of deferred decisions later in the book.
To formalize this argument, we first introduce a simple fact, known as the law of
total probability.

Theorem 1.6 [ Law of Total Probability ] : Let E 1 , E 2 ,..., Enbe mutually disjoint
events in the sample space  m , and let

⋃ n
i = 1 Ei = m. Then
Pr( B )=
∑ n
i = 1
Pr( B ∩ Ei )=
∑ n
i = 1
Pr( B | Ei )Pr( Ei ).
Proof: Since the events Ei ( i = 1 ,..., n ) are disjoint and cover the entire sample space
 m, it follows that

Pr( B )=
∑ n
i = 1
Pr( B ∩ Ei ).
Further,

∑ n
i = 1
Pr( B ∩ Ei )=
∑ n
i = 1
Pr( B | Ei )Pr( Ei )
by the definition of conditional probability.  /theta

events and probability
Now, using this law and summing over all collections of values ( x 2 , x 3 , x 4 ,..., xn )∈
{ 0 , 1 } n −^1 yields

Pr( AB r ̄= C r ̄)
=
∑
( x 2 ,..., xn )∈{ 0 , 1 } n −^1
Pr
(
( AB r ̄= C r ̄)∩(( r 2 ,..., rn )=( x 2 ,..., xn ))
)
≤
∑
( x 2 ,..., xn )∈{ 0 , 1 } n −^1
Pr
((
r 1 =−
∑ n
j = 2 d^1 jrj
d 11
)
∩(( r 2 ,..., rn )=( x 2 ,..., xn ))
)
=
∑
( x 2 ,..., xn )∈{ 0 , 1 } n −^1
Pr
(
r 1 =−
∑ n
j = 2 d^1 jrj
d 11
)
·Pr(( r 2 ,..., rn )=( x 2 ,..., xn ))
≤
∑
( x 2 ,..., xn )∈{ 0 , 1 } n −^1
1
2
Pr(( r 2 ,..., rn )=( x 2 ,..., xn ))
=
1
2
.
Here we have used the independence of r 1 and ( r 2 ,..., rn ) in the fourth line.  /theta

To improve on the error probability of Theorem1.4, we can again use the fact that
the algorithm has a one-sided error and run the algorithm multiple times. If we ever
find an ̄ r such that AB r ̄= C r ̄, then the algorithm will correctly return that AB = C .If
we always find AB r ̄= C ̄ r , then the algorithm returns that AB = C and there is some
probability of a mistake. Choosing ̄ r with replacement from{ 0 , 1 } n for each trial, we
obtain that, after k trials, the probability of error is at most 2− k. Repeated trials increase
the running time to /eta( kn^2 ).
Suppose we attempt this verification 100 times. The running time of the random-
ized checking algorithm is still /eta( n^2 ), which is faster than the known deterministic
algorithms for matrix multiplication for sufficiently large n. The probability that an
incorrect algorithm passes the verification test 100 times is at most 2−^100 , an astronom-
ically small number. In practice, the computer is much more likely to crash during the
execution of the algorithm than to return a wrong answer.
An interesting related problem is to evaluate the gradual change in our confidence in
the correctness of the matrix multiplication as we repeat the randomized test. Toward
that end we introduce Bayes’ law.

Theorem 1.7 [ Bayes’ Law ] : Assume that E 1 , E 2 ,..., Enare mutually disjoint events
in the sample space  m such that

⋃ n
i = 1 Ei = m. Then
Pr( Ej | B )=
Pr( Ej ∩ B )
Pr( B )
=
Pr( B | Ej )Pr( Ej )
∑ n
i = 1 Pr( B | Ei )Pr( Ei )
.
As a simple application of Bayes’ law, consider the following problem. We are given
three coins and are told that two of the coins are fair and the third coin is biased, landing
heads with probability 2/3. We are not told which of the three coins is biased. We
permute the coins randomly, and then flip each of the coins. The first and second coins
come up heads, and the third comes up tails. What is the probability that the first coin
is the biased one?

1.3 application: verifying matrix multiplication
The coins are in a random order and so, before our observing the outcomes of the
coin flips, each of the three coins is equally likely to be the biased one. Let Ei be the
event that the i th coin flipped is the biased one, and let B be the event that the three coin
flips came up heads, heads, and tails.
Before we flip the coins we have Pr( Ei )= 1 /3 for all i. We can also compute the
probability of the event B conditioned on Ei :

Pr( B | E 1 )=Pr( B | E 2 )=
2
3
·
1
2
·
1
2
=
1
6
,
and

Pr( B | E 3 )=
1
2
·
1
2
·
1
3
=
1
12
.
Applying Bayes’ law, we have

Pr( E 1 | B )=
Pr( B | E 1 )Pr( E 1 )
∑ 3
i = 1 Pr( B | Ei )Pr( Ei )
=
2
5
.
Thus, the outcome of the three coin flips increases the likelihood that the first coin is
the biased one from 1/3to2/5.
Returning now to our randomized matrix multiplication test, we want to evaluate
the increase in confidence in the matrix identity obtained through repeated tests. In the
Bayesian approach one starts with a prior model, giving some initial value to the model
parameters. This model is then modified, by incorporating new observations, to obtain
a posterior model that captures the new information.
In the matrix multiplication case, if we have no information about the process that
generated the identity then a reasonable prior assumption is that the identity is correct
with probability 1/2. If we run the randomized test once and it returns that the matrix
identity is correct, how does this change our confidence in the identity?
Let E be the event that the identity is correct, and let B be the event that the test
returns that the identity is correct. We start with Pr( E )=Pr( E ̄)= 1 /2, and since the
test has a one-sided error bounded by 1/2, we have Pr( B | E )=1 and Pr( B | E ̄)≤ 1 /2.
Applying Bayes’ law yields

Pr( E | B )=
Pr( B | E )Pr( E )
Pr( B | E )Pr( E )+Pr( B | E ̄)Pr( E ̄)
≥
1 / 2
1 / 2 + 1 / 2 · 1 / 2
=
2
3
.
Assume now that we run the randomized test again and it again returns that the
identity is correct. After the first test, I may naturally have revised my prior model, so
that I believe Pr( E )≥ 2 /3 and Pr( E ̄)≤ 1 /3. Now let B be the event that the new test
returns that the identity is correct; since the tests are independent, as before we have
Pr( B | E )=1 and Pr( B | E ̄)≤ 1 /2. Applying Bayes’ law then yields

Pr( E | B )≥
2 / 3
2 / 3 + 1 / 3 · 1 / 2
=
4
5
.
events and probability
In general: if our prior model (before running the test) is that Pr( E )≥ 2 i /(2 i +1)
and if the test returns that the identity is correct (event B ), then

Pr( E | B )≥
2 i
2 i + 1
2 i
2 i + 1
+
1
2
1
2 i + 1
=
2 i +^1
2 i +^1 + 1
= 1 −
1
2 i +^1 + 1
Thus, if all 100 calls to the matrix identity test return that the identity is correct, our
confidence in the correctness of this identity is at least 1− 1 /(2^101 +1).

1.4 Application: Naïve Bayesian Classifier
A naïve Bayesian classifier is a supervised learning algorithm that classifies objects by
estimating conditional probabilities using Bayes’ law in a simplified (“naïve”) prob-
abilistic model. While the independence assumptions that would justify the approach
are significant oversimplifications, this method proves very effective in many practical
applications such as subject classification of text documents and junk e-mail filtering. It
also provides an example of a deterministic algorithm that is based on the probabilistic
concept of conditional probability.
Assume that we are given a collection of n training examples

{( D 1 , c ( D 1 )),( D 2 , c ( D 2 )),...,( Dn , c ( Dn ))},
where each Di is represented as a features vector xi =( xi 1 ,..., xim ). Here Di is an object,
such as a text document, and an object has features ( X 1 , X 2 ,..., Xm ), where feature Xj
can take a value from a set of possibilities Fj .By xi =( xi 1 ,..., xim ) we mean that for
Di we have X 1 = xi 1 ,..., Xm = xim. For example, if Di is a text document, and we have
a list of important keywords, the Xj could be Boolean features where xij =1 if the

j th listed keyword appears in Di and xij =0 otherwise. In this case, the feature vector
of the document would just correspond to the set of keywords it contains. Finally, we
have a set C ={ c 1 , c 2 ,..., ct }of possible classifications for the object, and c ( Di ) is the
classification of Di. For example, the classification set C could be a collection of labels
such as{“spam”, “no-spam”}. Given a document, corresponding to a web page or e-
mail, we might want to classify the document according to the keywords the document
contains.
The classification paradigm assumes that the training set is a sample from an
unknown distribution in which the classification of an object is a function of the m
features. The goal is, given a new document, to return an accurate classification. More
generally, we can instead return a vector ( z 1 , z 2 ,..., zt ), where zj is an estimate of the
probability that c ( Di )= cj based on the training set. If we want to return just the most
likely classification, we can return the cj with the highest value of zj.
Suppose to begin that we had a very, very large training set. Then for each vec-
tor y =( y 1 ,..., ym ) and each classification cj , we could use the training set to com-
pute the empirical conditional probability that an object with a features vector y is

1.4application: naïve bayesian classifier
classified Cj :

py , j =
{| i : xi = y , c ( Di )= cj |}
{| i : xi = y )|}
.
Assuming that a new object D ∗with a features vector x ∗has the same distribution as
the training set, then px ∗, j is an empirical estimate for the conditional probability

Pr( c ( D ∗)= cj | x ∗=( x ∗ 1 ,..., x ∗ m )).
Indeed, we could compute these values ahead of time in a large lookup table and simply
return the vector ( z 1 , z 2 ,..., zt )=( px ∗, 1 , px ∗, 2 ,..., px ∗, t ) after computing the features
vector x ∗from the object.
The difficulty in this approach is that we need to obtain accurate estimates of a large
collection of conditional probabilities, corresponding to all possible combination of
values of the m features. Even if each feature has just two values we would need to esti-
mate 2 m conditional probabilities per class, which would generally require m(| C | 2 m )
samples.
The training process is faster and requires significantly fewer examples if we assume
a “naïve” model in which the m features are independent. In that case we have for

Pr( c ( D ∗)= cj | x ∗)=
Pr( x ∗| c ( D ∗)= cj )·Pr( c ( D ∗)= cj )
Pr( x ∗)
(1.2)
=
∏ m
k = 1 Pr( x
∗
k = xi | c ( D
∗)= cj )·Pr( c ( D ∗)= cj )
Pr( x ∗)
. (1.3)
Here x ∗ k is the k th component of the features vector x ∗of object D ∗. Notice that the
denominator is independent of cj , and can be treated as just a normalizing constant
factor.
With a constant number of possible values per feature, we only need to learn esti-
mates for O ( m | C |) probabilities. In what follows, we usePr to denoteˆ empirical prob-
abilities , which are the relative frequency of events in our training set of examples. This
notation emphasizes that we are taking estimates of these probabilities as determined
from the training set. (In practice, one often makes slight modifications, such as adding
1 /2 to the numerator in each of the fractions to guarantee that no empirical probability
equals 0.)
The training process is simple:
 /thetaFor each classification class cj , keep track of the fraction of objects classified as cj
to compute

Pr(ˆ c ( D ∗)= cj )=|{ i | c ( Di )= cj }|
| D |
,
where| D |is the number of objects in the training set.
 /thetaFor each feature Xk and feature value xk keep track of the fraction of objects with that
feature value that are classified as cj , to compute
Pr(ˆ x ∗ k = xk | c ( D ∗)= cj )=|{ i : x
i
k = xk , c ( Di )= cj }|
{ i | c ( Di )= cj }|
.
events and probability
Naïve Bayes Classifier Algorithm
Input: Set of possible classifications C , set of features and feature values
F 1 ,..., Fm , and a training set of classified itemsD.
Training Phase:
1. For each category c ∈ C , feature k = 1 ,..., m , and feature value xk ∈ Fk com-
pute

Pr(ˆ x ∗ k = xk | c ( D ∗)= c )=|{ i : x
ik = xk , c ( Di )= c }|
{ i | c ( Di )= c }|
.
2. For each category c ∈ C , compute

Pr(ˆ c ( D ∗)= c )=|{ i | c ( Di )= c }|
| D |
.
Classifying a new item D ∗ :
1. To compute the most likely classification for x ∗= x =( x 1 ,..., xm )

c ( D ∗)=arg max
cj ∈ C
( m
∏
k = 1
Pr(ˆ x ∗ k = xk | c ( D ∗)= cj )
)
Pr(ˆ c ( D ∗)= cj ).
2. To compute a classification distribution:

Pr(ˆ c ( D ∗)= cj )=
(∏ m
k = 1 Pr(ˆ x
∗
k = xk | c ( D
∗)= cj ))Pr(ˆ c ( D ∗)= cj )
Pr(ˆ x ∗= x ).
Algorithm 1.1: Naïve Bayes Classifier.
Once we train the classifier, the classification of a new object D ∗with features vector
x ∗=( x ∗ 1 ,..., x ∗ m ) is computed by calculating
( m
∏

k = 1
Pr(ˆ x ∗ k = xk | c ( D ∗)= cj )
)
Pr(ˆ c ( D ∗)= cj )
for each cj and taking the classification with the highest value.
In practice, the products may lead to underflow values; an easy solution to that prob-
lem is to instead compute the logarithm of the above expression. Estimates of the
entire probability vector can be found by normalizing appropriately. (Alternatively,
instead of normalizing, one could provide probability estimates by also computing
estimates for Pr( x ∗= x ) from the sample data. Under our independence assumption
Pr( x ∗=( x ∗ 1 ,..., x ∗ m ))=

∏ m
k = 1 Pr( x
∗
k = xk ), and one could estimate the denominator
of Equation1.2with the product of the corresponding estimates.)
The naïve Bayesian classifier is efficient and simple to implement due to the “naïve”
assumption of independence. This assumption may lead to misleading outcomes when
the classification depends on combinations of features. As a simple example consider

1.5 Application: A Randomized Min-Cut Algorithm
a collection of items characterized by two Boolean features X and Y .If X = Y the item
is in class A , and otherwise it is in class B. Assume further that for each value of X
and Y the training set has an equal number of items in each class. All the conditional
probabilities computed by the classifier equal 0.5, and therefore the classifier is not
better than a coin flip in this example. In practice such phenomena are rare and the
naïve Bayesian classifier is often very effective.

1.5. Application: A Randomized Min-Cut Algorithm
A cut-set in a graph is a set of edges whose removal breaks the graph into two or
more connected components. Given a graph G =( V , E ) with n vertices, the minimum
cut – or min-cut – problem is to find a minimum cardinality cut-set in G. Minimum
cut problems arise in many contexts, including the study of network reliability. In the
case where nodes correspond to machines in the network and edges correspond to con-
nections between machines, the min-cut is the smallest number of edges that can fail
before some pair of machines cannot communicate. Minimum cuts also arise in clus-
tering problems. For example, if nodes represent Web pages (or any documents in a
hypertext-based system) and two nodes have an edge between them if the correspond-
ing nodes have a hyperlink between them, then small cuts divide the graph into clus-
ters of documents with few links between clusters. Documents in different clusters are
likely to be unrelated.
We shall proceed by making use of the definitions and techniques presented so far
in order to analyze a simple randomized algorithm for the min-cut problem. The main
operation in the algorithm is edge contraction. In contracting an edge ( u ,v)wemerge
the two vertices u andvinto one vertex, eliminate all edges connecting u andv, and
retain all other edges in the graph. The new graph may have parallel edges but no
self-loops. Examples appear in Figure1.1, where in each step the dark edge is being
contracted.
The algorithm consists of n −2 iterations. In each iteration, the algorithm picks an
edge from the existing edges in the graph and contracts that edge. There are many pos-
sible ways one could choose the edge at each step. Our randomized algorithm chooses
the edge uniformly at random from the remaining edges.
Each iteration reduces the number of vertices in the graph by one. After n −2 iter-
ations, the graph consists of two vertices. The algorithm outputs the set of edges con-
necting the two remaining vertices.
It is easy to verify that any cut-set of a graph in an intermediate iteration of the
algorithm is also a cut-set of the original graph. On the other hand, not every cut-set of
the original graph is a cut-set of a graph in an intermediate iteration, since some edges
of the cut-set may have been contracted in previous iterations. As a result, the output of
the algorithm is always a cut-set of the original graph but not necessarily the minimum
cardinality cut-set (see Figure1.1).
We now establish a lower bound on the probability that the algorithm returns a cor-
rect output.

events and probability
1
4
5
2

(^31)
3,4
5
2
1,3,4
5
2
1,2,3,4
5
(a) A successful run of min-cut.
1
4
5
2
3 1
3,4
5
2
1
3,4,5
2
1
2,3,4,5
(b) An unsuccessful run of min-cut.
Figure 1.1: An example of two executions of min-cut in a graph with minimum cut-set of size 2.
Theorem 1.8: The algorithm outputs a min-cut set with probability at least
2 /( n ( n −1)).
Proof: Let k be the size of the min-cut set of G. The graph may have several cut-sets
of minimum size. We compute the probability of finding one specific such set C.
Since C is a cut-set in the graph, removal of the set C partitions the set of vertices into
two sets, S and V − S , such that there are no edges connecting vertices in S to vertices in
V − S. Assume that, throughout an execution of the algorithm, we contract only edges
that connect two vertices in S or two vertices in V − S , but not edges in C. In that case,
all the edges eliminated throughout the execution will be edges connecting vertices in
S or vertices in V − S , and after n −2 iterations the algorithm returns a graph with two
vertices connected by the edges in C. We may therefore conclude that, if the algorithm
never chooses an edge of C in its n −2 iterations, then the algorithm returns C as the
minimum cut-set.
This argument gives some intuition for why we choose the edge at each iteration
uniformly at random from the remaining existing edges. If the size of the cut C is
small and if the algorithm chooses the edge uniformly at each step, then the probability
that the algorithm chooses an edge of C is small – at least when the number of edges
remaining is large compared to C.
⋂Let Ei be the event that the edge contracted in iteration i is not in C , and let Fi =
i
j = 1 Ej be the event that no edge of C was contracted in the first i iterations. We need
to compute Pr( Fn − 2 ).
We start by computing Pr( E 1 )=Pr( F 1 ). Since the minimum cut-set has k edges, all
vertices in the graph must have degree k or larger. If each vertex is adjacent to at least k
edges, then the graph must have at least nk /2 edges. The first contracted edge is chosen
uniformly at random from the set of all edges. Since there are at least nk /2 edges in the
graph and since C has k edges, the probability that we do not choose an edge of C in
the first iteration is given by
Pr( E 1 )=Pr( F 1 )≥ 1 −
2 k
nk

= 1 −
2
n
.
1.6 Exercises
Let us suppose that the first contraction did not eliminate an edge of C. In other
words, we condition on the event F 1. Then, after the first iteration, we are left with an
( n −1)-node graph with minimum cut-set of size k. Again, the degree of each vertex
in the graph must be at least k , and the graph must have at least k ( n −1)/2 edges.
Thus,

Pr( E 2 | F 1 )≥ 1 −
k
k ( n −1)/ 2
= 1 −
2
n − 1
.
Similarly,

Pr( Ei | Fi − 1 )≥ 1 −
k
k ( n − i +1)/ 2
= 1 −
2
n − i + 1
.
To compute Pr( Fn − 2 ), we use
Pr( Fn − 2 )=Pr( En − 2 ∩ Fn − 3 )=Pr( En − 2 | Fn − 3 )·Pr( Fn − 3 )
=Pr( En − 2 | Fn − 3 )·Pr( En − 3 | Fn − 4 )···Pr( E 2 | F 1 )·Pr( F 1 )
≥
n ∏− 2
i = 1
(
1 −
2
n − i + 1
)
=
n ∏− 2
i = 1
(
n − i − 1
n − i + 1
)
=
(
n − 2
n
)(
n − 3
n − 1
)(
n − 4
n − 2
)
...
(
4
6
)(
3
5
)(
2
4
)(
1
3
)
=
2
n ( n −1)
.  /theta
Since the algorithm has a one-sided error, we can reduce the error probability by repeat-
ing the algorithm. Assume that we run the randomized min-cut algorithm n ( n −1)ln n
times and output the minimum size cut-set found in all the iterations. The probability
that the output is not a min-cut set is bounded by
(
1 −

2
n ( n −1)
) n ( n −1)ln n
≤e−2ln n =
1
n^2
.
In the first inequality we have used the fact that 1− x ≤e− x.

1.6. Exercises
Exercise 1.1: We flip a fair coin ten times. Find the probability of the following events.

(a) The number of heads and the number of tails are equal.
(b) There are more heads than tails.
(c) The i th flip and the (11− i )th flip are the same for i = 1 ,...,5.
(d) We flip at least four consecutive heads.

Exercise 1.2: We roll two standard six-sided dice. Find the probability of the following
events, assuming that the outcomes of the rolls are independent.

(a) The two dice show the same number.
(b) The number that appears on the first die is larger than the number on the second.

events and probability
(c) The sum of the dice is even.
(d) The product of the dice is a perfect square.

Exercise 1.3: We shuffle a standard deck of cards, obtaining a permutation that
is uniform over all 52! possible permutations. Find the probability of the following
events.

(a) The first two cards include at least one ace.
(b) The first five cards include at least one ace.
(c) The first two cards are a pair of the same rank.
(d) The first five cards are all diamonds.
(e) The first five cards form a full house (three of one rank and two of another rank).

Exercise 1.4: We are playing a tournament in which we stop as soon as one of us wins
n games. We are evenly matched, so each of us wins any game with probability 1/2,
independently of other games. What is the probability that the loser has won k games
when the match is over?

Exercise 1.5: After lunch one day, Alice suggests to Bob the following method to
determine who pays. Alice pulls three six-sided dice from her pocket. These dice are
not the standard dice, but have the following numbers on their faces:

 /thetadie A – 1, 1 , 6 , 6 , 8 , 8 ;
 /thetadie B – 2, 2 , 4 , 4 , 9 , 9 ;
 /thetadie C – 3, 3 , 5 , 5 , 7 ,7.
The dice are fair, so each side comes up with equal probability. Alice explains that she
and Bob will each pick up one of the dice. They will each roll their die, and the one
who rolls the lowest number loses and will buy lunch. So as to take no advantage, Alice
offers Bob the first choice of the dice.

(a) Suppose that Bob chooses die A and Alice chooses die B. Write out all of the
possible events and their probabilities, and show that the probability that Alice
wins is greater than 1/2.
(b) Suppose that Bob chooses die B and Alice chooses die C. Write out all of the
possible events and their probabilities, and show that the probability that Alice
wins is greater than 1/2.
(c) Since die A and die B lead to situations in Alice’s favor, it would seem that Bob
should choose die C. Suppose that Bob does choose die C and Alice chooses die
A. Write out all of the possible events and their probabilities, and show that the
probability that Alice wins is still greater than 1/2.

Exercise 1.6: Consider the following balls-and-bin game. We start with one black ball
and one white ball in a bin. We repeatedly do the following: choose one ball from the
bin uniformly at random, and then put the ball back in the bin with another ball of the
same color. We repeat until there are n balls in the bin. Show that the number of white
balls is equally likely to be any number between 1 and n −1.

1.6 exercises
Exercise 1.7: (a) Prove Lemma 3, the inclusion–exclusion principle.

(b) Prove that, when nis odd,

Pr
( n
⋃
i = 1
Ei
)
≤
∑ n
i = 1
Pr( Ei )−
∑
i < j
Pr( Ei ∩ Ej )+
∑
i < j < k
Pr( Ei ∩ Ej ∩ Ek )
− ···+(−1) n+^1
∑
i 1 < i 2 <···< i n
Pr( Ei 1 ∩···∩ Ei n).
(c) Prove that, when nis even,
Pr
( n
⋃
i = 1
Ei
)
≥
∑ n
i = 1
Pr( Ei )−
∑
i < j
Pr( Ei ∩ Ej )+
∑
i < j < k
Pr( Ei ∩ Ej ∩ Ek )
− ···+(−1) n+^1
∑
i 1 < i 2 <···< i n
Pr( Ei 1 ∩···∩ Ei n).
Exercise 1.8: I choose a number uniformly at random from the range [1, 1 , 000 ,000].
Using the inclusion–exclusion principle, determine the probability that the number cho-
sen is divisible by one or more of 4, 6, and 9.

Exercise 1.9: Suppose that a fair coin is flipped n times. For k >0, find an upper
bound on the probability that there is a sequence of log 2 n + k consecutive heads.

Exercise 1.10: I have a fair coin and a two-headed coin. I choose one of the two coins
randomly with equal probability and flip it. Given that the flip was heads, what is the
probability that I flipped the two-headed coin?

Exercise 1.11: I am trying to send you a single bit, either a 0 or a 1. When I transmit
the bit, it goes through a series of n relays before it arrives to you. Each relay flips the
bit independently with probability p.

(a) Argue that the probability you receive the correct bit is

(^) ∑ n / 2 
k = 0
( n
2 k

)
p^2 k (1− p ) n −^2 k.
(b) We consider an alternative way to calculate this probability. Let us say the relay
has bias q if the probability it flips the bit is (1− q )/2. The bias q is therefore a
real number in the range [− 1 ,1]. Prove that sending a bit through two relays with
bias q 1 and q 2 is equivalent to sending a bit through a single relay with bias q 1 q 2.
(c) Prove that the probability you receive the correct bit when it passes through n relays
as described before (a) is
1 +(1− 2 p ) n
2

.
Exercise 1.12: The following problem is known as the Monty Hall problem, after
the host of the game show “Let’s Make a Deal”. There are three curtains. Behind one
curtain is a new car, and behind the other two are goats. The game is played as follows.

events and probability
The contestant chooses the curtain that she thinks the car is behind. Monty then opens
one of the other curtains to show a goat. (Monty may have more than one goat to choose
from; in this case, assume he chooses which goat to show uniformly at random.) The
contestant can then stay with the curtain she originally chose or switch to the other
unopened curtain. After that, the location of the car is revealed, and the contestant wins
the car or the remaining goat. Should the contestant switch curtains or not, or does it
make no difference?

Exercise 1.13: A medical company touts its new test for a certain genetic disorder.
The false negative rate is small: if you have the disorder, the probability that the test
returns a positive result is 0.999. The false positive rate is also small: if you do not
have the disorder, the probability that the test returns a positive result is only 0.005.
Assume that 2% of the population has the disorder. If a person chosen uniformly from
the population is tested and the result comes back positive, what is the probability that
the person has the disorder?

Exercise 1.14: I am playing in a racquetball tournament, and I am up against a player
I have watched but never played before. I consider three possibilities for my prior
model: we are equally talented, and each of us is equally likely to win each game;
I am slightly better, and therefore I win each game independently with probability 0.6;
or he is slightly better, and thus he wins each game independently with probability 0.6.
Before we play, I think that each of these three possibilities is equally likely.
In our match we play until one player wins three games. I win the second game, but
he wins the first, third, and fourth. After this match, in my posterior model, with what
probability should I believe that my opponent is slightly better than I am?

Exercise 1.15: Suppose that we roll ten standard six-sided dice. What is the probabil-
ity that their sum will be divisible by 6, assuming that the rolls are independent? ( Hint:
Use the principle of deferred decisions, and consider the situation after rolling all but
one of the dice.)

Exercise 1.16: Consider the following game, played with three standard six-sided
dice. If the player ends with all three dice showing the same number, she wins. The
player starts by rolling all three dice. After this first roll, the player can select any one,
two, or all of the three dice and re-roll them. After this second roll, the player can
again select any of the three dice and re-roll them one final time. For questions (a)–(d),
assume that the player uses the following optimal strategy: if all three dice match, the
player stops and wins; if two dice match, the player re-rolls the die that does not match;
and if no dice match, the player re-rolls them all.

(a) Find the probability that all three dice show the same number on the first roll.
(b) Find the probability that exactly two of the three dice show the same number on
the first roll.
(c) Find the probability that the player wins, conditioned on exactly two of the three
dice showing the same number on the first roll.

1.6 exercises
(d) By considering all possible sequences of rolls, find the probability that the player
wins the game.

Exercise 1.17: In our matrix multiplication algorithm, we worked over the integers
modulo 2. Explain how the analysis would change if we worked over the integers mod-
ulo k for k >2.

Exercise 1.18: We have a function F :{ 0 ,..., n − 1 }→{ 0 ,..., m − 1 }.We
know that, for 0≤ x , y ≤ n −1, F (( x + y )mod n )=( F ( x )+ F ( y ))mod m. The only
way we have for evaluating F is to use a lookup table that stores the values of F. Unfor-
tunately, an Evil Adversary has changed the value of 1/5 of the table entries when we
were not looking.
Describe a simple randomized algorithm that, given an input z , outputs a value that
equals F ( z ) with probability at least 1/2. Your algorithm should work for every value
of z , regardless of what values the Adversary changed. Your algorithm should use as
few lookups and as little computation as possible.
Suppose I allow you to repeat your initial algorithm three times. What should you
do in this case, and what is the probability that your enhanced algorithm returns the
correct answer?

Exercise 1.19: Give examples of events where Pr( A | B )<Pr( A ), Pr( A | B )=Pr( A ),
and Pr( A | B )>Pr( A ).

Exercise 1.20: Show that, if E 1 , E 2 ,..., En are mutually independent, then so are
E ̄ 1 , E ̄ 2 ,..., E ̄ n.

Exercise 1.21: Give an example of three random events X , Y , Z for which any pair are
independent but all three are not mutually independent.

Exercise 1.22: (a) Consider the set{ 1 ,..., n }. We generate a subset X of this set as
follows: a fair coin is flipped independently for each element of the set; if the coin lands
heads then the element is added to X , and otherwise it is not. Argue that the resulting
set X is equally likely to be any one of the 2 n possible subsets.
(b) Suppose that two sets X and Y are chosen independently and uniformly at
random from all the 2 n subsets of{ 1 ,..., n }. Determine Pr( X ⊆ Y ) and Pr( X ∪ Y =
{ 1 ,..., n }). ( Hint: Use the part (a) of this problem.)

Exercise 1.23: There may be several different min-cut sets in a graph. Using the
analysis of the randomized min-cut algorithm, argue that there can be at most
n ( n −1)/2 distinct min-cut sets.

Exercise 1.24: Generalizing on the notion of a cut-set, we define an r -way cut-set in a
graph as a set of edges whose removal breaks the graph into r or more connected com-
ponents. Explain how the randomized min-cut algorithm can be used to find minimum
r -way cut-sets, and bound the probability that it succeeds in one iteration.

events and probability
Exercise 1.25: To improve the probability of success of the randomized min-cut algo-
rithm, it can be run multiple times.

(a) Consider running the algorithm twice. Determine the number of edge contractions
and bound the probability of finding a min-cut.
(b) Consider the following variation. Starting with a graph with n vertices, first con-
tract the graph down to k vertices using the randomized min-cut algorithm. Make
copies of the graph with k vertices, and now run the randomized algorithm on this
reduced graph ntimes, independently. Determine the number of edge contractions
and bound the probability of finding a minimum cut.
(c) Find optimal (or at least near-optimal) values of k and nfor the variation in (b) that
maximize the probability of finding a minimum cut while using the same number
of edge contractions as running the original algorithm twice.

Exercise 1.26: Tic-tac-toe always ends up in a tie if players play optimally. Instead,
we may consider random variations of tic-tac-toe.

(a) First variation: Each of the nine squares is labeled either X or O according to an
independent and uniform coin flip. If only one of the players has one (or more)
winning tic-tac-toe combinations, that player wins. Otherwise, the game is a tie.
Determine the probability that X wins. (You may want to use a computer program
to help run through the configurations.)
(b) Second variation: X and O take turns, with the X player going first. On the X
player’s turn, an X is placed on a square chosen independently and uniformly at
random from the squares that are still vacant; O plays similarly. The first player to
have a winning tic-tac-toe combination wins the game, and a tie occurs if neither
player achieves a winning combination. Find the probability that each player wins.
(Again, you may want to write a program to help you.)

chapter two

Discrete Random Variables

and Expectation

In this chapter, we introduce the concepts of discrete random variables and expectation
and then develop basic techniques for analyzing the expected performance of algo-
rithms. We apply these techniques to computing the expected running time of the well-
known Quicksort algorithm. In analyzing two versions of Quicksort, we demonstrate
the distinction between the analysis of randomized algorithms, where the probability
space is defined by the random choices made by the algorithm, and the probabilistic
analysis of deterministic algorithms, where the probability space is defined by some
probability distribution on the inputs.
Along the way we define the Bernoulli, binomial, and geometric random variables,
study the expected size of a simple branching process, and analyze the expectation of
the coupon collector’s problem – a probabilistic paradigm that reappears throughout
the book.

2.1 Random Variables and Expectation
When studying a random event, we are often interested in some value associated with
the random event rather than in the event itself. For example, in tossing two dice we
are often interested in the sum of the two dice rather than the separate value of each
die. The sample space in tossing two dice consists of 36 events of equal probability,
given by the ordered pairs of numbers{(1,1),(1,2),...,(6,5),(6,6)}. If the quantity
we are interested in is the sum of the two dice, then we are interested in 11 events (of
unequal probability): the 11 possible outcomes of the sum. Any such function from the
sample space to the real numbers is called a random variable.

Definition 2.1: A random variable X on a sample space  m is a real-valued (mea-
surable) function on  m ; that is, X : m→R_. A discrete random variable is a random
variable that takes on only a finite or countably infinite number of values._

Since random variables are functions, they are usually denoted by a capital letter such
as X or Y , while real numbers are usually denoted by lowercase letters.

discrete random variables and expectation
For a discrete random variable X and a real value a , the event “ X = a ” includes all
the basic events of the sample space in which the random variable X assumes the value
a. That is, “ X = a ” represents the set{ s ∈ m| X ( s )= a }. We denote the probability of
that event by

Pr( X = a )=
∑
s ∈ m: X ( s )= a
Pr( s ).
If X is the random variable representing the sum of the two dice, then the event X = 4
corresponds to the set of basic events{(1,3),(2,2),(3,1)}. Hence

Pr( X =4)=
3
36
=
1
12
.
The definition of independence that we developed for events extends to random
variables.

Definition 2.2: Two random variables X and Y are independent if and only if

Pr(( X = x )∩( Y = y ))=Pr( X = x )·Pr( Y = y )
for all values x and y. Similarly, random variables X 1 , X 2 ,..., Xkare mutually inde-
pendent if and only if, for any subset I ⊆[1, k ] and any values xi,i ∈ I,

Pr
(⋂
i ∈ I
( Xi = xi )
)
=
∏
i ∈ I
Pr( Xi = xi ).
A basic characteristic of a random variable is its expectation , which is also often called
the mean. The expectation of a random variable is a weighted average of the values
it assumes, where each value is weighted by the probability that the variable assumes
that value.

Definition 2.3: The expectation of a discrete random variable X, denoted by E [ X ] ,is
given by

E [ X ]=
∑
i
i Pr( X = i ),
where the summation is over all values in the range of X. The expectation is finite if ∑

i | i |Pr( X = i ) converges; otherwise, the expectation is unbounded.
For example, the expectation of the random variable X representing the sum of two
dice is

E [ X ]=
1
36
· 2 +
2
36
· 3 +
3
36
· 4 +···+
1
36
· 12 = 7.
You may try using symmetry to give simpler argument for why E [ X ]= 7.
As an example of where the expectation of a discrete random variable is unbounded,
consider a random variable X that takes on the value 2 i with probability 1/ 2 i for i =
1 , 2 ,.... The expected value of X is

E [ X ]=
∑∞
i = 1
1
2 i
2 i =
∑∞
i = 1
1 =∞.
2.1random variables and expectation
Here we use the somewhat informal notation E [ X ]=∞to express that E [ X ]is
unbounded.

2.1.1 Linearity of Expectations
A key property of expectation that significantly simplifies its computation is the linear-
ity of expectations. By this property, the expectation of the sum of random variables is
equal to the sum of their expectations. Formally, we have the following theorem.

Theorem 2.1 [ Linearity of Expectations ] : For any finite collection of discrete ran-
dom variables X 1 , X 2 ,..., Xnwith finite expectations,

E
[∑ n
i = 1
Xi
]
=
∑ n
i = 1
E [ Xi ].
Proof: We prove the statement for two random variables X and Y ; the general case
follows by induction. The summations that follow are understood to be over the ranges
of the corresponding random variables:

E [ X + Y ]=
∑
i
∑
j
( i + j )Pr(( X = i )∩( Y = j ))
=
∑
i
∑
j
i Pr(( X = i )∩( Y = j ))+
∑
i
∑
j
j Pr(( X = i )∩( Y = j ))
=
∑
i
i
∑
j
Pr(( X = i )∩( Y = j ))+
∑
j
j
∑
i
Pr(( X = i )∩( Y = j ))
=
∑
i
i Pr( X = i )+
∑
j
j Pr( Y = j )
= E [ X ]+ E [ Y ].
The first equality follows from Definition1.2. In the penultimate equation we have used
Theorem1.6, the Law of Total Probability.  /theta

We now use this property to compute the expected sum of two standard dice. Let X =
X 1 + X 2 , where Xi represents the outcome of die i for i = 1 ,2. Then

E [ Xi ]=
1
6
∑^6
j = 1
j =
7
2
.
Applying the linearity of expectations, we have

E [ X ]= E [ X 1 ]+ E [ X 2 ]= 7.
It is worth emphasizing that linearity of expectations holds for any collection of
random variables, even if they are not independent! For example, consider again the

discrete random variables and expectation
previous example and let the random variable Y = X 1 + X 12 .Wehave

E [ Y ]= E
[
X 1 + X 12
]
= E [ X 1 ]+ E
[
X 12
]
,
even though X 1 and X 12 are clearly dependent. As an exercise, you may verify this
identity by considering the six possible outcomes for X 1.
Linearity of expectations also holds for countably infinite summations in certain
cases. Specifically, it can be shown that

E
[∑∞
i = 1
Xi
]
=
∑∞
i = 1
E [ Xi ]
whenever

∑∞
i = 1 E [| Xi |] converges. The issue of dealing with the linearity of expecta-
tions with countably infinite summations is further considered in Exercise2.29.
This chapter contains several examples in which the linearity of expectations signif-
icantly simplifies the computation of expectations. One result related to the linearity of
expectations is the following simple lemma.

Lemma 2.2: For any constant c and discrete random variable X,

E [ cX ]= c E [ X ].
Proof: The lemma is obvious for c =0. For c =0,

E [ cX ]=
∑
j
j Pr( cX = j )
= c
∑
j
( j / c )Pr( X = j / c )
= c
∑
k
k Pr( X = k )
= c E [ X ].  /theta
2.1.2 Jensen’s Inequality
Suppose that we choose the length X of a side of a square uniformly at random from
the range [1,99]. What is the expected value of the area? We can write this as E [ X^2 ].
It is tempting to think of this as being equal to E [ X ]^2 , but a simple calculation shows
that this is not correct. In fact, E [ X ]^2 =2500 whereas E [ X^2 ]= 9950 / 3 >2500.
More generally, we can prove that E [ X^2 ]≥( E [ X ])^2. Consider Y =( X − E [ X ])^2.
The random variable Y is nonnegative and hence its expectation must also be non-
negative. Therefore,

0 ≤ E [ Y ]= E [( X − E [ X ])^2 ]
= E [ X^2 − 2 X E [ X ]+( E [ X ])^2 ]
= E [ X^2 ]− 2 E [ X E [ X ]]+( E [ X ])^2
= E [ X^2 ]−( E [ X ])^2.
2.2 The Bernoulli and Binomial Random Variables
To obtain the penultimate line, we used the linearity of expectations. To obtain the last
line we used Lemma2.2to simplify E [ X E [ X ]]= E [ X ]· E [ X ].
The fact that E [ X^2 ]≥( E [ X ])^2 is an example of a more general theorem known as
Jensen’s inequality. Jensen’s inequality shows that, for any convex function f ,wehave
E [ f ( X )]≥ f ( E [ X ]).

Definition 2.4: A function f : R → R is said to be convex if, for any x 1 , x 2 and 0 ≤
λ≤ 1 ,

f (λ x 1 +(1−λ) x 2 )≤λ f ( x 1 )+(1−λ) f ( x 2 ).
Visually, a convex function f has the property that, if you connect two points on the
graph of the function by a straight line, this line lies on or above the graph of the
function. The following fact, which we state without proof, is often a useful alternative
to Definition2.4.

Lemma 2.3: If f is a twice differentiable function, then f is convex if and only if
f ′′( x )≥ 0_._

Theorem 2.4 [ Jensen’s Inequality ] : If f is a convex function, then

E [ f ( X )]≥ f ( E [ X ]).
Proof: We prove the theorem assuming that f has a Taylor expansion. Letμ= E [ X ].
By Taylor’s theorem there is a value c such that

f ( x )= f (μ)+ f ′(μ)( x −μ)+
f ′′( c )( x −μ)^2
2
≥ f (μ)+ f ′(μ)( x −μ),
since f ′′( c )>0 by convexity. Taking expectations of both sides and applying linearity
of expectations and Lemma2.2yields the result:

E [ f ( X )]≥ E [ f (μ)+ f ′(μ)( X −μ)]
= E [ f (μ)]+ f ′(μ)( E [ X ]−μ)
= f (μ)= f ( E [ X ]).  /theta
An alternative proof of Jensen’s inequality, which holds for any random variable X that
takes on only finitely many values, is presented in Exercise2.10.

2.2. The Bernoulli and Binomial Random Variables
Suppose that we run an experiment that succeeds with probability p and fails with
probability 1− p.

discrete random variables and expectation
Let Y be a random variable such that
Y =
{
1 if the experiment succeeds,
0 otherwise.
The variable Y is called a Bernoulli or an indicator random variable. Note that, for a
Bernoulli random variable,

E [ Y ]= p · 1 +(1− p )· 0 = p =Pr( Y =1).
For example, if we flip a fair coin and consider the outcome “heads” a success, then
the expected value of the corresponding indicator random variable is 1/2.
Consider now a sequence of n independent coin flips. What is the distribution of the
number of heads in the entire sequence? More generally, consider a sequence of n inde-
pendent experiments, each of which succeeds with probability p .Ifwelet X represent
the number of successes in the n experiments, then X has a binomial distribution.

Definition 2.5: A binomial random variable X with parameters n and p, denoted by
B ( n , p ) , is defined by the following probability distribution on j = 0 , 1 , 2 ,..., n:

Pr( X = j )=
(
n
j
)
pj (1− p ) n − j.
That is, the binomial random variable X equals j when there are exactly j successes and
n − j failures in n independent experiments, each of which is successful with proba-
bility p.
As an exercise, you should show that Definition2.5ensures that

∑ n
j = 0 Pr( X = j )=
This is necessary for the binomial random variable to have a valid probability func-
tion, according to Definition1.2.
The binomial random variable arises in many contexts, especially in sampling. As a
practical example, suppose that we want to gather data about the packets going through
a router by postprocessing them. We might want to know the approximate fraction of
packets from a certain source or of a certain data type. We do not have the memory
available to store all of the packets, so we choose to store a random subset – or sample
of the packets for later analysis. If each packet is stored with probability p and if n
packets go through the router each day, then the number of sampled packets each day
is a binomial random variable X with parameters n and p. If we want to know how
much memory is necessary for such a sample, a natural starting point is to determine
the expectation of the random variable X.
Sampling in this manner arises in other contexts as well. For example, by sampling
the program counter while a program runs, one can determine what parts of a program
are taking the most time. This knowledge can be used to aid dynamic program opti-
mization techniques such as binary rewriting, where the executable binary form of a
program is modified while the program executes. Since rewriting the executable as the
program runs is expensive, sampling helps the optimizer to determine when it will be
worthwhile.
2.3 Conditional Expectation
What is the expectation of a binomial random variable X? We can compute it directly
from the definition as

E [ X ]=
∑ n
j = 0
j
(
n
j
)
pj (1− p ) n − j
=
∑ n
j = 0
j
n!
j !( n − j )!
pj (1− p ) n − j
=
∑ n
j = 1
n!
( j −1)! ( n − j )!
pj (1− p ) n − j
= np
∑ n
j = 1
( n −1)!
( j −1)! (( n −1)−( j −1))!
pj −^1 (1− p )( n −1)−( j −1)
= np
∑ n −^1
k = 0
( n −1)!
k !(( n −1)− k )!
pk (1− p )( n −1)− k
= np
∑ n −^1
k = 0
(
n − 1
k
)
pk (1− p )( n −1)− k
= np ,
where the last equation uses the binomial identity

( x + y ) n =
∑ n
k = 0
( n
k
)
xkyn − k.
The linearity of expectations allows for a significantly simpler argument. If X is a
binomial random variable with parameters n and p , then X is the number of successes
in n trials, where each trial is successful with probability p. Define a set of n indicator
random variables X 1 ,..., Xn , where Xi =1 if the i th trial is successful and 0 otherwise.
Clearly, E [ Xi ]= p and X =

∑ n
i = 1 Xi and so, by the linearity of expectations,
E [ X ]= E
[∑ n
i = 1
Xi
]
=
∑ n
i = 1
E [ Xi ]= np.
The linearity of expectations makes this approach of representing a random variable
by a sum of simpler random variables, such as indicator random variables, extremely
useful.

2.3. Conditional Expectation
Just as we have defined conditional probability, it is useful to define the conditional
expectation of a random variable. The following definition is quite natural.

discrete random variables and expectation
Definition 2.6: E [ Y | Z = z ]=

∑
y
y Pr( Y = y | Z = z ),
where the summation is over all y in the range of Y.

The definition states that the conditional expectation of a random variable is, like the
expectation, a weighted sum of the values it assumes. The difference is that now each
value is weighted by the conditional probability that the variable assumes that value.
One can similarly define the conditional expectation of a random variable Y conditioned
on an eventEas

E [ Y |E]=
∑
y
y Pr( Y = y |E).
For example, suppose that we independently roll two standard six-sided dice. Let X 1
be the number that shows on the first die, X 2 the number on the second die, and X the
sum of the numbers on the two dice. Then

E [ X | X 1 =2]=
∑
x
x Pr( X = x | X 1 =2)=
∑^8
x = 3
x ·
1
6
=
11
2
.
As another example, consider E [ X 1 | X =5]:
E [ X 1 | X =5]=
∑^4
x = 1
x Pr( X 1 = x | X =5)
=
∑^4
x = 1
x
Pr( X 1 = x ∩ X =5)
Pr( X =5)
=
∑^4
x = 1
x
1 / 36
4 / 36
=
5
2
.
The following natural identity follows from Definition2.6.
Lemma 2.5: For any random variables X and Y,

E [ X ]=
∑
y
Pr( Y = y ) E [ X | Y = y ],
where the sum is over all values in the range of Y and all of the expectations exist.

Proof:

∑
y
Pr( Y = y ) E [ X | Y = y ]=
∑
y
Pr( Y = y )
∑
x
x Pr( X = x | Y = y )
=
∑
x
∑
y
x Pr( X = x | Y = y )Pr( Y = y )
=
∑
x
∑
y
x Pr( X = x ∩ Y = y )
=
∑
x
x Pr( X = x )= E [ X ].  /theta
The linearity of expectations also extends to conditional expectations. This is clarified
in Lemma2.6, whose proof is left as Exercise2.11.

2.3 conditional expectation
Lemma 2.6: ForanyfinitecollectionofdiscreterandomvariablesX 1 , X 2 ,..., Xnwith
finite expectations and for any random variable Y,

E
[∑ n
i = 1
Xi | Y = y
]
=
∑ n
i = 1
E [ Xi | Y = y ].
Perhaps somewhat confusingly, the conditional expectation is also used to refer to the
following random variable.

Definition 2.7: The expression E [ Y | Z ] is a random variable f ( Z ) that takes on the
value E [ Y | Z = z ] when Z = z.

We emphasize that E [ Y | Z ]is not a real value; it is actually a function of the random
variable Z. Hence E [ Y | Z ] is itself a function from the sample space to the real numbers
and can therefore be thought of as a random variable.
In the previous example of rolling two dice,

E [ X | X 1 ]=
∑
x
x Pr( X = x | X 1 )=
X ∑ 1 + 6
x = X 1 + 1
x ·
1
6
= X 1 +
7
2
.
We see that E [ X | X 1 ] is a random variable whose value depends on X 1.
If E [ Y | Z ] is a random variable, then it makes sense to consider its expectation
E [ E [ Y | Z ]]. In our example, we found that E [ X | X 1 ]= X 1 + 7 /2. Thus

E [ E [ X | X 1 ]]= E
[
X 1 +
7
2
]
=
7
2
+
7
2
= 7 = E [ X ].
More generally, we have the following theorem.

Theorem 2.7:

E [ Y ]= E [ E [ Y | Z ]].
Proof: From Definition2.7we have E [ Y | Z ]= f ( Z ), where f ( Z ) takes on the value
E [ Y | Z = z ] when Z = z. Hence

E [ E [ Y | Z ]]=
∑
z
E [ Y | Z = z ]Pr( Z = z ).
The right-hand side equals E [ Y ] by Lemma2.5.  /theta

We now demonstrate an interesting application of conditional expectations. Consider
a program that includes one call to a processS. Assume that each call to processS
recursively spawns new copies of the processS, where the number of new copies is
a binomial random variable with parameters n and p. We assume that these random
variables are independent for each call toS. What is the expected number of copies of
the processSgenerated by the program?
To analyze this recursive spawning process, we introduce the idea of generations.
The initial processSis in generation 0. Otherwise, we say that a processSis in gen-
eration i if it was spawned by another processSin generation i −1. Let Yi denote

discrete random variables and expectation
the number ofSprocesses in generation i. Since we know that Y 0 =1, the number of
processes in generation 1 has a binomial distribution. Thus,

E [ Y 1 ]= np.
Similarly, suppose we knew that the number of processes in generation i −1was
yi − 1 ,so Yi − 1 = yi − 1. Let Zk be the number of copies spawned by the k th process spawned
in the ( i −1)th generation for 1≤ k ≤ yi − 1. Each Zk is a binomial random variable with
parameters n and p. Then

E [ Yi | Yi − 1 = yi − 1 ]= E
[∑ yi − 1
k = 1
Zk | Yi − 1 = yi − 1
]
=
∑
j ≥ 0
j Pr
(∑ yi − 1
k = 1
Zk = j | Yi − 1 = yi − 1
)
=
∑
j ≥ 0
j Pr
(∑ yi − 1
k = 1
Zk = j
)
= E
[∑ yi − 1
k = 1
Zk
]
=
∑ yi −^1
k = 1
E [ Zk ]
= yi − 1 np.
In the third line we have used that the Zk are all independent binomial random variables;
in particular, the value of each Zk is independent of Yi − 1 , allowing us to remove the
conditioning. In the fifth line, we have applied the linearity of expectations.
Applying Theorem2.7, we can compute the expected size of the i th generation
inductively. We have

E [ Yi ]= E [ E [ Yi | Yi − 1 ]]= E [ Yi − 1 np ]= np E [ Yi − 1 ].
By induction on i , and using the fact that Y 0 =1, we then obtain

E [ Yi ]=( np ) i.
The expected total number of copies of processSgenerated by the program is
given by

E
[∑
i ≥ 0
Yi
]
=
∑
i ≥ 0
E [ Yi ]=
∑
i ≥ 0
( np ) i.
If np ≥1 then the expectation is unbounded; if np <1, the expectation is 1/(1− np ).
Thus, the expected number of processes generated by the program is bounded if and
only if the expected number of processes spawned by each process is less than 1.
The process analyzed here is a simple example of a branching process , a probabilis-
tic paradigm extensively studied in probability theory.

2.4 The Geometric Distribution
2.4. The Geometric Distribution
Suppose that we flip a coin until it lands on heads. What is the distribution of the number
of flips? This is an example of a geometric distribution , which arises in the following
situation: we perform a sequence of independent trials until the first success, where
each trial succeeds with probability p.

Definition 2.8: A geometric random variable X with parameter p is given by the fol-
lowing probability distribution on n = 1 , 2 ,..., :

Pr( X = n )=(1− p ) n −^1 p.
That is, for the geometric random variable X to equal n , there must be n −1 failures,
followed by a success.
As an exercise, you should show that the geometric random variable satisfies
∑

n ≥ 1
Pr( X = n )= 1.
Again, this is necessary for the geometric random variable to have a valid probability
function, according to Definition1.2.
In the context of our example from Section2.2of sampling packets on a router, if
packets are sampled with probability p , then the number of packets transmitted after the
last sampled packet until and including the next sampled packet is given by a geometric
random variable with parameter p.
Geometric random variables are said to be memoryless because the probability that
you will reach your first success n trials from now is independent of the number of
failures you have experienced. Informally, one can ignore past failures because they do
not change the distribution of the number of future trials until first success. Formally,
we have the following statement.

Lemma 2.8: For a geometric random variable X with parameter p and for n > 0 ,

Pr( X = n + k | X > k )=Pr( X = n ).
Proof: Pr( X = n + k | X > k )=Pr(( X = n + k )∩( X > k ))
Pr( X > k )

=
Pr( X = n + k )
Pr( X > k )
=
(1− p ) n + k −^1 p
∑∞
i = k (1− p ) ip
=
(1− p ) n + k −^1 p
(1− p ) k
=(1− p ) n −^1 p
=Pr( X = n ).
The fourth equality uses the fact that, for 0< x <1,

∑∞
i = kx
i = xk /(1− x ).  /theta
discrete random variables and expectation
We now turn to computing the expectation of a geometric random variable. When a
random variable takes values in the set of natural numbers N ={ 0 , 1 , 2 , 3 ,...}, there
is an alternative formula for calculating its expectation.

Lemma 2.9: Let X be a discrete random variable that takes on only nonnegative inte-
ger values. Then

E [ X ]=
∑∞
i = 1
Pr( X ≥ i ).
Proof:

∑∞
i = 1
Pr( X ≥ i )=
∑∞
i = 1
∑∞
j = i
Pr( X = j )
=
∑∞
j = 1
∑ j
i = 1
Pr( X = j )
=
∑∞
j = 1
j Pr( X = j )
= E [ X ].
The interchange of (possibly) infinite summations is justified, since the terms being
summed are all nonnegative.  /theta

For a geometric random variable X with parameter p ,

Pr( X ≥ i )=
∑∞
n = i
(1− p ) n −^1 p =(1− p ) i −^1.
Hence

E [ X ]=
∑∞
i = 1
Pr( X ≥ i )
=
∑∞
i = 1
(1− p ) i −^1
=
1
1 −(1− p )
=
1
p
.
Thus, for a fair coin where p = 1 /2, on average it takes two flips to see the first
heads.
There is another approach to finding the expectation of a geometric random variable
X with parameter p – one that uses conditional expectations and the memoryless prop-
erty of geometric random variables. Recall that X corresponds to the number of flips
until the first heads given that each flip is heads with probability p. Let Y =0 if the first

2.4 the geometric distribution
flip is tails and Y =1 if the first flip is heads. By the identity from Lemma2.5,

E [ X ]=Pr( Y =0) E [ X | Y =0]+Pr( Y =1) E [ X | Y =1]
=(1− p ) E [ X | Y =0]+ p E [ X | Y =1].
If Y =1 then X =1, so E [ X | Y =1]=1. If Y =0, then X >1. In this case, let
the number of remaining flips (after the first flip until the first heads) be Z. Then, by the
linearity of expectations,

E [ X ]=(1− p ) E [ Z +1]+ p · 1 =(1− p ) E [ Z ]+ 1.
By the memoryless property of geometric random variables, Z is also a geometric ran-
dom variable with parameter p. Hence E [ Z ]= E [ X ], since they both have the same
distribution. We therefore have

E [ X ]=(1− p ) E [ Z ]+ 1 =(1− p ) E [ X ]+ 1 ,
which yields E [ X ]= 1 / p.
This method of using conditional expectations to compute an expectation is often
useful, especially in conjunction with the memoryless property of a geometric random
variable.

2.4.1 Example: Coupon Collector’s Problem
The coupon collector’s problem arises from the following scenario. Suppose that each
box of cereal contains one of n different coupons. Once you obtain one of every type
of coupon, you can send in for a prize. Assuming that the coupon in each box is chosen
independently and uniformly at random from the n possibilities and that you do not
collaborate with others to collect coupons, how many boxes of cereal must you buy
before you obtain at least one of every type of coupon? This simple problem arises in
many different scenarios and will reappear in several places in the book.
Let X be the number of boxes bought until at least one of every type of coupon is
obtained. We now determine E [ X ]. If Xi is the number of boxes bought while you had
exactly i −1 different coupons, then clearly X =

∑ n
i = 1 Xi.
The advantage of breaking the random variable X into a sum of n random variables
Xi , i = 1 ,..., n , is that each Xi is a geometric random variable. When exactly i − 1
coupons have been found, the probability of obtaining a new coupon is

pi = 1 −
i − 1
n
.
Hence, Xi is a geometric random variable with parameter pi , and

E [ Xi ]=
1
pi
=
n
n − i + 1
.
discrete random variables and expectation
Using the linearity of expectations, we have that

E [ X ]= E
[∑ n
i = 1
Xi
]
=
∑ n
i = 1
E [ Xi ]
=
∑ n
i = 1
n
n − i + 1
= n
∑ n
i = 1
1
i
.
The summation
∑ n
i = 11 / i is known as the harmonic number H ( n ), and as we show
next, H ( n )=ln n + /eta(1). Thus, for the coupon collector’s problem, the expected num-
ber of random coupons required to obtain all n coupons is n ln n + /eta( n ).

Lemma 2.10: The harmonic number H ( n )=

∑ n
i = 11 / i satisfies H ( n )=ln n + /eta(1).
Proof: Since 1/ x is monotonically decreasing, we can write

ln n =
∫ n
x = 1
1
x
dx ≤
∑ n
k = 1
1
k
and

∑ n
k = 2
1
k
≤
∫ n
x = 1
1
x
dx =ln n.
This is clarified in Figure2.1, where the area below the curve f ( x )= 1 / x corre-
sponds to the integral and the areas of the shaded regions correspond to the summations∑
n
k = 11 / k and

∑ n
k = 21 / k.
Hence ln n ≤ H ( n )≤ln n +1, proving the claim.  /theta
As a simple application of the coupon collector’s problem, suppose that packets are
sent in a stream from a source host to a destination host along a fixed path of routers.
The host at the destination would like to know which routers the stream of packets has
passed through, in case it finds later that some router damaged packets that it processed.
If there is enough room in the packet header, each router can append its identification
number to the header, giving the path. Unfortunately, there may not be that much room
available in the packet header.
Suppose instead that each packet header has space for exactly one router identi-
fication number, and this space is used to store the identification of a router chosen
uniformly at random from all of the routers on the path. This can actually be accom-
plished easily; we consider how in Exercise2.18. Then, from the point of view of the
destination host, determining all the routers on the path is like a coupon collector’s
problem. If there are n routers along the path, then the expected number of packets in

2.5 Application: The Expected Run-Time of Quicksort
++
+
Figure 2.1: Approximating the area above and below f ( x )= 1 / x.
the stream that must arrive before the destination host knows all of the routers on the
path is nH ( n )= n ln n + /eta( n ).

2.5. Application: The Expected Run-Time of Quicksort
Quicksort is a simple – and, in practice, very efficient – sorting algorithm. The input is
a list of n numbers x 1 , x 2 ,..., xn. For convenience, we will assume that the numbers
are distinct. A call to the Quicksort function begins by choosing a pivot element from
the set. Let us assume the pivot is x. The algorithm proceeds by comparing every other
element to x , dividing the list of elements into two sublists: those that are less than x
and those that are greater than x. Notice that if the comparisons are performed in the
natural order, from left to right, then the order of the elements in each sublist is the
same as in the initial list. Quicksort then recursively sorts these sublists.
In the worst case, Quicksort requires m( n^2 ) comparison operations. For example,
suppose our input has the form x 1 = n , x 2 = n −1,..., xn − 1 =2, xn =1. Suppose
also that we adopt the rule that the pivot should be the first element of the list. The
first pivot chosen is then n , so Quicksort performs n −1 comparisons. The division
has yielded one sublist of size 0 (which requires no additional work) and another of
size n −1, with the order n − 1 , n − 2 ,..., 2 ,1. The next pivot chosen is n −1, so
Quicksort performs n −2 comparisons and is left with one group of size n −2inthe
order n − 2 , n − 3 ,..., 2 ,1. Continuing in this fashion, Quicksort performs

( n −1)+( n −2)+···+ 2 + 1 =
n ( n −1)
2
comparisons.
This is not the only bad case that leads to m( n^2 ) comparisons; similarly poor perfor-
mance occurs if the pivot element is chosen from among the smallest few or the largest
few elements each time.

discrete random variables and expectation
Quicksort Algorithm:
Input: A list S ={ x 1 ,..., xn }of n distinct elements over a totally ordered universe.
Output: The elements of S in sorted order.
1. If S has one or zero elements, return S. Otherwise continue.
2. Choose an element of S as a pivot; call it s.
3. Compare every other element of S to x in order to divide the other elements into
two sublists:
(a) S 1 has all the elements of S that are less than x ;
(b) S 2 has all those that are greater than x.
4. Use Quicksort to sort S 1 and S 2.
5. Return the list S 1 , x , S 2.

Algorithm 2.1: Quicksort.
We clearly made a bad choice of pivots for the given input. A reasonable choice of
pivots would require many fewer comparisons. For example, if our pivot always splits
the list into two sublists of size at most n / 2 , then the number of comparisons C ( n )
would obey the following recurrence relation:

C ( n )≤ 2 C ( n / 2 )+ /eta( n ).
The solution to this equation yields C ( n )= O ( n log n ), which is the best possible result
for comparison-based sorting. In fact, any sequence of pivot elements that always split
the input list into two sublists each of size at least cn for some constant c would yield
an O ( n log n ) running time.
This discussion provides some intuition for how we would like pivots to be chosen.
In each iteration of the algorithm there is a good set of pivot elements that split the
input list into two almost equal sublists; it suffices if the sizes of the two sublists are
within a constant factor of each other. There is also a bad set of pivot elements that do
not split up the list significantly. If good pivots are chosen sufficiently often, Quicksort
will terminate quickly. How can we guarantee that the algorithm chooses good pivot
elements sufficiently often? We can resolve this problem in one of two ways.
First, we can change the algorithm to choose the pivots randomly. This makes Quick-
sort a randomized algorithm; the randomization makes it extremely unlikely that we
repeatedly choose the wrong pivots. We demonstrate shortly that the expected number
of comparisons made by a simple randomized Quicksort is 2 n ln n + O ( n ), matching
(up to constant factors) the m( n log n ) bound for comparison-based sorting. Here, the
expectation is over the random choice of pivots.
A second possibility is that we can keep our deterministic algorithm, using the first
list element as a pivot, but consider a probabilistic model of the inputs. A permutation
of a set of n distinct items is just one of the n! orderings of these items. Instead of
looking for the worst possible input, we assume that the input items are given to us in
a random order. This may be a reasonable assumption for some applications; alterna-
tively, this could be accomplished by ordering the input list according to a randomly

2.5application: the expected run-time of quicksort
chosen permutation before running the deterministic Quicksort algorithm. In this case,
we have a deterministic algorithm but a probabilistic analysis based on a model of the
inputs. We again show in this setting that the expected number of comparisons made
is 2 n ln n + O ( n ). Here, the expectation is over the random choice of inputs.
The same techniques are generally used both in analyses of randomized algorithms
and in probabilistic analyses of deterministic algorithms. Indeed, in this application the
analysis of the randomized Quicksort and the probabilistic analysis of the deterministic
Quicksort under random inputs are essentially the same.
Let us first analyze Random Quicksort, the randomized algorithm version of
Quicksort.
Theorem 2.11: Suppose that, whenever a pivot is chosen for Random Quicksort, it
is chosen independently and uniformly at random from all possibilities. Then, for any
input, the expected number of comparisons made by Random Quicksort is 2 n ln n +
O ( n ).
Proof: Let y 1 , y 2 ,..., yn be the same values as the input values x 1 , x 2 ,..., xn but sorted
in increasing order. For i < j , let Xij be a random variable that takes on the value 1 if
yi and yj are compared at any time over the course of the algorithm, and 0 otherwise.
Then the total number of comparisons X satisfies
X =
∑ n −^1
i = 1
∑ n
j = i + 1
Xij ,
and
E [ X ]= E
[ n ∑− 1
i = 1
∑ n
j = i + 1
Xij
]
=
∑ n −^1
i = 1
∑ n
j = i + 1
E [ Xij ]
by the linearity of expectations.
Since Xij is an indicator random variable that takes on only the values 0 and 1,
E [ Xij ] is equal to the probability that Xij is 1. Hence all we need to do is compute the
probability that two elements yi and yj are compared. Now, yi and yj are compared if
and only if either yi or yj is the first pivot selected by Random Quicksort from the set
Yij ={ yi , yi + 1 ,..., yj − 1 , yj }. This is because if yi (or yj ) is the first pivot selected from
this set, then yi and yj must still be in the same sublist, and hence they will be compared.
Similarly, if neither is the first pivot from this set, then yi and yj will be separated into
distinct sublists and so will not be compared.
Since our pivots are chosen independently and uniformly at random from each sub-
list, it follows that, the first time a pivot is chosen from Yij , it is equally likely to be
any element from this set. Thus the probability that yi or yj is the first pivot selected
from Yij , which is the probability that Xij =1, is 2/( j − i +1). Using the substitution

discrete random variables and expectation
k = j − i +1 then yields

E [ X ]=
∑ n −^1
i = 1
∑ n
j = i + 1
2
j − i + 1
=
∑ n −^1
i = 1
n ∑− i + 1
k = 2
2
k
=
∑ n
k = 2
n +∑ 1 − k
i = 1
2
k
=
∑ n
k = 2
( n + 1 − k )
2
k
=
(
( n +1)
∑ n
k = 2
2
k
)
−2( n −1)
=(2 n +2)
∑ n
k = 1
1
k
− 4 n.
Notice that we used a rearrangement of the double summation to obtain a clean form
for the expectation.
Recalling that the summation H ( n )=

∑ n
k = 11 / k satisfies H ( n )=ln n + /eta(1), we
have E [ X ]= 2 n ln n + /eta( n ).  /theta

Next we consider the deterministic version of Quicksort, on random input. We assume
that the order of the elements in each recursively constructed sublist is the same as in
the initial list.

Theorem 2.12: Suppose that, whenever a pivot is chosen for Quicksort, the first ele-
ment of the sublist is chosen. If the input is chosen uniformly at random from all pos-
sible permutations of the values, then the expected number of comparisons made by
Deterministic Quicksort is 2 n ln n + O ( n ).

Proof: The proof is essentially the same as for Random Quicksort. Again, yi and yj
are compared if and only if either yi or yj is the first pivot selected by Quicksort from
the set Yij. Since the order of elements in each sublist is the same as in the original list,
the first pivot selected from the set Yij is just the first element from Yij in the input list,
and since all possible permutations of the input values are equally likely, every element
in Yij is equally likely to be first. From this, we can again use linearity of expectations
in the same way as in the analysis of Random Quicksort to obtain the same expression
for E [ X ].  /theta

2.6 Exercises
Exercise 2.1: Suppose we roll a fair k -sided die with the numbers 1 through k on the
die’s faces. If X is the number that appears, what is E [ X ]?

2.6 exercises
Exercise 2.2: A monkey types on a 26-letter keyboard that has lowercase letters only.
Each letter is chosen independently and uniformly at random from the alphabet. If the
monkey types 1,000,000 letters, what is the expected number of times the sequence
“proof” appears?

Exercise 2.3: Give examples of functions f and random variables X where E [ f ( X )]<
f ( E [ X ]), E [ f ( X )]= f ( E [ X ]), and E [ f ( X )]> f ( E [ X ]).

Exercise 2.4: Prove that E [ Xk ]≥ E [ X ] k for any even integer k ≥1.

Exercise 2.5: If X is a B ( n , 1 /2) random variable with n ≥1, show that the probability
that X is even is 1/2.

Exercise 2.6: Suppose that we independently roll two standard six-sided dice. Let X 1
be the number that shows on the first die, X 2 the number on the second die, and X the
sum of the numbers on the two dice.

(a) What is E [ X | X 1 is even]?
(b) What is E [ X | X 1 = X 2 ]?
(c) What is E [ X 1 | X =9]?
(d) What is E [ X 1 − X 2 | X = k ]for k in the range [2,12]?

Exercise 2.7: Let X and Y be independent geometric random variables, where X has
parameter p and Y has parameter q.

(a) What is the probability that X = Y?
(b) What is E [max( X , Y )]?
(c) What is Pr(min( X , Y )= k )?
(d) What is E [ X | X ≤ Y ]?

You may find it helpful to keep in mind the memoryless property of geometric random
variables.

Exercise 2.8: (a) Alice and Bob decide to have children until either they have their first
girl or they have k ≥1 children. Assume that each child is a boy or girl independently
with probability 1/2 and that there are no multiple births. What is the expected number
of female children that they have? What is the expected number of male children that
they have?
(b) Suppose Alice and Bob simply decide to keep having children until they have
their first girl. Assuming that this is possible, what is the expected number of boys that
they have?

Exercise 2.9: (a) Suppose that we roll twice a fair k -sided die with the numbers 1
through k on the die’s faces, obtaining values X 1 and X 2. What is E [max( X 1 , X 2 )]?
What is E [min( X 1 , X 2 )]?

discrete random variables and expectation
(b) Show from your calculations in part (a) that
E [max( X 1 , X 2 )]+ E [min( X 1 , X 2 )]= E [ X 1 ]+ E [ X 2 ]. (2.1)
(c) Explain why Eqn. (2.1) must be true by using the linearity of expectations instead
of a direct computation.

Exercise 2.10: (a) Show by induction that if f : R → R is convex then, for any
x 1 , x 2 ,..., xn andλ 1 ,λ 2 ,...,λ n with

∑ n
i = 1 λ i =1,
f
(∑ n
i = 1
λ ixi
)
≤
∑ n
i = 1
λ if ( xi ). (2.2)
(b) Use Eqn. (2.2) to prove that if f : R → R is convex then
E [ f ( X )]≥ f ( E [ X ])
for any random variable X that takes on only finitely many values.

Exercise 2.11: Prove Lemma2.6.

Exercise 2.12: We draw cards uniformly at random with replacement from a deck of
n cards. What is the expected number of cards we must draw until we have seen all n
cards in the deck? If we draw 2 n cards, what is the expected number of cards in the
deck that are not chosen at all? Chosen exactly once?

Exercise 2.13: (a) Consider the following variation of the coupon collector’s problem.
Each box of cereal contains one of 2 n different coupons. The coupons are organized
into n pairs, so that coupons 1 and 2 are a pair, coupons 3 and 4 are a pair, and so on.
Once you obtain one coupon from every pair, you can obtain a prize. Assuming that
the coupon in each box is chosen independently and uniformly at random from the 2 n
possibilities, what is the expected number of boxes you must buy before you can claim
the prize?
(b) Generalize the result of the problem in part (a) for the case where there are kn
different coupons, organized into n disjoint sets of k coupons, so that you need one
coupon from every set.

Exercise 2.14: The geometric distribution arises as the distribution of the number of
times we flip a coin until it comes up heads. Consider now the distribution of the number
of flips X until the k th head appears, where each coin flip comes up heads independently
with probability p. Prove that this distribution is given by

Pr( X = n )=
(
n − 1
k − 1
)
pk (1− p ) n − k
for n ≥ k. (This is known as the negative binomial distribution.)

Exercise 2.15: For a coin that comes up heads independently with probability p on
each flip, what is the expected number of flips until the k th heads?

2.6 exercises
Exercise 2.16: Suppose we flip a coin n times to obtain a sequence of flips X 1 ,
X 2 ,..., Xn .A streak of flips is a consecutive subsequence of flips that are all the same.
For example, if X 3 , X 4 , and X 5 are all heads, there is a streak of length 3 starting at the
third flip. (If X 6 is also heads, then there is also a streak of length 4 starting at the third
flip.)

(a) Let n be a power of 2. Show that the expected number of streaks of length log 2 n + 1
is 1− o (1).
(b) Show that, for sufficiently large n , the probability that there is no streak of length
at least log 2 n −2 log 2 log 2 n is less than 1/ n .( Hint: Break the sequence of flips
up into disjoint blocks of log 2 n −2 log 2 log 2 n consecutive flips, and use that the
event that one block is a streak is independent of the event that any other block is
a streak.)

Exercise 2.17: Recall the recursive spawning process described in Section2.3. Sup-
pose that each call to processSrecursively spawns new copies of the processS, where
the number of new copies is 2 with probability p and 0 with probability 1− p .If Yi
denotes the number of copies ofSin the i th generation, determine E [ Yi ]. For what
values of p is the expected total number of copies bounded?

Exercise 2.18: The following approach is often called reservoir sampling. Suppose
we have a sequence of items passing by one at a time. We want to maintain a sample
of one item with the property that it is uniformly distributed over all the items that we
have seen at each step. Moreover, we want to accomplish this without knowing the total
number of items in advance or storing all of the items that we see.
Consider the following algorithm, which stores just one item in memory at all times.
When the first item appears, it is stored in the memory. When the k th item appears, it
replaces the item in memory with probability 1/ k. Explain why this algorithm solves
the problem.

Exercise 2.19: Suppose that we modify the reservoir sampling algorithm of Exer-
cise2.18so that, when the k th item appears, it replaces the item in memory with prob-
ability 1/2. Describe the distribution of the item in memory.

Exercise 2.20: A permutation on the numbers [1, n ] can be represented as a function
π:[1, n ]→[1, n ], whereπ( i ) is the position of i in the ordering given by the permu-
tation. A fixed point of a permutationπ:[1, n ]→[1, n ] is a value for whichπ( x )= x.
Find the expected number of fixed points of a permutation chosen uniformly at random
from all permutations.

Exercise 2.21: Let a 1 , a 2 ,..., an be a random permutation of{ 1 , 2 ,..., n }, equally
likely to be any of the n! possible permutations. When sorting the list a 1 , a 2 ,..., an ,
the element ai must move a distance of| ai − i |places from its current position to reach

discrete random variables and expectation
its position in the sorted order. Find

E
[∑ n
i = 1
| ai − i |
]
,
the expected total distance that elements will have to be moved.

Exercise 2.22: Let a 1 , a 2 ,..., an be a list of n distinct numbers. We say that ai and
aj are inverted if i < j but ai > aj. The Bubblesort sorting algorithm swaps pairwise
adjacent inverted numbers in the list until there are no more inversions, so the list is
in sorted order. Suppose that the input to Bubblesort is a random permutation, equally
likely to be any of the n! permutations of n distinct numbers. Determine the expected
number of inversions that need to be corrected by Bubblesort.

Exercise 2.23: Linear insertion sort can sort an array of numbers in place. The first
and second numbers are compared; if they are out of order, they are swapped so that
they are in sorted order. The third number is then placed in the appropriate place in the
sorted order. It is first compared with the second; if it is not in the proper order, it is
swapped and compared with the first. Iteratively, the k th number is handled by swap-
ping it downward until the first k numbers are in sorted order. Determine the expected
number of swaps that need to be made with a linear insertion sort when the input is a
random permutation of n distinct numbers.

Exercise 2.24: We roll a standard fair die over and over. What is the expected number
of rolls until the first pair of consecutive sixes appears? ( Hint: The answer is not 36.)

Exercise 2.25: A blood test is being performed on n individuals. Each person can
be tested separately, but this is expensive. Pooling can decrease the cost. The blood
samples of k people can be pooled and analyzed together. If the test is negative, this
one test suffices for the group of k individuals. If the test is positive, then each of the k
persons must be tested separately and thus k +1 total tests are required for the k people.
Suppose that we create n / k disjoint groups of k people (where k divides n ) and use the
pooling method. Assume that each person has a positive result on the test independently
with probability p.

(a) What is the probability that the test for a pooled sample of k people will be positive?
(b) What is the expected number of tests necessary?
(c) Describe how to find the best value of k.
(d) Give an inequality that shows for what values of p pooling is better than just testing
every individual.

Exercise 2.26: A permutationπ:[1, n ]→[1, n ] can be represented as a set of cycles
as follows. Let there be one vertex for each number i , i = 1 ,..., n. If the permutation
maps the number i to the numberπ( i ), then a directed arc is drawn from vertex i to
vertexπ( i ). This leads to a graph that is a set of disjoint cycles. Notice that some of

2.6 exercises
the cycles could be self-loops. What is the expected number of cycles in a random
permutation of n numbers?

Exercise 2.27: Consider the following distribution on the integers x ≥1: Pr( X = x )
=(6/π^2 ) x −^2. This is a valid distribution, since

∑∞
k = 1 k
− (^2) =π (^2) /6. What is its
expectation?
Exercise 2.28: Consider a simplified version of roulette in which you wager x dollars
on either red or black. The wheel is spun, and you receive your original wager plus
another x dollars if the ball lands on your color; if the ball doesn’t land on your color,
you lose your wager. Each color occurs independently with probability 1/2. (This is a
simplification because real roulette wheels have one or two spaces that are neither red
nor black, so the probability of guessing the correct color is actually less than 1/2.)
The following gambling strategy is a popular one. On the first spin, bet 1 dollar. If
you lose, bet 2 dollars on the next spin. In general, if you have lost on the first k − 1
spins, bet 2 k −^1 dollars on the k th spin. Argue that by following this strategy you will
eventually win a dollar. Now let X be the random variable that measures your maximum
loss before winning (i.e., the amount of money you have lost before the play on which
you win). Show that E [ X ] is unbounded. What does it imply about the practicality of
this strategy?
Exercise 2.29: Prove that, if X 0 , X 1 ,...is a sequence of random variables such that
∑∞
j = 0
E [| Xj |]
converges, then the linearity of expectations holds:

E
[∑∞
j = 0
Xj
]
=
∑∞
j = 0
E [ Xj ].
Exercise 2.30: In the roulette problem of Exercise2.28, we found that with probability
1 you eventually win a dollar. Let Xj be the amount you win on the j th bet. (This
might be 0 if you have already won a previous bet.) Determine E [ Xj ] and show that,
by applying the linearity of expectations, you find your expected winnings are 0. Does
the linearity of expectations hold in this case? (Compare with Exercise2.29.)

Exercise 2.31: A variation on the roulette problem of Exercise2.28is the following.
We repeatedly flip a fair coin. You pay j dollars to play the game. If the first head comes
up on the k th flip, you win 2 k / k dollars. What are your expected winnings? How much
would you be willing to pay to play the game?

Exercise 2.32: You need a new staff assistant, and you have n people to interview. You
want to hire the best candidate for the position. When you interview a candidate, you
can give them a score, with the highest score being the best and no ties being possible.

discrete random variables and expectation
You interview the candidates one by one. Because of your company’s hiring practices,
after you interview the k th candidate, you either offer the candidate the job before the
next interview or you forever lose the chance to hire that candidate. We suppose the
candidates are interviewed in a random order, chosen uniformly at random from all n!
possible orderings.
We consider the following strategy. First, interview m candidates but reject them all;
these candidates give you an idea of how strong the field is. After the m th candidate,
hire the first candidate you interview who is better than all of the previous candidates
you have interviewed.

(a) Let E be the event that we hire the best assistant, and let Ei be the event that i th
candidate is the best and we hire him. Determine Pr( Ei ), and show that
Pr( E )=
m
n
∑ n
j = m + 1
1
j − 1
.
(b) Bound

∑ n
j = m + 1
1
j − 1 to obtain
m
n
(ln n −ln m )≤Pr( E )≤
m
n
(ln( n −1)−ln( m −1)).
(c) Show that m (ln n −ln m )/ n is maximized when m = n /e, and explain why this
means Pr( E )≥ 1 /e for this choice of m.
chapter three

Moments and Deviations

In this and the next chapter we examine techniques for bounding the tail distribution,
the probability that a random variable assumes values that are far from its expectation.
In the context of analysis of algorithms, these bounds are the major tool for estimating
the failure probability of algorithms and for establishing high probability bounds on
their run-time. In this chapter we study Markov’s and Chebyshev’s inequalities and
demonstrate their application in an analysis of a randomized median algorithm. The
next chapter is devoted to the Chernoff bound and its applications.

3.1 Markov’s Inequality
Markov’s inequality, formulated in the next theorem, is often too weak to yield useful
results, but it is a fundamental tool in developing more sophisticated bounds.

Theorem 3.1 [ Markov’s Inequality ] : Let X be a random variable that assumes only
nonnegative values. Then, for all a > 0 ,

Pr( X ≥ a )≤
E [ X ]
a
.
Proof: For a >0, let

I =
{
1if X ≥ a ,
0 otherwise,
and note that, since X ≥0,

I ≤
X
a
. (3.1)
Because I is a 0–1 random variable, E [ I ]=Pr( I =1)=Pr( X ≥ a ).
Taking expectations in (3.1) thus yields

Pr( X ≥ a )= E [ I ]≤ E
[
X
a
]
=
E [ X ]
a
.
 /theta
moments and deviations
For example, suppose we use Markov’s inequality to bound the probability of obtaining
more than 3 n /4 heads in a sequence of n fair coin flips. Let

Xi =
{
1 if the i th coin flip is heads,
0 otherwise,
and let X =

∑ n
i = 1 Xi denote the number of heads in the n coin flips. Since E [ Xi ]=
Pr( Xi =1)= 1 /2, it follows that E [ X ]=

∑ n
i = 1 E [ Xi ]= n /2. Applying Markov’s
inequality, we obtain

Pr( X ≥ 3 n /4)≤
E [ X ]
3 n / 4
=
n / 2
3 n / 4
=
2
3
.
3.2. Variance and Moments of a Random Variable
Markov’s inequality gives the best tail bound possible when all we know is the expec-
tation of the random variable and that the variable is nonnegative (see Exercise3.16). It
can be improved upon if more information about the distribution of the random variable
is available.
Additional information about a random variable is often expressed in terms of its
moments. The expectation is also called the first moment of a random variable. More
generally, we define the moments of a random variable as follows.

Definition 3.1: The k th moment of a random variable X is E [ Xk ].

A significantly stronger tail bound is obtained when the second moment ( E [ X^2 ]) is also
available. Given the first and second moments, one can compute the variance and stan-
dard deviation of the random variable. Intuitively, the variance and standard deviation
offer a measure of how far the random variable is likely to be from its expectation.

Definition 3.2: The variance of a random variable X is defined as

Var [ X ]= E [( X − E [ X ])^2 ]= E [ X^2 ]−( E [ X ])^2.
The standard deviation of a random variable X is

σ[ X ]=
√
Var [ X ].
The two forms of the variance in the definition are equivalent, as is easily seen by using
the linearity of expectations. Keeping in mind that E [ X ] is a constant, we have

E [( X − E [ X ])^2 ]= E [ X^2 − 2 X E [ X ]+ E [ X ]^2 ]
= E [ X^2 ]− 2 E [ X E [ X ]]+ E [ X ]^2
= E [ X^2 ]− 2 E [ X ] E [ X ]+ E [ X ]^2
= E [ X^2 ]−( E [ X ])^2.
If a random variable X is constant – so that it always assumes the same value – then
its variance and standard deviation are both zero. More generally, if a random vari-
able X takes on the value k E [ X ] with probability 1/ k and the value 0 with probability

3.2 Variance and Moments of a Random Variable
1 − 1 / k , then its variance is ( k −1)( E [ X ])^2 and its standard deviation is

√
k − 1 E [ X ].
These cases help demonstrate the intuition that the variance (and standard deviation)
of a random variable are small when the random variable assumes values close to its
expectation and are large when it assumes values far from its expectation.
We have previously seen that the expectation of the sum of two random variables is
equal to the sum of their individual expectations. It is natural to ask whether the same
is true for the variance. We find that the variance of the sum of two random variable
has an extra term, called the covariance.

Definition 3.3: The covariance of two random variables X and Y is

Cov ( X , Y )= E [( X − E [ X ])( Y − E [ Y ])].
Theorem 3.2: For any two random variables X and Y,

Var [ X + Y ]= Var [ X ]+ Var [ Y ]+ 2 Cov ( X , Y ).
Proof:

Var [ X + Y ]= E [( X + Y − E [ X + Y ])^2 ]
= E [( X + Y − E [ X ]− E [ Y ])^2 ]
= E [( X − E [ X ])^2 +( Y − E [ Y ])^2 +2( X − E [ X ])( Y − E [ Y ])]
= E [( X − E [ X ])^2 ]+ E [( Y − E [ Y ])^2 ]+ 2 E [( X − E [ X ])( Y − E [ Y ])]
= Var [ X ]+ Var [ Y ]+ 2 Cov ( X , Y ).  /theta
The extension of this theorem to a sum of any finite number of random variables is
proven in Exercise3.14.
The variance of the sum of two (or any finite number of) random variables does equal
the sum of the variances when the random variables are independent. Equivalently, if X
and Y are independent random variables, then their covariance is equal to zero. To prove
this result, we first need a result about the expectation of the product of independent
random variables.

Theorem 3.3: If X and Y are two independent random variables, then

E [ X · Y ]= E [ X ]· E [ Y ].
Proof: In the summations that follow, let i take on all values in the range of X , and let
j take on all values in the range of Y :

E [ X · Y ]=
∑
i
∑
j
( i · j )·Pr(( X = i )∩( Y = j ))
=
∑
i
∑
j
( i · j )·Pr( X = i )·Pr( Y = j )
=
(∑
i
i ·Pr( X = i )
)(∑
j
j ·Pr( Y = j )
)
= E [ X ]· E [ Y ],
where the independence of X and Y is used in the second line.  /theta

moments and deviations
Unlike the linearity of expectations, which holds for the sum of random variables
whether they are independent or not, the result that the expectation of the product of
two (or more) random variables is equal to the product of their expectations does not
necessarily hold if the random variables are dependent. To see this, let Y and Z each
correspond to fair coin flips, with Y and Z taking on the value 0 if the flip is heads
and 1 if the flip is tails. Then E [ Y ]= E [ Z ]= 1 /2. If the two flips are independent,
then Y · Z is 1 with probability 1/4 and 0 otherwise, so indeed E [ Y · Z ]= E [ Y ]· E [ Z ].
Suppose instead that the coin flips are dependent in the following way: the coins are
tied together, so Y and Z either both come up heads or both come up tails together. Each
coin considered individually is still a fair coin flip, but now Y · Z is 1 with probability
1 /2 and so E [ Y · Z ]= E [ Y ]· E [ Z ].

Corollary 3.4: If X and Y are independent random variables, then

Cov ( X , Y )= 0
and

Var [ X + Y ]= Var [ X ]+ Var [ Y ].
Proof:

Cov ( X , Y )= E [( X − E [ X ])( Y − E [ Y ])]
= E [ X − E [ X ]]· E [ Y − E [ Y ]]
= 0.
In the second equation we have used the fact that, since X and Y are independent, so
are X − E [ X ] and Y − E [ Y ] and hence Theorem 3.3 applies. For the last equation we
use the fact that, for any random variable Z ,

E [( Z − E [ Z ])]= E [ Z ]− E [ E [ Z ]]= 0.
Since Cov ( X , Y )=0, we have Var [ X + Y ]= Var [ X ]+ Var [ Y ].  /theta

By induction we can extend the result of Corollary3.4to show that the variance of
the sum of any finite number of independent random variables equals the sum of their
variances.

Theorem 3.5: Let X 1 , X 2 ,..., Xnbe mutually independent random variables. Then

Var
[∑ n
i = 1
Xi
]
=
∑ n
i = 1
Var [ Xi ].
3.3chebyshev’s inequality
3.2.1 Example: Variance of a Binomial Random Variable
The variance of a binomial random variable X with parameters n and p can be deter-
mined directly by computing E [ X^2 ]:

E [ X^2 ]=
∑ n
j = 0
(
n
j
)
pj (1− p ) n − jj^2
=
∑ n
j = 0
n!
( n − j )! j!
pj (1− p ) n − j (( j^2 − j )+ j )
=
∑ n
j = 0
n !( j^2 − j )
( n − j )! j!
pj (1− p ) n − j +
∑ n
j = 0
n! j
( n − j )! j!
pj (1− p ) n − j
= n ( n −1) p^2
∑ n
j = 2
( n −2)!
( n − j )! ( j −2)!
pj −^2 (1− p ) n − j
+ np
∑ n
j = 1
( n −1)!
( n − j )! ( j −1)!
pj −^1 (1− p ) n − j
= n ( n −1) p^2 + np.
Here we have simplified the summations by using the binomial theorem. We conclude
that

Var [ X ]= E [ X^2 ]−( E [ X ])^2
= n ( n −1) p^2 + np − n^2 p^2
= np − np^2
= np (1− p ).
An alternative derivation makes use of independence. Recall from Section2.2that a
binomial random variable X can be represented as the sum of n independent Bernoulli
trials, each with success probability p. Such a Bernoulli trial Y has variance

E [( Y − E [ Y ])^2 ]= p (1− p )^2 +(1− p )(− p )^2 = p − p^2 = p (1− p ).
By Theorem3.5, the variance of X is then np (1− p ).

3.3. Chebyshev’s Inequality
Using the expectation and the variance of the random variable, one can derive a signif-
icantly stronger tail bound known as Chebyshev’s inequality.

Theorem 3.6 [ Chebyshev’s Inequality ] : For any a > 0 ,

Pr(| X − E [ X ]|≥ a )≤
Var [ X ]
a^2
.
moments and deviations
Proof: We first observe that

Pr(| X − E [ X ]|≥ a )=Pr(( X − E [ X ])^2 ≥ a^2 ).
Since ( X − E [ X ])^2 is a nonnegative random variable, we can apply Markov’s inequality
to prove:

Pr(( X − E [ X ])^2 ≥ a^2 )≤
E [( X − E [ X ])^2 ]
a^2
=
Var [ X ]
a^2
.
 /theta
The following useful variants of Chebyshev’s inequality bound the deviation of the ran-
dom variable from its expectation in terms of a constant factor of its standard deviation
or expectation.

Corollary 3.7: For any t > 1 ,

Pr(| X − E [ X ]|≥ t ·σ[ X ])≤
1
t^2
and
Pr(| X − E [ X ]|≥ t · E [ X ])≤
Var [ X ]
t^2 ( E [ X ])^2
.
Let us again consider our coin-flipping example, and this time use Chebyshev’s inequal-
ity to bound the probability of obtaining more than 3 n /4 heads in a sequence of n
fair coin flips. Recall that Xi =1 if the i th coin flip is heads and 0 otherwise, and
that X =

∑ n
i = 1 Xi denotes the number of heads in the n coin flips. To use Cheby-
shev’s inequality we need to compute the variance of X. Observe first that, since Xi is a
0–1 random variable,

E [( Xi )^2 ]= E [ Xi ]=
1
2
.
Thus,

Var [ Xi ]= E [( Xi )^2 ]−( E [ Xi ])^2 =
1
2
−
1
4
=
1
4
.
Now, since X =
∑ n
i = 1 Xi and the Xi are independent, we can use Theorem3.5to
compute

Var [ X ]= Var
[∑ n
i = 1
Xi
]
=
∑ n
i = 1
Var [ Xi ]=
n
4
.
Applying Chebyshev’s inequality then yields

Pr( X ≥ 3 n /4)≤Pr(| X − E [ X ]|≥ n /4)
≤
Var [ X ]
( n /4)^2
=
n / 4
( n /4)^2
=
4
n
.
3.3chebyshev’s inequality
In fact, we can do slightly better. Chebyshev’s inequality yields that 4/ n is actu-
ally a bound on the probability that X is either smaller than n /4 or larger than 3 n /4,
so by symmetry the probability that X is greater than 3 n /4 is actually 2/ n. Cheby-
shev’s inequality gives a significantly better bound than Markov’s inequality for large
n.

3.3.1 Example: Coupon Collector’s Problem
We apply Markov’s and Chebyshev’s inequalities to the coupon collector’s prob-
lem. Recall that the time∑ X to collect n coupons has expectation nHn , where Hn =
n
i = 11 / n =ln n + O (1). Hence Markov’s inequality yields

Pr( X ≥ 2 nHn )≤
1
2
.
To use Chebyshev’s inequality, we need to find the variance of X. Recall again from
Section2.4.1that X =

∑ n
i = 1 Xi , where the Xi are geometric random variables with
parameter ( n − i +1)/ n. In this case, the Xi are independent because the time to col-
lect the i th coupon does not depend on how long it took to collect the previous i − 1
coupons. Hence

Var [ X ]= Var
[∑ n
i = 1
Xi
]
=
∑ n
i = 1
Var [ Xi ],
so we need to find the variance of a geometric random variable.
Let Y be a geometric random variable with parameter p. As we saw in Section2.4,
E [ X ]= 1 / p. We calculate E [ Y^2 ]. The following trick proves useful. We know that, for
0 < x <1,

1
1 − x
=
∑∞
i = 0
xi.
Taking derivatives, we find:

1
(1− x )^2
=
∑∞
i = 0
ixi −^1
=
∑∞
i = 0
( i +1) xi ;
2
(1− x )^3
=
∑∞
i = 0
i ( i −1) xi −^2
=
∑∞
i = 0
( i +1)( i +2) xi.
moments and deviations
We can conclude that

∑∞
i = 1
i^2 xi =
∑∞
i = 0
i^2 xi
=
∑∞
i = 0
( i +1)( i +2) xi − 3
∑∞
i = 0
( i +1) xi +
∑∞
i = 0
xi
=
2
(1− x )^3
− 3
1
(1− x )^2
+
1
(1− x )
=
x^2 + x
(1− x )^3
.
We now use this to find
E [ Y^2 ]=
∑∞
i = 1
p (1− p ) i −^1 i^2
=
p
1 − p
∑∞
i = 1
(1− p ) ii^2
=
p
1 − p
(1− p )^2 +(1− p )
p^3
=
2 − p
p^2
.
Finally, we reach

Var [ Y ]= E [ Y^2 ]− E [ Y ]^2
=
2 − p
p^2
−
1
p^2
=
1 − p
p^2
.
We have just proven the following useful lemma.
Lemma 3.8: The variance of a geometric random variable with parameter p is
(1− p )/ p^2_._

For a geometric random variable Y , E [ Y^2 ] can also be derived using conditional expec-
tations. We use that Y corresponds to the number of flips until the first heads, where
each flip is heads with probability p. Let X =0 if the first flip is tails and X =1 if the
first flip is heads. By Lemma2.5,

E [ Y^2 ]=Pr( X =0) E [ Y^2 | X =0]+Pr( X =1) E [ Y^2 | X =1]
=(1− p ) E [ Y^2 | X =0]+ p E [ Y^2 | X =1].
3.4 Median and Mean
If X =1, then Y =1 and so E [ Y^2 | X =1]=1. If X =0, then Y >1. In this case,
let the number of remaining flips after the first flip until the first head be Z. Then

E [ Y^2 ]=(1− p ) E [( Z +1)^2 ]+ p · 1
=(1− p ) E [ Z^2 ]+2(1− p ) E [ Z ]+ 1 (3.2)
by the linearity of expectations. By the memoryless property of geometric random vari-
ables, Z is also a geometric random variable with parameter p. Hence E [ Z ]= 1 / p and
E [ Z^2 ]= E [ Y^2 ]. Plugging these values into Eqn. (3.2), we have

E [ Y^2 ]=(1− p ) E [ Y^2 ]+
2(1− p )
p
+ 1 =(1− p ) E [ Y^2 ]+
2 − p
p
,
which yields E [ Y^2 ]=(2− p )/ p^2 , matching our other derivation.
We return now to the question of the variance in the coupon collector’s problem.
We simplify the argument by using the upper bound Var [ Y ]≤ 1 / p^2 for a geometric
random variable, instead of the exact result of Lemma3.8. Then

Var [ X ]=
∑ n
i = 1
Var [ Xi ]≤
∑ n
i = 1
(
n
n − i + 1
) 2
= n^2
∑ n
i = 1
(
1
i
) 2
≤
π^2 n^2
6
.
Here we have used the identity

∑∞
i = 1
(
1
i
) 2
=
π^2
6
.
Now, by Chebyshev’s inequality,
Pr(| X − nHn |≥ nHn )≤
n^2 π^2 / 6
( nHn )^2
=
π^2
6( Hn )^2
= O
(
1
ln^2 n
)
.
In this case, Chebyshev’s inequality again gives a much better bound than Markov’s
inequality. But it is still a fairly weak bound, as we can see by considering instead a
fairly simple union bound argument.
Consider the probability of not obtaining the i th coupon after n ln n + cn steps. This
probability is
(
1 −

1
n
) n (ln n + c )
<e−(ln n + c )=
1
e cn
.
By a union bound, the probability that some coupon has not been collected after n ln n +
cn steps is only e− c. In particular, the probability that all coupons are not collected after
2 n ln n steps is at most 1/ n , a bound that is significantly better than what can be achieved
even with Chebyshev’s inequality.

3.4. Median and Mean
Let X be a random variable. The median of X is defined to be any value m such that

Pr( X ≤ m )≥ 1 /2 and Pr( X ≥ m )≥ 1 / 2.
moments and deviations
For example, for a discrete random variable that is uniformly distributed over an odd
number of distinct, sorted values x 1 , x 2 ,..., x 2 k + 1 , the median is the middle value xk + 1.
For a discrete random variable that is uniformly distributed over an even number of
values x 1 , x 2 ,..., x 2 k , any value in the range ( xk , xk + 1 ) would be a median.
The expectation E [ X ] and the median are usually different numbers. For distribu-
tions with a unique median that are symmetric around either the mean or median, the
median is equal to the mean. For some distributions, the median can be easier to work
with than the mean, and in some settings it is a more natural quantity to work with.
The following theorem gives an alternate characterization of the mean and median:

Theorem 3.9: For any random variable X with finite expectation E [ X ] and finite
median m,

1. the expectation E [ X ] is the value of c that minimizes the expression

E [( X − c )^2 ],
and
2. the median m is a value of c that minimizes the expression

E [| X − c |].
Proof: The first result follows from linearity of expectations.

E [( X − c )^2 ]= E [ X^2 ]− 2 c E [ X ]+ c^2 ,
and taking the derivative with respect to c shows that c = E [ X ] yields the minimum.
For the second result, we want to show that that for any value c that is not a
median and for any median m ,wehave E [| X − c |]> E [| X − m |], or equivalently that
E [| X − c |−| X − m |]>0. In that case the value of c that minimizes E [| X − c |] will
be a median. (In fact, as a by-product, we show that for any two medians m and m ′,
E [| X − m |]= E [| X − m ′|].)
Let us take the case where c > m for a median m , and c is not a median, so Pr( X ≥
c )< 1 /2. A similar argument holds for any value of c such that Pr( X ≤ c )< 1 /2.
For x ≥ c ,| x − c |−| x − m |= m − c .For m < x < c ,| x − c |−| x − m |= c + m −
2 x > m − c. Finally, for x ≤ m ,| x − c |−| x − m |= c − m. Combining the three cases,
we have

E [| X − c |−| X − m |]
=Pr( X ≥ c )( m − c )+
∑
x : m < x < c
Pr( X = x )( c + m − 2 x )+Pr( X ≤ m )( c − m ).
We now consider two cases. If Pr( m < X < c )=0, then

E [| X − c |−| X − m |]=Pr( X ≥ c )( m − c )+Pr( X ≤ m )( c − m )
>
1
2
( m − c )+
1
2
( c − m )
= 0 ,
3.5 Application: A Randomized Algorithm for Computing the Median
where the inequality comes from Pr( X ≥ c )< 1 /2 and m < c. (Note here that if c were
another median, so Pr( X ≥ c )= 1 /2, we would obtain E [| X − c |−| X − m |]=0, as
stated earlier.)
If Pr( m < X < c )=0, then
E [| X − c |−| X − m |]
=Pr( X ≥ c )( m − c )+

∑
x : m < x < c
Pr( X = x )( c + m − 2 x )+Pr( X ≤ m )( c − m )
>Pr( X > m )( m − c )+Pr( X ≤ m )( c − m )
>
1
2
( m − c )+
1
2
( c − m )
= 0 ,
where here the first inequality comes from c + m − 2 x > m − c for any value of x
with non-zero probability in the range m < x < c. (This case cannot hold if c and m
are both medians, as in this case we cannot have Pr( X ≥ m )= 1 /2 and Pr( X ≥ c )=
1 /2.)  /theta

Interestingly, for well-behaved random variables, the median and the mean cannot
deviate from each other too much.

Theorem 3.10: If X is a random variable with finite standard deviation σ , expectation
μ , and median m, then

|μ− m |≤σ.
Proof: The proof follows from the following sequence:

|μ− m |=| E [ X ]− m |
=| E [ X − m ]|
≤ E [| X − m |]
≤ E [| X −μ|]
≤
√
E [( X −μ)^2 ]
=σ.
Here the first inequality follows from Jensen’s inequality, the second inequality follows
from the result that the median minimizes E [| X − c |], and the third inequality is again
Jensen’s inequality.  /theta

In Exercise3.19, we suggest another way of proving this result.
3.5. Application: A Randomized Algorithm for Computing the
Median
Given a set S of n elements drawn from a totally ordered universe, the median of S is
an element m of S such that at least n / 2 elements in S are less than or equal to m and
at least n / 2 +1 elements in S are greater than or equal to m. If the elements in S are

moments and deviations
distinct, then m is the ( n / 2 )th element in the sorted order of S. Note that the median
of a set is similar to but slightly different from the the median of a random variable
defined in Section3.4.
The median can be easily found deterministically in O ( n log n ) steps by sorting,
and there is a relatively complex deterministic algorithm that computes the median in
O ( n ) time. Here we analyze a randomized linear time algorithm that is significantly
simpler than the deterministic one and yields a smaller constant factor in the linear
running time. To simplify the presentation, we assume that n is odd and that the ele-
ments in the input set S are distinct. The algorithm and analysis can be easily modified
to include the case of a multi-set S (see Exercise3.24) and a set with an even number of
elements.

3.5.1 The Algorithm
The main idea of the algorithm involves sampling, which we first discussed in Sec-
tion1.2. The goal is to find two elements that are close together in the sorted order of S
and that have the median lie between them. Specifically, we seek two elements d , u ∈ S
such that:

1. d ≤ m ≤ u (the median m is between d and u ); and
2. for C ={ s ∈ S : d ≤ s ≤ u }, | C |= o ( n /log n ) (the total number of elements
between d and u is small).

Sampling gives us a simple and efficient method for finding two such elements.
We claim that, once these two elements are identified, the median can easily be
found in linear time with the following steps. The algorithm counts (in linear time) the
number n d of elements of S that are smaller than d and then sorts (in sublinear, or o ( n ),
time) the set C. Notice that, since| C |= o ( n /log n ), the set C can be sorted in time o ( n )
using any standard sorting algorithm that requires O ( m log m ) time to sort m elements.
The ( n / 2 − n d +1)th element in the sorted order of C is m , since there are exactly
n / 2 elements in S that are smaller than that value ( n / 2 − n d in the set C and n d in
S − C ).
To find the elements d and u , we sample with replacement a multi-set R of n^3 /^4 
elements from S. Recall that sampling with replacement means each element in R is
chosen uniformly at random from the set S , independent of previous choices. Thus, the
same element of S might appear more than once in the multi-set R. Sampling without
replacement might give marginally better bounds, but both implementing and analyzing
it are significantly harder. It is worth noting that we assume that an element can be
sampled from S in constant time.
Since R is a random sample of S we expect m , the median element of S , to be close to
the median element of R. We therefore choose d and u to be elements of R surrounding
the median of R.
We require all the steps of our algorithm to work with high probability, by which we
mean with probability at least 1− O (1/ nc ) for some constant c >0. To guarantee that
with high probability the set C includes the median m ,wefix d and u to be respectively
the n^3 /^4 / 2 −

√
n th and the n^3 /^4 / 2 +
√
n th elements in the sorted order of R. With
3.5 application: a randomized algorithm for computing the median
Randomized Median Algorithm:
Input: A set S of n elements over a totally ordered universe.
Output: The median element of S , denoted by m.
1. Pick a (multi-)set R of n^3 /^4 elements in S , chosen independently and uniformly
at random with replacement.
2. Sort the set R.
3. Let d be the

(⌊ 1
2 n
3 / (^4) −√ n ⌋)th smallest element in the sorted set R.

4. Let u be the

(⌈ 1
2 n
3 / (^4) +√ n ⌉)th smallest element in the sorted set R.

5. By comparing every element in S to d and u , compute the set
C ={ x ∈ S : d ≤ x ≤ u }and the numbers n d =|{ x ∈ S : x < d }|and
 n u =|{ x ∈ S : x > u }|.
6. If n d > n /2or n u > n /2 then FAIL.
7. If| C |≤ 4 n^3 /^4 then sort the set C , otherwise FAIL.
8. Output the ( n / 2 − n d +1)th element in the sorted order of C.

Algorithm 3.1: Randomized median algorithm.
this choice, the set C includes all the elements of S that are between the 2

√
n sample
points surrounding the median of R. The analysis will clarify that the choice of the size
of R and the choices for d and u are tailored to guarantee both that (a) the set C is large
enough to include m with high probability and (b) the set C is sufficiently small so that
it can be sorted in sublinear time with high probability.
A formal description of the procedure is presented as Algorithm3.1. In what follows,
for convenience we treat

√
n and n^3 /^4 as integers.
3.5.2 Analysis of the Algorithm
Based on our previous discussion, we first prove that – regardless of the random choices
made throughout the procedure – the algorithm (a) always terminates in linear time and
(b) outputs either the correct result or FAIL.

Theorem 3.11: The randomized median algorithm terminates in linear time, and if it
does not output FAIL then it outputs the correct median element of the input set S.

Proof: Correctness follows because the algorithm could only give an incorrect answer
if the median were not in the set C. But then either n d > n /2or n u > n /2 and thus
step 6 of the algorithm guarantees that, in these cases, the algorithm outputs FAIL.
Similarly, as long as C is sufficiently small, the total work is only linear in the size of
S. Step 7 of the algorithm therefore guarantees that the algorithm does not take more
than linear time; if the sorting might take too long, the algorithm outputs FAIL without
sorting.  /theta

The interesting part of the analysis that remains after Theorem3.11is bounding the
probability that the algorithm outputs FAIL. We bound this probability by identifying

moments and deviations
three “bad” events such that, if none of these bad events occurs, the algorithm does not
fail. In a series of lemmas, we then bound the probability of each of these events and
show that the sum of these probabilities is only O ( n −^1 /^4 ).
Consider the following three events:

E 1 : Y 1 =|{ r ∈ R | r ≤ m }|<^12 n^3 /^4 −

√
n ;
E 2 : Y 2 =|{ r ∈ R | r ≥ m }|<^12 n^3 /^4 −

√
n ;
E 3 :| C |> 4 n^3 /^4.

Lemma 3.12: The randomized median algorithm fails if and only if at least one of E 1 ,
E 2 ,or E 3 occurs.

Proof: Failure in step 7 of the algorithm is equivalent to the eventE 3. Failure in step
6 of the algorithm occurs if and only if( n d > n /2or n u > n /2. But for n d > n /2, the
1
2 n

3 / (^4) −√ n )th smallest element of R must be larger than m ;this is equivalent to the
eventE 1. Similarly, n u > n /2 is equivalent to the eventE 2.  /theta
Lemma 3.13:
Pr(E 1 )≤

1
4
n −^1 /^4.
Proof: Define a random variable Xi by

Xi =
{
1 if the i th sample is less than or equal to the median,
0 otherwise.
The Xi are independent, since the sampling is done with replacement. Because there are
( n −1)/ 2 +1 elements in S that are less than or equal to the median, the probability
that a randomly chosen element of S is less than or equal to the median can be written as

Pr( Xi =1)=
( n −1)/ 2 + 1
n
=
1
2
+
1
2 n
.
The eventE 1 is equivalent to

Y 1 =
∑ n^3 /^4
i = 1
Xi <
1
2
n^3 /^4 −
√
n.
Since Y 1 is the sum of Bernoulli trials, it is a binomial random variable with para-
meters n^3 /^4 and 1/ 2 + 1 / 2 n. Hence, using the result of Section3.2.1yields

Var [ Y 1 ]= n^3 /^4
(
1
2
+
1
2 n
)(
1
2
−
1
2 n
)
=
1
4
n^3 /^4 −
1
4 n^5 /^4
<
1
4
n^3 /^4.
3.5 application: a randomized algorithm for computing the median
Applying Chebyshev’s inequality then yields
Pr(E 1 )=Pr
(
Y 1 <
1
2
n^3 /^4 −
√
n
)
≤Pr
(
| Y 1 − E [ Y 1 ]|>
√
n
)
≤
Var [ Y 1 ]
n
<
1
4 n
3 / 4
n
=
1
4
n −^1 /^4.  /theta
We similarly obtain the same bound for the probability of the eventE 2. We now bound
the probability of the third bad event,E 3.
Lemma 3.14:
Pr(E 3 )≤
1
2
n −^1 /^4.
Proof: IfE 3 occurs, so| C |> 4 n^3 /^4 , then at least one of the following two events occurs:
E 3 , 1 : at least 2 n^3 /^4 elements of C are greater than the median;
E 3 , 2 : at least 2 n^3 /^4 elements of C are smaller than the median.

Let us bound the probability that the first event occurs; the second will have the same
bound by symmetry. If there are at least 2 n^3 /^4 elements of C above the median, then
the order of u in the sorted order of S was at least^12 n + 2 n^3 /^4 and thus the set R has at
least^12 n^3 /^4 −
√
n samples among the^12 n − 2 n^3 /^4 largest elements in S.
Let X be the number of samples among the^12 n − 2 n^3 /^4 largest elements in S. Let
X =
∑ n 3 / 4
i = 1 Xi , where
Xi =
{
1 if the i th sample is among the^12 n − 2 n^3 /^4 largest elements in S ,
0 otherwise.
Again, X is a binomial random variable, and we find
E [ X ]=
1
2
n^3 /^4 − 2
√
n
and
Var [ X ]= n^3 /^4
(
1
2
− 2 n −^1 /^4
)(
1
2
+ 2 n −^1 /^4
)
=
1
4
n^3 /^4 − 4 n^1 /^4 <
1
4
n^3 /^4.
Applying Chebyshev’s inequality yields
Pr(E 3 , 1 )=Pr
(
X ≥^12 n^3 /^4 −
√
n
)
(3.3)
≤Pr
(
| X − E [ X ]|≥
√
n
)
≤
Var [ X ]
n
<
1
4 n
3 / 4
n
=
1
4
n −^1 /^4. (3.4)
Similarly,
Pr(E 3 , 2 )≤
1
4
n −^1 /^4
moments and deviations
and

Pr(E 3 )≤Pr(E 3 , 1 )+Pr(E 3 , 2 )≤
1
2
n −^1 /^4.
 /theta
Combining the bounds just derived, we conclude that the probability that the algorithm
outputs FAIL is bounded by

Pr(E 1 )+Pr(E 2 )+Pr(E 3 )≤ n −^1 /^4.
This yields the following theorem.

Theorem 3.15: The probability that the randomized median algorithm fails is
bounded by n −^1 /^4_._

By repeating Algorithm3.1until it succeeds in finding the median, we can obtain an
iterative algorithm that never fails but has a random running time. The samples taken
in successive runs of the algorithm are independent, so the success of each run is inde-
pendent of other runs, and hence the number of runs until success is achieved is a
geometric random variable. As an exercise, you may wish to show that this variation
of the algorithm (that runs until it finds a solution) still has linear expected running
time.
Randomized algorithms that may fail or return an incorrect answer are called Monte
Carlo algorithms. The running time of a Monte Carlo algorithm often does not depend
on the random choices made. For example, we showed in Theorem3.11that the ran-
domized median algorithm always terminates in linear time, regardless of its random
choices.
A randomized algorithm that always returns the right answer is called a Las Vegas
algorithm. We have seen that the Monte Carlo randomized algorithm for the median can
be turned into a Las Vegas algorithm by running it repeatedly until it succeeds. Again,
turning it into a Las Vegas algorithm means the running time is variable, although the
expected running time is still linear.

3.6 Exercises
Exercise 3.1: Let X be a number chosen uniformly at random from [1, n ]. Find Var [ X ].

Exercise 3.2: Let X be a number chosen uniformly at random from [− k , k ]. Find
Var [ X ].

Exercise 3.3: Suppose that we roll a standard fair die 100 times. Let X be the sum
of the numbers that appear over the 100 rolls. Use Chebyshev’s inequality to bound
Pr(| X − 350 |≥50).

Exercise 3.4: Prove that, for any real number c and any discrete random variable X ,
Var [ cX ]= c^2 Var [ X ].

3.6 exercises
Exercise 3.5: Given any two random variables X and Y , by the linearity of expecta-
tions we have E [ X − Y ]= E [ X ]− E [ Y ]. Prove that, when X and Y are independent,
Var [ X − Y ]= Var [ X ]+ Var [ Y ].

Exercise 3.6: For a coin that comes up heads independently with probability p on each
flip, what is the variance in the number of flips until the k th head appears?

Exercise 3.7: A simple model of the stock market suggests that, each day, a stock with
price q will increase by a factor r >1to qr with probability p and will fall to q / r with
probability 1− p. Assuming we start with a stock with price 1, find a formula for the
expected value and the variance of the price of the stock after d days.

Exercise 3.8: Suppose that we have an algorithm that takes as input a string of n
bits. We are told that the expected running time is O ( n^2 ) if the input bits are chosen
independently and uniformly at random. What can Markov’s inequality tell us about
the worst-case running time of this algorithm on inputs of size n?

Exercise 3.9: (a) Let X be the sum of Bernoulli random variables, X =

∑ n
i = 1 Xi. The
Xi do not need to be independent. Show that

E [ X^2 ]=
∑ n
i = 1
Pr( Xi =1) E [ X | Xi =1]. (3.5)
Hint: Start by showing that

E [ X^2 ]=
∑ n
i = 1
E [ XiX ],
and then apply conditional expectations.
(b) Use Eqn. (3.5) to provide another derivation for the variance of a binomial ran-
dom variable with parameters n and p.

Exercise 3.10: For a geometric random variable X ,find E [ X^3 ] and E [ X^4 ]. ( Hint: Use
Lemma2.5.)

Exercise 3.11: Recall the Bubblesort algorithm of Exercise2.22. Determine the vari-
ance of the number of inversions that need to be corrected by Bubblesort.

Exercise 3.12: Find an example of a random variable with finite expectation and
unbounded variance. Give a clear argument showing that your choice has these
properties.

Exercise 3.13: Find an example of a random variable with finite j th moments for
1 ≤ j ≤ k but an unbounded ( k +1)th moment. Give a clear argument showing that
your choice has these properties.

moments and deviations
Exercise 3.14: Prove that, for any finite collection of random variables X 1 , X 2 ,..., Xn ,

Var
[∑ n
i = 1
Xi
]
=
∑ n
i = 1
Var [ Xi ]+ 2
∑ n
i = 1
∑
j > i
Cov ( Xi , Xj ).
Exercise 3.15: Let the random variable X be representable as a sum of random vari-
ables X =

∑ n
i = 1 Xi. Show that, if E [ XiXj ]= E [ Xi ] E [ Xj ] for every pair of i and j with
1 ≤ i < j ≤ n , then Var [ X ]=

∑ n
i = 1 Var [ Xi ].
Exercise 3.16: This problem shows that Markov’s inequality is as tight as it could
possibly be. Given a positive integer k , describe a random variable X that assumes only
nonnegative values such that

Pr( X ≥ k E [ X ])=
1
k
.
Exercise 3.17: Can you give an example (similar to that for Markov’s inequality in
Exercise3.16) that shows that Chebyshev’s inequality is tight? If not, explain why not.

Exercise 3.18: Show that, for a random variable X with standard deviationσ[ X ] and
any positive real number t :

(a) Pr( X − E [ X ]≥ t σ[ X ])≤
1
1 + t^2
;
(b) Pr(| X − E [ X ]|≥ t σ[ X ])≤

2
1 + t^2
.
Exercise 3.19: Using Exercise3.18, show that|μ− m |≤σfor a random variable
with finite standard deviationσ, expectationμ, and median m.

Exercise 3.20: Let Y be a nonnegative integer-valued random variable with positive
expectation. Prove

E [ Y ]^2
E [ Y^2 ]
≤Pr[ Y =0]≤ E [ Y ].
Exercise 3.21: (a) Chebyshev’s inequality uses the variance of a random variable to
bound its deviation from its expectation. We can also use higher moments. Suppose
that we have a random variable X and an even integer k for which E [( X − E [ X ]) k ]is
finite. Show that

Pr
(
| X − E [ X ]|> tk
√
E [( X − E [ X ]) k ]
)
≤
1
tk
.
(b) Why is it difficult to derive a similar inequality when k is odd?
Exercise 3.22: A fixed point of a permutationπ[1, n ]→[1, n ] is a value for which
π( x )= x. Find the variance in the number of fixed points of a permutation chosen uni-
formly at random from all permutations. ( Hint: Let Xi be 1 ifπ( i )= i , so that

∑ n
i = 1 Xi
3.6 exercises
is the number of fixed points. You cannot use linearity to find Var

[∑ n
i = 1 Xi
]
, but you
can calculate it directly.)

Exercise 3.23: Suppose that we flip a fair coin n times to obtain n random bits. Con-
sider all m =

( n
2
)
pairs of these bits in some order. Let Yi be the exclusive-or of the i th
pair of bits, and let Y =

∑ m
i = 1 Yi be the number of Yi that equal 1.
(a) Show that each Yi is 0 with probability 1/2 and 1 with probability 1/2.
(b) Show that the Yi are not mutually independent.
(c) Show that the Yi satisfy the property that E [ YiYj ]= E [ Yi ] E [ Yj ].
(d) Using Exercise3.15,find Var [ Y ].
(e) Using Chebyshev’s inequality, prove a bound on Pr(| Y − E [ Y ]|≥ n ).

Exercise 3.24: Generalize the median-finding algorithm for the case where the input
S is a multi-set. Bound the error probability and the running time of the resulting
algorithm.

Exercise 3.25: Generalize the median-finding algorithm to find the k th largest item in
a set of n items for any given value of k. Prove that your resulting algorithm is correct,
and bound its running time.

Exercise 3.26: The weak law of large numbers states that, if X 1 , X 2 , X 3 ,...are inde-
pendent and identically distributed random variables with meanμand standard devia-
tionσ, then for any constantε>0wehave

lim
n →∞
Pr
(∣∣
∣∣ X^1 + X^2 +···+ Xn
n
−μ
∣
∣∣
∣>ε
)
= 0.
Use Chebyshev’s inequality to prove the weak law of large numbers.

chapter four

Chernoff and Hoeffding Bounds

This chapter introduces large deviation bounds commonly called Chernoff and
Hoeffding bounds. These bounds are extremely powerful, giving exponentially
decreasing bounds on the tail distribution. These bounds are derived by applying
Markov’s inequality to the moment generating function of a random variable. We start
this chapter by defining and discussing the properties of the moment generating func-
tion. We then derive Chernoff bounds for the binomial distribution and other related
distributions, using a set balancing problem as an example, and the Hoeffding bound
for sums of bounded random variables. To demonstrate the power of Chernoff bounds,
we apply them to the analysis of randomized packet routing schemes on the hypercube
and butterfly networks.

4.1 Moment Generating Functions
Before developing Chernoff bounds, we discuss the special role of the moment gener-
ating function E [e tX ].

Definition 4.1: The moment generating function of a random variable X is

MX ( t )= E [e tX ].
We are mainly interested in the existence and properties of this function in the neigh-
borhood of zero.
The function MX ( t ) captures all of the moments of X.

Theorem 4.1: Let X be a random variable with moment generating function MX ( t ).
Under the assumption that exchanging the expectation and differentiation operands is
legitimate, for all n > 1 we then have

E [ Xn ]= M ( Xn )(0),
where M ( Xn )(0) is the n th derivative of MX ( t ) evaluated at t = 0_._

4.1 moment generating functions
Proof: Assuming that we can exchange the expectation and differentiation operands,
then

MX ( n )( t )= E [ Xn e tX ].
Computed at t =0, this expression yields

M ( Xn )(0)= E [ Xn ].  /theta
The assumption that expectation and differentiation operands can be exchanged holds
whenever the moment generating function exists in a neighborhood of zero, which will
be the case for all distributions considered in this book.
As a specific example, consider a geometric random variable X with parameter p ,as
in Definition2.8. Then, for t <−ln(1− p ),

MX ( t )= E [e tX ]
=
∑∞
k = 1
(1− p ) k −^1 p e tk
=
p
1 − p
∑∞
k = 1
(1− p ) k e tk
=
p
1 − p
((1−(1− p )e t )−^1 −1).
It follows that

M (1) X ( t )= p (1−(1− p )e t )−^2 e t and
M (2) X ( t )= 2 p (1− p )(1−(1− p )e t )−^3 e^2 t + p (1−(1− p )e t )−^2 e t.
Evaluating these derivatives at t =0 and using Theorem 4.1 gives E [ X ]= 1 / p
and E [ X^2 ]=(2− p )/ p^2 , matching our previous calculations from Section2.4and
Section3.3.1.
Another useful property is that the moment generating function of a random variable
(or, equivalently, all of the moments of the variable) uniquely defines its distribution.
However, the proof of the following theorem is beyond the scope of this book.

Theorem 4.2: Let X and Y be two random variables. If

MX ( t )= MY ( t )
for all t ∈(−δ, δ) for some δ> 0 , then X and Y have the same distribution.

One application of Theorem4.2is in determining the distribution of a sum of indepen-
dent random variables.

Theorem 4.3: If X and Y are independent random variables, then

MX + Y ( t )= MX ( t ) MY ( t ).
Proof:

MX + Y ( t )= E [e t ( X + Y )]= E [e tX e tY ]= E [e tX ] E [e tY ]= MX ( t ) MY ( t ).
chernoff and hoeffding bounds
Here we have used that X and Y are independent – and hence e tX and e tY are indepen-
dent – to conclude that E [e tX e tY ]= E [e tX ] E [e tY ].  /theta

Thus, if we know MX ( t ) and MY ( t ) and if we recognize the function MX ( t ) MY ( t )as
the moment generating function of a known distribution, then that must be the distribu-
tion of X + Y when Theorem4.2applies. We will see examples of this in subsequent
sections and in the exercises.

4.2 Deriving and Applying Chernoff Bounds
The Chernoff bound for a random variable X is obtained by applying Markov’s inequal-
ity to e tX for some well-chosen value t. From Markov’s inequality, we can derive the
following useful inequality: for any t >0,

Pr( X ≥ a )=Pr(e tX ≥e ta )≤
E [e tX ]
e ta
In particular,

Pr( X ≥ a )≤min
t > 0
E [e tX ]
e ta
Similarly, for any t <0,

Pr( X ≤ a )=Pr(e tX ≥e ta )≤
E [e tX ]
e ta
Hence

Pr( X ≤ a )≤min
t < 0
E [e tX ]
e ta
Bounds for specific distributions are obtained by choosing appropriate values for t.
While the value of t that minimizes E [e tX ]/e ta gives the best possible bounds, often one
chooses a value of t that gives a convenient form. Bounds derived from this approach
are generally referred to collectively as Chernoff bounds. When we speak of a Chernoff
bound for a random variable, it could actually be one of many bounds derived in this
fashion.

4.2.1 Chernoff Bounds for the Sum of Poisson Trials
We now develop the most commonly used version of the Chernoff bound: for the tail
distribution of a sum of independent 0–1 random variables, which are also known as
Poisson trials. (Poisson trials differ from Poisson random variables, which will be dis-
cussed in Section5.3.) The distributions of the random variables in Poisson trials are
not necessarily identical. Bernoulli trials are a special case of Poisson trials where the
independent 0–1 random variables have the same distribution; in other words, all trials
are Poisson trials that take on the value 1 with the same probability. Also recall that
the binomial distribution gives the number of successes in n independent Bernoulli

4.2deriving and applying chernoff bounds
trials. Our Chernoff bound will hold for the binomial distribution and also for the more
general setting of the sum of Poisson trials.
Let X 1 ,..., Xn be a sequence of independent Poisson trials with Pr( Xi =1)= pi.
Let X =

∑ n
i = 1 Xi , and let
μ= E [ X ]= E
[ n
∑
i = 1
Xi
]
=
∑ n
i = 1
E [ Xi ]=
∑ n
i = 1
pi.
For a givenδ>0, we are interested in bounds on Pr( X ≥(1+δ)μ) and Pr( X ≤
(1−δ)μ) – that is, the probability that X deviates from its expectationμbyδμor more.
To develop a Chernoff bound we need to compute the moment generating function of
X. We start with the moment generating function of each Xi :

MXi ( t )= E [e tXi ]
= pi e t +(1− pi )
= 1 + pi (e t −1)
≤e pi (e
t −1)
,
where in the last inequality we have used the fact that, for any y ,1+ y ≤e y. Applying
Theorem4.3, we take the product of the n generating functions to obtain

MX ( t )=
∏ n
i = 1
MXi ( t )
≤
∏ n
i = 1
e pi (e
t −1)
=exp
{ n
∑
i = 1
pi (e t −1)
}
=e(e
t −1)μ
.
Now that we have determined a bound on the moment generating function, we are
ready to develop concrete versions of the Chernoff bound for a sum of Poisson trials.
We start with bounds on the deviation above the mean.

Theorem 4.4: LetX 1 ,..., XnbeindependentPoissontrialssuchthat Pr( Xi =1)= pi.
Let X =

∑ n
i = 1 Xiand μ= E [ X ]. Then the following Chernoff bounds hold:
1. for any δ> 0 ,

Pr( X ≥(1+δ)μ)≤
(
eδ
(1+δ)(1+δ)
)μ
; (4.1)
2. for 0 <δ≤ 1 ,

Pr( X ≥(1+δ)μ)≤e−μδ^2 /^3 ; (4.2)
3. for R ≥ 6 μ ,

Pr( X ≥ R )≤ 2 − R. (4.3)
chernoff and hoeffding bounds
The first bound of the theorem is the strongest, and it is from this bound that we derive
the other two bounds, which have the advantage of being easier to state and compute
with in many situations.

Proof: Applying Markov’s inequality, for any t >0wehave

Pr( X ≥(1+δ)μ)=Pr(e tX ≥e t (1+δ)μ)
≤
E [e tX ]
e t (1+δ)μ
≤
e(e t −1)μ
e t (1+δ)μ
.
For anyδ>0, we can set t =ln(1+δ)>0 to get Eqn. (4.1):
Pr( X ≥(1+δ)μ)≤
(
eδ
(1+δ)(1+δ)
)μ
.
To obtain Eqn. (4.2) we need to show that, for 0<δ≤1,

eδ
(1+δ)(1+δ)
≤e−δ
(^2) / 3
.
Taking the logarithm of both sides, we obtain the equivalent condition
f (δ)=δ−(1+δ)ln(1+δ)+
δ^2
3

≤ 0.
Computing the derivatives of f (δ), we have:

f ′(δ)= 1 −
1 +δ
1 +δ
−ln(1+δ)+
2
3
δ
=−ln(1+δ)+
2
3
δ;
f ′′(δ)=−
1
1 +δ
+
2
3
.
We see that f ′′(δ)<0for0≤δ< 1 /2 and that f ′′(δ)>0forδ> 1 /2. Hence f ′(δ)
first decreases and then increases over the interval [0,1]. Since f ′(0)=0 and f ′(1)<
0, we can conclude that f ′(δ)≤0 in the interval [0,1]. Since f (0)=0, it follows that
f (δ)≤0 in that interval, proving Eqn. (4.2).
To prove Eqn. (4.3), let R =(1+δ)μ. Then, for R ≥ 6 μ,δ= R /μ− 1 ≥5. Hence,
using Eqn. (4.1),

Pr( X ≥(1+δ)μ)≤
(
eδ
(1+δ)(1+δ)
)μ
≤
(
e
1 +δ
)(1+δ)μ
≤
(e
6
) R
≤ 2 − R.  /theta
4.2deriving and applying chernoff bounds
We obtain similar results bounding the deviation below the mean.

Theorem 4.5: LetX 1 ,..., XnbeindependentPoissontrialssuchthat Pr( Xi =1)= pi.
Let X =

∑ n
i = 1 Xiand μ= E [ X ]. Then, for^0 <δ<^1 :
1. Pr( X ≤(1−δ)μ)≤

(
e−δ
(1−δ)(1−δ)
)μ
; (4.4)
2. Pr( X ≤(1−δ)μ)≤e−μδ^2 /^2. (4.5)

Again, the bound of Eqn. (4.4) is stronger than Eqn. (4.5), but the latter is generally
easier to use and sufficient in most applications.

Proof: Using Markov’s inequality, for any t <0wehave

Pr( X ≤(1−δ)μ)=Pr(e tX ≥e t (1−δ)μ)
≤
E [e tX ]
e t (1−δ)μ
≤
e(e t −1)μ
e t (1−δ)μ
.
For 0<δ<1, we set t =ln(1−δ)<0 to get Eqn. (4.4):

Pr( X ≤(1−δ)μ)≤
(
e−δ
(1−δ)(1−δ)
)μ
.
To prove Eqn. (4.5) we must show that, for 0<δ<1,
e−δ
(1−δ)(1−δ)
≤e−δ
(^2) / 2
.
Taking the logarithm of both sides, we obtain the equivalent condition
f (δ)=−δ−(1−δ)ln(1−δ)+
δ^2
2

≤ 0
for 0<δ< 1.
Differentiating f (δ) yields
f ′(δ)=ln(1−δ)+δ,

f ′′(δ)=−
1
1 −δ
+ 1.
Since f ′′(δ)<0 in the range (0,1) and since f ′(0)=0, we have f ′(δ)≤0 in the range
[0,1). Therefore, f (δ) is nonincreasing in that interval. Since f (0)=0, it follows that
f (δ)≤0 when 0<δ<1, as required.  /theta

Often the following form of the Chernoff bound, which is derived immediately from
Eqn. (4.2) and Eqn. (4.4), is used.

Corollary 4.6: Let X 1 ,..., Xnbe independent Poisson trials such that Pr( Xi =1)=
pi.LetX =

∑ n
i = 1 Xiand μ= E [ X ] .For^0 <δ<^1 ,
Pr(| X −μ|≥δμ)≤2e−μδ
(^2) / 3

. (4.6)

chernoff and hoeffding bounds
In practice we often do not have the exact value of E [ X ]. Instead we can useμ≥ E [ X ]
in Theorem4.4andμ≤ E [ X ] in Theorem4.5(see Exercise4.7).

4.2.2 Example: Coin Flips
Let X be the number of heads in a sequence of n independent fair coin flips. Applying
the Chernoff bound of Eqn. (4.6), we have

Pr
(∣
∣
∣ X −
n
2
∣∣
∣≥
1
2
√
6 n ln n
)
≤2exp
{
−
1
3
n
2
6ln n
n
}
=
2
n
.
This demonstrates that the concentration of the number of heads around the mean
n /2 is very tight; most of the time, the deviations from the mean are on the order of
O

(√
n ln n
)
.
To compare the power of this bound to Chebyshev’s bound, consider the probability
of having no more than n /4 heads or no fewer than 3 n /4 heads in a sequence of n
independent fair coin flips. In the previous chapter, we used Chebyshev’s inequality to
show that

Pr
(∣∣
∣ X −
n
2
∣∣
∣≥
n
4
)
≤
4
n
.
Already, this bound is worse than the Chernoff bound just calculated for a significantly
larger event! Using the Chernoff bound in this case, we find that

Pr
(∣∣
∣ X −
n
2
∣∣
∣≥
n
4
)
≤2exp
{
−
1
3
n
2
1
4
}
≤2e− n /^24.
Thus, Chernoff’s technique gives a bound that is exponentially smaller than the bound
obtained using Chebyshev’s inequality.

4.2.3 Application: Estimating a Parameter
Suppose that we are interested in evaluating the probability that a particular gene muta-
tion occurs in the population. Given a DNA sample, a lab test can determine if it carries
the mutation. However, the test is expensive and we would like to obtain a relatively
reliable estimate from a small number of samples.
Let p be the unknown value that we are trying to estimate. Assume that we have
n samples and that X = pn ̃ of these samples have the mutation. Given a sufficiently
large number of samples, we expect the value p to be close to the sampled value ̃ p .We
express this intuition using the concept of a confidence interval.

Definition 4.2: A 1 −γ confidence interval for a parameter p is an interval [ ̃ p −
δ, p ̃+δ] such that

Pr( p ∈[ ̃ p −δ, p ̃+δ])≥ 1 −γ.
4.3 Better Bounds for Some Special Cases
Notice that, instead of predicting a single value for the parameter, we give an interval
that is likely to contain the parameter. If p can take on any real value, it may not make
sense to try to pin down its exact value from a finite sample, but it does make sense to
estimate it within some small range.
Naturally we want both the interval size 2δand the error probabilityγ to be as
small as possible. We derive a trade-off between these two parameters and the number
of samples n. In particular, given that among n samples (chosen uniformly at random
from the entire population) we find the mutation in exactly X = pn ̃ samples, we need
to find values ofδandγfor which

Pr( p ∈[ ̃ p −δ, p ̃+δ])=Pr( np ∈[ n ( ̃ p −δ), n ( ̃ p +δ)])≥ 1 −γ.
Now X = np ̃has a binomial distribution with parameters n and p ,so E [ X ]= np .If
p ∈/[ ̃ p −δ, p ̃+δ] then we have one of the following two events:

1. if p < p ̃−δ, then X = np ̃> n ( p +δ)= E [ X ](1+δ/ p );
2. if p > p ̃+δ, then np ̃< n ( p −δ)= E [ X ](1−δ/ p ).

We can apply the Chernoff bounds in Eqns. (4.2) and (4.5) to compute

Pr( p ∈/[ ̃ p −δ, p ̃+δ])=Pr
(
X < np
(
1 −
δ
p
))
+Pr
(
X > np
(
1 +
δ
p
))
(4.7)
<e− np (δ/ p )
(^2) / 2
+e− np (δ/ p )
(^2) / 3
(4.8)
=e− n δ
(^2) / 2 p
+e− n δ
(^2) / 3 p

. (4.9)

The bound given in Eqn. (4.9) is not useful because the value of p is unknown. A
simple solution is to use the fact that p ≤1, yielding

Pr( p ∈/[ ̃ p −δ, p ̃+δ])<e− n δ
(^2) / 2
+e− n δ
(^2) / 3
.
Settingγ=e− n δ^2 /^2 +e− n δ^2 /^3 , we obtain a trade-off betweenδ, n , and the error proba-
bilityγ.
We can apply other Chernoff bounds, such as those in Exercises4.13and4.16,to
obtain better bounds. We return to the subject of parameter estimation when we discuss
the Monte Carlo method in Chapter 11.

4.3. Better Bounds for Some Special Cases
We can obtain stronger bounds using a simpler proof technique for some special cases
of symmetric random variables.
We consider first the sum of independent random variables when each variable
assumes the value 1 or−1 with equal probability.

Theorem 4.7: Let X 1 ,..., Xnbe independent random variables with

Pr( Xi =1)=Pr( Xi =−1)=
1
2
.
chernoff and hoeffding bounds
Let X =

∑ n
i = 1 Xi. For any a >^0 ,
Pr( X ≥ a )≤e− a
(^2) / 2 n
.
Proof: For any t >0,
E [e tXi ]=

1
2
e t +
1
2
e− t.
To estimate E [e tXi ], we observe that

e t = 1 + t +
t^2
2!
+···+
ti
i!
+···
and

e− t = 1 − t +
t^2
2!
+···+(−1) i
ti
i!
+···,
using the Taylor series expansion for e t. Thus,

E [e tXi ]=
1
2
e t +
1
2
e− t
=
∑
i ≥ 0
t^2 i
(2 i )!
≤
∑
i ≥ 0
( t^2 /2) i
i!
=e t
(^2) / 2
.
Using this estimate yields
E [e tX ]=
∏ n
i = 1
E [e tXi ]≤e t
(^2) n / 2
and
Pr( X ≥ a )=Pr(e tX ≥e ta )≤
E [e tX ]
e ta
≤e t
(^2) n / 2 − ta
.
Setting t = a / n , we obtain
Pr( X ≥ a )≤e− a
(^2) / 2 n

.  /theta

By symmetry we also have

Pr( X ≤− a )≤e− a
(^2) / 2 n
.
Combining the two results yields our next corollary.
Corollary 4.8: Let X 1 ,..., Xnbe independent random variables with
Pr( Xi =1)=Pr( Xi =−1)=

1
2
.
4.3better bounds for some special cases
Let X =

∑ n
i = 1 Xi. Then, for any a >^0 ,
Pr(| X |≥ a )≤2e− a
(^2) / 2 n
.
Applying the transformation Yi =( Xi +1)/2 allows us to prove the following.
Corollary 4.9: Let Y 1 ,..., Ynbe independent random variables with
Pr( Yi =1)=Pr( Yi =0)=

1
2
.
Let Y =

∑ n
i = 1 Yiand μ= E [ Y ]= n /^2.
1. For any a > 0 ,

Pr( Y ≥μ+ a )≤e−^2 a
(^2) / n
.

2. For any δ> 0 ,

Pr( Y ≥(1+δ)μ)≤e−δ
(^2) μ

. (4.10)

Proof: Using the notation of Theorem4.7,wehave

Y =
∑ n
i = 1
Yi =
1
2
( n
∑
i = 1
Xi
)
+
n
2
=
1
2
X +μ.
Applying Theorem4.7yields

Pr( Y ≥μ+ a )=Pr( X ≥ 2 a )≤e−^4 a
(^2) / 2 n
,
proving the first part of the corollary. The second part follows from setting a =δμ=
δ n /2. Again applying Theorem4.7,wehave
Pr( Y ≥(1+δ)μ)=Pr( X ≥ 2 δμ)≤e−^2 δ
(^2) μ (^2) / n
=e−δ
(^2) μ

.  /theta

Note that the constant in the exponent of the bound of Eqn. (4.10) is 1 instead of the
1/3 in the bound of Eqn. (4.2).
Similarly, we have the following result.

Corollary 4.10: Let Y 1 ,..., Ynbe independent random variables with

Pr( Yi =1)=Pr( Yi =0)=
1
2
.
Let Y =

∑ n
i = 1 Yiand μ= E [ Y ]= n /^2.
1. For any 0 < a <μ ,

Pr( Y ≤μ− a )≤e−^2 a
(^2) / n
.

2. For any 0 <δ< 1 ,

Pr( Y ≤(1−δ)μ)≤e−δ
(^2) μ

. (4.11)

chernoff and hoeffding bounds
4.4 Application: Set Balancing
Given an n × m matrix A with entries in{ 0 , 1 }, let

⎛
⎜
⎜⎜
⎜
⎝
a 11 a 12 ··· a 1 m
a 21 a 22 ··· a 2 m
..
.
..
.
... ..
.
an 1 an 2 ··· anm
⎞
⎟
⎟⎟
⎟
⎠
⎛
⎜
⎜⎜
⎜
⎝
b 1
b 2
..
.
bm
⎞
⎟
⎟⎟
⎟
⎠
=
⎛
⎜
⎜⎜
⎜
⎝
c 1
c 2
..
.
cn
⎞
⎟
⎟⎟
⎟
⎠
.
Suppose that we are looking for a vector b ̄with entries in{− 1 , 1 }that minimizes
‖ A b ̄‖∞= max
i = 1 ,..., n
| ci |.
This problem arises in designing statistical experiments. Each column of the matrix A
represents a subject in the experiment and each row represents a feature. The vector
b ̄partitions the subjects into two disjoint groups, so that each feature is roughly as
balanced as possible between the two groups. One of the groups serves as a control
group for an experiment that is run on the other group.
Our randomized algorithm for computing a vector b ̄is extremely simple. We ran-
domly choose the entries of b ̄, with Pr( bi =1)=Pr( bi =−1)= 1 /2. The choices
for different entries are independent. Surprisingly, although this algorithm ignores the
entries of the matrix A , the following theorem shows that‖ A ̄ b ‖∞is likely to be only
O

(√
m ln n
)
. This bound is fairly tight. In Exercise4.15you are asked to show that,
when m = n , there exists a matrix A for which‖ A ̄ b ‖∞is m

(√
n
)
for any choice of ̄ b.
Theorem 4.11: For a random vectorb with entries chosen independently and with ̄
equal probability from the set {− 1 , 1 } ,

Pr
(
‖ A b ̄‖∞≥
√
4 m ln n
)
≤
2
n
.
Proof: Consider the i th row ̄ ai = ai , 1 ,..., ai , m , and let k be the number of 1s in that
row. If k ≤

√
4 m ln n , then clearly| a ̄ i · b ̄|=| ci |≤
√
√^4 m ln n. On the other hand, if k >
4 m ln n then we note that the k nonzero terms in the sum

Zi =
∑ m
j = 1
ai , jbj
are independent random variables, each with probability 1/2 of being either+1or−1.
Now using the Chernoff bound of Corollary4.8and the fact that m ≥ k ,

Pr
(
| Zi |>
√
4 m ln n
)
≤2e−^4 m ln n /^2 k ≤
2
n^2
.
By the union bound, the probability that the bound fails for any row is at most
2 / n.  /theta

4.5 The Hoeffding Bound
4.5. The Hoeffding Bound
Hoeffding’s bound extends the Chernoff bound technique to general random variables
with a bounded range.

Theorem 4.12 [ Hoeffding Bound ] : Let X 1 ,..., Xnbe independent random variables
such that for all 1 ≤ i ≤ n, E [ Xi ]=μ and Pr( a ≤ Xi ≤ b )= 1_. Then_

Pr
(∣∣
∣∣
∣
1
n
∑ n
i = 1
Xi −μ
∣∣
∣∣
∣
≥ /theta
)
≤2e−^2 n  /theta
(^2) /( b − a ) 2
.
Proof: The proof relies on the following bound for the moment generating function,
which we prove first.
Lemma 4.13 [ Hoeffding’s Lemma ] : Let X be a random variable such that
Pr( X ∈[ a , b ])= 1 and E [ X ]= 0_. Then for every_ λ> 0 ,
E [eλ X ]≤eλ
(^2) ( b − a ) (^2) / 8
.
Proof: Before beginning, note that since E [ X ]=0, if a =0 then b =0 and the state-
ment is trivial. Hence we may assume a <0 and b >0.
Since f ( x )=eλ x is a convex function, for anyα∈(0,1),
f (α a +(1−α) b )≤αeλ a +(1−α)eλ b.
For x ∈[ a , b ], letα= bb −− xa ; then x =α a +(1−α) b and we have
eλ x ≤
b − x
b − a
eλ a +
x − a
b − a
eλ b.
We consider eλ X and take expectations. Using the fact that E [ X ]=0, we have
E [eλ X ]≤ E

[
b − X
b − a
eλ a
]
+ E
[
X − a
b − a
eλ b
]
=
b
b − a
eλ a −
E [ X ]
b − a
eλ a −
a
b − a
eλ b +
E [ X ]
b − a
eλ b
=
b
b − a
eλ a −
a
b − a
eλ b.
We now require some manipulation of this final expression. Letφ( t )=−θ t +
ln(1−θ+θe t ), forθ= b −− aa >0. Then

eφ(λ( b − a ))=e−θλ( b − a )(1−θ+θeλ( b − a ))
=eλ a (1−θ+θeλ( b − a ))
=eλ a
(
b
b − a
−
a
b − a
eλ( b − a )
)
=
b
b − a
eλ a −
a
b − a
eλ b ,
which equals the upper bound we derived for E [eλ X ]. It is not hard to verify thatφ(0)=
φ′(0)=0, andφ′′( t )≤ 1 /4 for all t. By Taylor’s theorem, for any t >0 there is a

chernoff and hoeffding bounds
t ′∈[0, t ] such that

φ( t )=φ(0)+ t φ′(0)+
1
2
t^2 φ′′( t ′)≤
1
8
t^2.
Thus, for t =λ( b − a ), we have

φ(λ( b − a ))≤
λ^2 ( b − a )^2
8
.
It follows that
E [eλ X ]≤eφ(λ( b − a ))≤eλ
(^2) ( b − a ) (^2) / 8

.  /theta

We now return to the proof of Theorem 4.12. Let Zi = Xi − E [ Xi ] and Z =
1
n
∑ n
i = 1 Zi.
For anyλ>0, by Markov’s inequality,
Pr( Z ≥ /theta)=Pr(eλ Z ≥eλ /theta)≤e−λ /theta E [eλ Z ]≤e−λ /theta
∏ n
i = 1
E [eλ Zi / n ]
≤e−λ /theta
∏ n
i = 1
eλ
(^2) ( b − a ) (^2) / n 2
≤e−λ /theta+λ
(^2) ( b − a ) (^2) / 8 n
,
where for the key second to last inequality we have used Hoeffding’s Lemma with the
fact that Zi / n is bounded between ( a −μ)/ n and ( b −μ)/ n. Settingλ=( b^4 − na  /theta) 2 gives
Pr

(
1
n
∑ n
i = 1
Xi −μ≥ /theta
)
=Pr( Z ≥ /theta)≤e−^2 n  /theta
(^2) /( b − a ) 2
.
Applying the same argument for Pr( Z ≤− /theta) withλ=−( b^4 − na  /theta) 2 gives
Pr

(
1
n
∑ n
i = 1
Xi −μ≤− /theta
)
=Pr( Z ≤− /theta)≤e−^2 n  /theta
(^2) /( b − a ) 2
.
Applying a union bound on the two cases gives the theorem.  /theta
The proof of the following more general version of the bound is left as an exercise
(Exercise4.20).
Theorem 4.14: Let X 1 ,..., Xnbe independent random variables with E [ Xi ]=μ iand
Pr( ai ≤ Xi ≤ bi )= 1 for constants aiand bi. Then
Pr

(∣∣
∣
∣∣
∑ n
i = 1
Xi −
∑ n
i = 1
μ i
∣∣
∣
∣∣≥ /theta
)
≤2e−^2  /theta
(^2) /∑ ni = 1 ( bi − ai ) 2
.
Note that Theorem4.12bounds the deviation of the average of the n random vari-
ables while Theorem4.14bounds the deviation of the sum of the variables.

4.6 ∗ Application: Packet Routing in Sparse Networks
Examples:

1. Consider n independent random variables X 1 ,..., Xn such that Xi is uniformly dis-
tributed in{ 0 ,..., n}. For all i ,μ= E [ Xi ]= n/2, and

Pr
(∣∣
∣
∣∣
1
n
∑ n
i = 1
Xi −
 n
2
∣∣
∣
∣∣≥ /theta
)
≤2e−^2 n  /theta
(^2) / n 2
.
In particular,
Pr

(∣∣
∣∣
∣
1
n
∑ n
i = 1
Xi −μ
∣∣
∣∣
∣
≥δμ
)
≤2e− n δ
(^2) / 2
.

2. Consider n independent random variables Y 1 ,..., Yn such that Yi is uniformly dis-
tributed in{ 0 , i }. Let Y =

∑ n
i = 1 Yi. Then E [ Yi ]= i /2, andμ= E [ Y ]=
∑ n
i = 1 i /^2 =
n ( n +1)/4. Applying Theorem4.14with ci = i we have
Pr
(∣∣
∣∣ Y − n ( n +1)
4
∣
∣∣
∣≥ /theta
)
≤2e−^2  /theta
(^2) /∑ ni = 1 c (^2) i
=2e−^2  /theta
(^2) /( n ( n +1)(2 n +1)/6)
=2e−^12  /theta
(^2) /( n ( n +1)(2 n +1))
.
We can conclude
Pr(| Y −μ|≥δμ)≤2e−^12 δ
(^2) n (^2) ( n +1) (^2) /(16 n ( n +1)(2 n +1))
≤2e−^3 n δ
(^2) / 8
.

4.6. ∗ Application: Packet Routing in Sparse Networks
A fundamental problem in parallel computing is how to communicate efficiently over
sparse communication networks. We model a communication network by a directed
graph on N nodes. Each node is a routing switch. A directed edge models a commu-
nication channel, which connects two adjacent routing switches. We consider a syn-
chronous computing model in which (a) an edge can carry one packet in each time step
and (b) a packet can traverse no more than one edge per step. We assume that switches
have buffers or queues to store packets waiting for transmission through each of the
switch’s outgoing edges.
Given a network topology, a routing algorithm specifies, for each pair of nodes, a
route – or a sequence of edges – connecting the pair in the network. The algorithm
may also specify a queuing policy for ordering packets in the switches’ queues. For
example, the First In First Out (FIFO) policy orders packets by their order of arrival.
The Furthest To Go (FTG) policy orders packets in decreasing order of the number of
edges they must still cross in the network.
Our measure of the performance of a routing algorithm on a given network topology
is the maximum time – measured as the number of parallel steps – required to route an
arbitrary permutation routing problem, where each node sends exactly one packet and
each node is the address of exactly one packet.
Of course, routing a permutation can be done in just one parallel step if the net-
work is a complete graph connecting all of the nodes to each other. Practical consider-
ations, however, dictate that a network for a large-scale parallel machine must be sparse.

chernoff and hoeffding bounds
Each node can be connected directly to only a few neighbors, and most packets must
traverse intermediate nodes en route to their final destination. Since an edge may be
on the path of more than one packet and since each edge can process only one packet
per step, parallel packet routing on sparse networks may lead to congestion and bottle-
necks. The practical problem of designing an efficient communication scheme for par-
allel computers leads to an interesting combinatorial and algorithmic problem: design-
ing a family of sparse networks connecting any number of processors, together with
a routing algorithm that routes an arbitrary permutation request in a small number of
parallel steps.
We discuss here a simple and elegant randomized routing technique and then use
Chernoff bounds to analyze its performance on the hypercube network and the butterfly
network. We first analyze the case of routing a permutation on a hypercube, a network
with N processors and O ( N log N ) edges. We then present a tighter argument for the
butterfly network, which has N nodes and only O ( N ) edges.

4.6.1 Permutation Routing on the Hypercube
LetN={ 0 ≤ i ≤ N − 1 }be the set of processors in our parallel machine and assume
that N = 2 n for some integer n. Let ̄ x =( x 1 ,..., xn ) be the binary representation of the
number 0≤ x ≤ N − 1.

Definition 4.3: The n-dimensional hypercube (or n- cube ) is a network with N = 2 n
nodes such that node x has a direct connection to node y if and only ifx and ̄ y differ in ̄
exactly one bit.

See Figure4.1. Note that the total number of directed edges in the n -cube is nN , since
each node is adjacent to n outgoing and n ingoing edges. Also, the diameter of the
network is n ; that is, there is a directed path of length up to n connecting any two
nodes in the network, and there are pairs of nodes that are not connected by any shorter
path.
The topology of the hypercube allows for a simple bit-fixing routing mechanism, as
shown in Algorithm4.1. When determining which edge to cross next, the algorithm
simply considers each bit in order and crosses the edge if necessary.
Although it seems quite natural, using only the bit-fixing routes can lead to high
levels of congestion and poor performance, as shown in Exercise4.22. There are certain
permutations on which the bit-fixing routes behave poorly. It turns out, as we will show,
that these routes perform well if each packet is being sent from a source to a destination
chosen uniformly at random. This motivates the following approach: first route each
packet to a randomly chosen intermediate point, and then route it from this intermediate
point to its final destination.
It may seem unusual to first route packets to a random intermediate point. In some
sense, this is similar in spirit to our analysis of Quicksort in Section2.5. We found there
that for a list already sorted in reverse order, Quicksort would take m( n^2 ) comparisons,
whereas the expected number of comparisons for a randomly chosen permutation is
only O ( n log n ). Randomizing the data can lead to a better running time for Quicksort.

4.6 ∗ application: packet routing in sparse networks
0
1
(a) n = 1.
00 10
01 11
(b) n = 2.
000
100
010
001 011
101 111
110
(c) n = 3.
0000
0100
0010
0001 0011
0101 0111
0110
1000
1100
1010
1001 1011
1101 1111
1110
(d) n = 4.
Figure 4.1: Hypercubes of dimensions 1, 2, 3, and 4.
n -Cube Bit-Fixing Routing Algorithm:
1. Let ̄ a and b ̄be the origin and the destination of the packet.
2. For i =1to n , do:
(a) If ai = bi then traverse the edge ( b 1 ,..., bi − 1 , ai ,..., an )→
( b 1 ,..., bi − 1 , bi , ai + 1 ,..., an ).

Algorithm 4.1: n -Cube bit-fixing routing algorithm.
Here, too, randomizing the routes that packets take – by routing them through a ran-
dom intermediate point – avoids bad initial permutations and leads to good expected
performance.
The two-phase routing algorithm (Algorithm4.2) is executed in parallel by all the
packets. The random choices are made independently for each packet. Our analysis
holds for any queueing policy that obeys the following natural requirement: if a queue
is not empty at the beginning of a time step, some packet is sent along the edge associ-
ated with that queue during that time step. We prove that this routing strategy achieves
asymptotically optimal parallel time.

chernoff and hoeffding bounds
Two-Phase Routing Algorithm:
Phase I – Route the packet to a randomly chosen node in the network using the
bit-fixing route.
Phase II – Route the packet from its random location to its final destination using
the bit-fixing route.
Algorithm 4.2: Two-phase routing algorithm.
Theorem 4.15: Given an arbitrary permutation routing problem, with probability
1 − O ( N −^1 ) the two-phase routing scheme of Algorithm 4.2 routes all packets to their
destinations on the n-cube in O ( n )= O (log N ) parallel steps.

Proof: We first analyze the run-time of Phase I. To simplify the analysis we assume that
no packet starts the execution of Phase II before all packets have finished the execution
of Phase I. We show later that this assumption can be removed.
We emphasize a fact that we use implicitly throughout. If a packet is routed to a
randomly chosen node ̄ x in the network, we can think of ̄ x =( x 1 ,..., xn ) as being
generated by setting each xi independently to be 0 with probability 1/2 and 1 with
probability 1/2.
For a given packet M , let T 1 ( M ) be the number of steps for M to finish Phase I. For
a given edge e , let X 1 ( e ) denote the total number of packets that traverse edge e during
Phase I.
In each step of executing Phase I, packet M is either traversing an edge or waiting in a
queue while some other packet traverses an edge on M ’s route. This simple observation
relates the routing time of M to the total number of packet transitions through edges on
the path of M , as follows.

Lemma 4.16: Let e 1 ,..., embe the m ≤ n edges traversed by a packet M in Phase I.
Then

T 1 ( M )≤
∑ m
i = 1
X 1 ( ei ).
Let us call any path P =( e 1 , e 2 ,..., em )of m ≤ n edges that follows the bit-fixing
algorithm a possible packet path. We denote the corresponding nodes byv 0 ,v 1 ,...,v m
with ei =(v i − 1 ,v i ). Following the definition of T 1 ( M ), for any possible packet path P
we let

T 1 ( P )=
∑ m
i = 1
X 1 ( ei ).
By Lemma4.16, the probability that Phase I takes more than T steps is bounded by
the probability that, for some possible packet path P , T 1 ( P )≥ T. Note that there are
at most 2 n · 2 n = 22 n possible packet paths, since there are 2 n possible origins and 2 n
possible destinations.

4.6 ∗ application: packet routing in sparse networks
To prove the theorem, we need a high-probability bound on T 1 ( P ). Since T 1 ( P )
equals the summation
∑ n
i = 1 X^1 ( ei ), it would be natural to try to use a Chernoff bound.
The difficulty here is that the X 1 ( ei ) are not independent random variables, since a
packet that traverses an edge is likely to traverse one of its adjacent edges. To circum-
vent this difficulty, we first use a Chernoff bound to prove that, with high probability, no
more than 6 n different packets cross any edge of P. We then condition on this event to
derive a high-probability bound on the total number of transitions these packets make
through edges of the path P , again using a Chernoff bound.^1
Let us now fix a specific possible packet path P with m edges. To obtain a high-
probability bound on the number of packets that cross an edge of P , let us call a packet
active at a nodev i − 1 on the path P if it reachesv i − 1 and has the possibility of crossing
edge ei tov i. That is, ifv i − 1 andv i differ in the j th bit then – in order for a packet to
be active atv i − 1 – its j th bit cannot have been fixed by the bit-fixing algorithm when it
reachesv i − 1. We may also call a packet active if it is active at some vertex on the path
P. We bound the total number of active packets.
For k = 1 ,..., N , let Hk be a 0–1 random variable such that Hk =1 if the packet
starting at node k is active and Hk =0 otherwise. Notice that the Hk are independent
because (a) each Hk depends only on the choice of the intermediate destination of the
packet starting at node k and (b) these choices are independent for all packets. Let
H =
∑ N
k = 1 Hk be the total number of active packets.
We first bound E [ H ]. Consider all the active packets atv i − 1. Assume thatv i − 1 =
( b 1 ,..., bj − 1 , aj , aj + 1 ,..., an ) andv i =( b 1 ,..., bj − 1 , bj , aj + 1 ,..., an ). Then only
packets that start at one of the addresses (∗,...,∗, aj ,..., an ), where∗stands for
either a 0 or a 1, can reachv i − 1 before the j th bit is fixed. Similarly, each of these
packets actually reachesv i − 1 only if its random destination is one of the addresses
( b 1 ,..., bj − 1 ,∗,...,∗). Thus, there are no more than 2 j −^1 possible active packets at
v i − 1 , and the probability that each of these packets is actually active atv i − 1 is 2−( j −1).
Hence the expected number of active packets per vertex is 1 and, since we need only
consider the m verticesv 0 ,...,v m − 1 , it follows by linearity of expectations that
E [ H ]≤ m · 1 ≤ n.
Since H is the sum of independent 0–1 random variables, we can apply the Chernoff
bound (we use the bound of Eqn. (4.3)) to prove
Pr( H ≥ 6 n ≥ 6 E [ H ])≤ 2 −^6 n.
The high-probability bound for H can help us obtain a bound for T 1 ( P ) as follows.
Using
Pr( A )=Pr( A | B )Pr( B )+Pr( A | B ̄)Pr( B ̄)
≤Pr( B )+Pr( A | B ̄),
(^1) This approach overestimates the time to finish a phase. In fact, there is a deterministic argument showing that,
in this setting, the delay of a packet on a path is bounded by the number of different packets that traverse edges
of the path, and hence there is no need to bound the total number of traversals of these packets on the path.
However, in the spirit of this book we prefer to present the probabilistic argument.

chernoff and hoeffding bounds
we find for a given possible packet path P that

Pr( T 1 ( P )≥ 30 n )≤Pr( H ≥ 6 n )+Pr( T 1 ( P )≥ 30 n | H < 6 n )
≤ 2 −^6 n +Pr( T 1 ( P )≥ 30 n | H < 6 n ).
Hence if we show

Pr( T 1 ( P )≥ 30 n | H < 6 n )≤ 2 −^3 n −^1 ,
we then have

Pr( T 1 ( P )≥ 30 n )≤ 2 −^3 n ,
which proves sufficient for our purposes.
We therefore need to bound the conditional probability Pr( T 1 ( P )≥ 30 n | H ≤ 6 n ).
In other words, conditioning on having no more than 6 n active packets that might use
edges of P , we need a bound on the total number of transitions that these packets take
through edges of P.
We first observe that, if a packet leaves the path, it cannot return to that path in this
phase of the routing algorithm. Indeed, assume that the active packet was atv i and that
it moved tow=v i + 1. The smallest index bit in whichv i + 1 and w differ cannot be fixed
later in this phase, so the route of the packet and the path P cannot meet again in this
phase.
Now suppose we have an active packet on our path P at nodev i. What is the prob-
ability that the packet crosses ei? Let us think of our packet as fixing the bits in the
binary representation of its destination one at a time by independent random coin flips.
The nodes of the edge ei differ in one bit (say, the j th bit) in this representation. It is
therefore clear that the probability of the packet crossing edge ei is at most 1/2, since
to cross this edge it must choose the appropriate value for the j th bit. (In fact, the prob-
ability might be less than 1/2; the packet might cross some other edge before choosing
the value of the j th bit.)
To obtain our bound, let us view as a trial each point in the algorithm where an
active packet at a nodev i on the path P might cross edge ei. The trial is successful if
the packet leaves the path but a failure if the packet stays on the path. Since the packet
leaves the path on a successful trial, if there are at most 6 n active packets then there
can be at most 6 n successes. Each trial is successful, independently, with probability at
least 1/2. The number of trials is itself a random variable, which we use in our bound
of T 1 ( P ).
We claim that the probability that the active packets cross edges of P more than 30 n
times is less than the probability that a fair coin flipped 36 n times comes up heads fewer
than 6 n times. To see this, think of a coin being flipped for each trial, with heads corre-
sponding to a success. The coin is biased to come up heads with the proper probability
for each trial, but this probability is always at least 1/2 and the coins are independent
for each trial. Each failure (tails) corresponds to an active packet crossing an edge, but
once there have been 6 n successes we know there are no more active packets left that
can cross an edge of the path. Using a fair coin instead of a coin possibly biased in
favor of success can only lessen the probability that the active packets cross edges of

4.6 ∗ application: packet routing in sparse networks
P more than 30 n times, as can be shown easily by induction (on the number of biased
coins).
Letting Z be the number of heads in 36 n fair coin flips, we now apply the Chernoff
bound of Eqn. (4.5) to prove:

Pr( T 1 ( P )≥ 30 n | H ≤ 6 n )≤Pr( Z ≤ 6 n )≤e−^18 n (2/3)
(^2) / 2
=e−^4 n ≤ 2 −^3 n −^1.
It follows that
Pr( T 1 ( P )≥ 30 n )≤Pr( H ≥ 6 n )+Pr( T 1 ( P )≥ 30 n | H ≤ 6 n )≤ 2 −^3 n ,
as we wanted to show. Because there are at most 2^2 n possible packet paths in the hyper-
cube, the probability that there is any possible packet path for which T 1 ( P )≥ 30 n is
bounded by
22 n 2 −^3 n = 2 − n = O ( N −^1 ).
This completes the analysis of Phase I. Consider now the execution of Phase II,
assuming that all packets completed their Phase I route. In this case, Phase II can be
viewed as running Phase I backwards: instead of packets starting at a given origin
and going to a random destination, they start at a random origin and end at a given
destination. Hence no packet spends more than 30 n steps in Phase II with probability
1 − O ( N −^1 ).
In fact, we can remove the assumption that packets begin Phase II only after Phase
I has completed. The foregoing argument allows us to conclude that the total number
of packet traversals across the edges of any packet path during Phase I and Phase II
together is bounded by 60 n with probability 1− O ( N −^1 ). Since a packet can be delayed
only by another packet traversing that edge, we find that every packet completes both
Phase I and Phase II after 60 n steps with probability 1− O ( N −^1 ) regardless of how the
phases interact, concluding the proof of Theorem4.15  /theta
Note that the run-time of the routing algorithm is optimal up to a constant factor, since
the diameter of the hypercube is n. However, the network is not fully utilized because
2 nN directed edges are used to route just N packets. At any give time, at most 1/ 2 n of
the edges are actually being used. This issue is addressed in the next section.

4.6.2 Permutation Routing on the Butterfly
In this section we adapt the result for permutation routing on the hypercube networks
to routing on butterfly networks, yielding a significant improvement in network utiliza-
tion. Specifically, our goal in this section is to route a permutation on a network with
N nodes and O ( N ) edges in O (log N ) parallel time steps. Recall that the hypercube
network had N nodes but m( N log N ) edges. Although the argument will be similar
in spirit to that for the hypercube network, there is some additional complexity to the
argument for the butterfly network.
We work on the wrapped butterfly network, defined as follows.

Definition 4.4: The wrapped butterfly network has N = n 2 nnodes. The nodes are
arranged in n columns and 2 nrows. A node’s address is a pair ( x , r ) , where 1 ≤ x ≤ 2 n

chernoff and hoeffding bounds
row 000
row 001
row 010
row 011
row 100
row 101
row 110
row 111
level 0 level 1 level 2 level 3
Figure 4.2: The butterfly network. In the wrapped butterfly, levels 0 and 3 are collapsed into one
level.

is the row number and 0 ≤ r ≤ n − 1 is the column number of the node. Node ( x , r ) is
connected to node ( y , s ) if and only if s = r +1mod n and either:

1. x = y (the “direct” edge); or
2. x and y differ in precisely the s th bit in their binary representation (the “flip” edge).

See Figure4.2. To see the relation between the wrapped butterfly and the hypercube,
observe that by collapsing the n nodes in each row of the wrapped butterfly into one
“super node” we obtain an n -cube network. Using this correspondence, one can easily
verify that there is a unique directed path of length n connecting node ( x , r )toany
other node (w, r ) in the same column. This path is obtained by bit fixing: first fixing
bits r +1to n , then bits 1 to r. See Algorithm4.3. Our randomized permutation routing
algorithm on the butterfly consists of three phases, as shown in Algorithm4.4.
Unlike our analysis of the hypercube, our analysis here cannot simply bound the
number of active packets that possibly traverse edges of a path. Given the path of a
packet, the expected number of other packets that share edges with this path when

4.6 ∗ application: packet routing in sparse networks
Wrapped Butterfly Bit-Fixing Routing Algorithm:
1. Let ( x , r ) and ( y , r ) be the origin and the destination of a packet.
2. For i =0to n −1, do:
(a) j =(( i + r )mod n )+ 1 ;
(b) if aj = bj then traverse the direct edge to column j mod n , else traverse the
flip edge to column j mod n.

Algorithm 4.3: Wrapped butterfly bit-fixing routing algorithm.
Three-Phase Routing Algorithm:
For a packet sent from node ( x , r ) to node ( y , s ):
Phase I – Choose a randomw∈[1,..., 2 n ]. Route the packet from node ( x , r )to
node (w, r ) using the bit-fixing route.
Phase II – Route the packet to node (w, s ) using direct edges.
Phase III – Route the packet from node (w, s ) to node ( y , s ) using the bit-fixing
route.
Algorithm 4.4: Three-phase routing algorithm.
routing a random permutation on the butterfly network is m( n^2 ) and not O ( n ) as in the
n -cube. To obtain an O ( n ) routing time, we need a more refined analysis technique that
takes into account the order in which packets traverse edges.
Because of this, we need to consider the priority policy that the queues use when
there are several packets waiting to use the edge. A variety of priority policies would
work here; we assume the following rules.

1. The priority of a packet traversing an edge is ( i −1) n + t , where i is the current
phase of the packet and t is the number of edge traversals the packet has already
executed in this phase.
2. If at any step more than one packet is available to traverse an edge, the packet with
the smallest priority number is sent first.

Theorem 4.17: Given an arbitrary permutation routing problem on the wrapped but-
terflywithN = n 2 nnodes,withprobability 1 − O ( N −^1 ) thethree-phaseroutingscheme
of Algorithm4.4routes all packets to their destinations in O ( n )= O (log N ) parallel
steps.

Proof: The priority rule in the edge queues guarantees that packets in a phase cannot
delay packets in earlier phases. Because of this, in our forthcoming analysis we can
consider the time for each phase to complete separately and then add these times to
bound the total time for the three-phase routing scheme to complete.
We begin by considering the second phase. We first argue that with high probability
each row transmits at most 4 n packets in the second phase. To see this, let X wbe the

chernoff and hoeffding bounds
number of packets whose intermediate row choice is w in the three-phase routing algo-
rithm. Then X wis the sum of 0–1 independent random variables, one for each packet,
and E [ X w]= n. Hence, we can directly apply the Chernoff bound of Eqn. (4.1)tofind

Pr( X w≥ 4 n )≤
(
e^3
44
) n
≤ 3 −^2 n.
There are 2 n possible rows w. By the union bound, the probability that any row has
more than 4 n packets is only 2 n · 3 −^2 n = O ( N −^1 ).
We now argue that, if each row has at most 4 n packets for the second phase, then the
second phase takes at most 5 n steps to complete. Combined with our previous observa-
tions, this means the second phase takes at most 5 n steps with probability 1− O ( N −^1 ).
To see this, note that in the second phase the routing has a special structure: each packet
moves from edge to edge along its row. Because of the priority rule, each packet can
be delayed only by packets already in a queue when it arrives. Therefore, to place an
upper bound on the number of packets that delay a packet p , we can bound the total
number of packets found in each queue when p arrives at the queue. But in Phase II, the
number of other packets that an arriving packet finds in a queue cannot increase in size
over time, since at each step a queue sends a packet and receives at most one packet.
(It is worth considering the special case when a queue becomes empty at some point
in Phase II; this queue can receive another packet at some later step, but the number of
packets an arriving packet will find in the queue after that point is always zero.) Since
there are at most 4 n packets total in the row to begin with, p finds at most 4 n packets
that delay it as it moves from queue to queue. Since each packet moves at most n times
in the second phase, the total time for the phase is 5 n steps.
We now consider the other phases. The first and third phases are again the same by
symmetry, so we consider just the first phase. Our analysis will use a delay sequence
argument.  /theta

Definition 4.5: A delay sequence for an execution of Phase I is a sequence of n edges
e 1 ,..., ensuch that either ei = ei + 1 or ei + 1 is an outgoing edge from the end vertex of
ei. The sequence e 1 ,..., enhas the further property that eiis (one of) the last edges to
transmit packets with priority up to i among ei + 1 and the two incoming edges of ei + 1_._

The relation between the delay sequence and the time for Phase I to complete is given
by the following lemma.

Lemma 4.18: For a given execution of Phase I and delay sequence e 1 ,..., en, lettibe
the number of packets with priority i sent through edge ei.LetTibe the time that edge ei
finishes sending all packets with priority number up to i, so that Tnis the earliest time
at which all packets passing through enduring Phase I have passed through it. Then:

1. Tn ≤

∑ n
i = 1 ti.
2. If the execution of Phase I takes T steps, then there is a delay sequence for this
execution for which

∑ n
i = 1 ti ≥ T.
Proof: By the design of the delay sequence, at time Ti the queue of ei + 1 already holds
all of the packets that it will need to subsequently transmit with priority i +1, and at

4.6 ∗ application: packet routing in sparse networks
that time it has already finished transmitting all packets with priority numbers up to i.
Thus,

Ti + 1 ≤ Ti + ti + 1.
Since T 1 = t 1 ,wehave

Tn ≤ Tn − 1 + tn
≤ Tn − 2 + tn − 1 + tn
≤
∑ n
i = 1
ti ,
proving the first part of the lemma.
For the second part, assume that Phase I took T steps and let e be an edge that trans-
mitted a packet at time T. We can construct a delay sequence with en = e by choosing
en − 1 to be the edge among e and its two incoming edges that last transmits packets of
priority∑ n −1, and similarly choosing en − 2 down to e 1. By the first part of the lemma,
n
i = 1 ti ≥ T.  /theta

Returning to the proof of Theorem4.17, we now show that the probability of a delay
sequence with T ≥ 40 n is only O ( N −^1 ). We call any sequence of edges e 1 ,..., en such
that either ei = ei + 1 or ei + 1 is an outgoing edge from the end vertex of ei a possibledelay
sequence. For a given execution and a possible delay sequence, let ti be the number of
packets with priority i sent through ei. Let T =

∑ n
i = 1 ti. We first bound E [ T ]. Consider
the edge ei =v→v′. Packets with priority i pass through this edge only if their source
is at distance i −1 from v. There are precisely 2 i −^1 nodes that are connected to v by a
directed path of length i −1. Since packets are sent in Phase I to random destinations,
the probability that each of these nodes sends a packet that traverses edge ei is 2− i ,
giving

E [ ti ]= 2 i −^12 − i =
1
2
and E [ T ]=
n
2
.
The motivation for using the delay sequence argument should now be clear. Each
possible delay sequence defines a random variable T , where E [ T ]= n /2. The max-
imum of T over all delay sequences bounds the run-time of the phase. So we need
a bound on T that holds with sufficiently high probability to cover all possible delay
sequences. A high-probability bound on T can now be obtained using an argument sim-
ilar to the one used in the proof of Theorem4.15. We first bound the number of different
packets that contribute to edge traversals counted in T.
For j = 1 ,..., N , let Hj =1 if any traversal of the packet sent by node j is counted
in T ;otherwise, Hj =0. Clearly, H =

∑ N
j = 1 Hj ≤ T and E [ H ]≤ E [ T ]= n /2, where
the Hj are independent random variables. Applying the Chernoff bound of Eqn. (4.3)
therefore yields

Pr( H ≥ 5 n )≤ 2 −^5 n.
Conditioning on the event H ≤ 5 n , we now proceed to prove a bound on T , following
the same line as in the proof of Theorem4.15. Given a packet u with at least one

chernoff and hoeffding bounds
traversal counted in T , we consider how many additional traversals of u are counted in
T. Specifically, if u is counted in ti then we consider the probability that it is counted
in ti + 1. We distinguish between two cases as follows.

1. If ei + 1 = ei then u cannot be counted in ti + 1 , since its traversal with priority i +1is
in the next column. Similarly, it cannot be counted in any tj , j > i.
2. If ei + 1 = ei , then the probability that u continues through ei + 1 (and is counted in
ti + 1 )isatmost1/2. If it does not continue through ei + 1 , then it cannot intersect with
the delay sequence in any further traversals in this phase.

As in the proof of Theorem4.15, the probability that T ≥ 40 n is less than the prob-
ability that a fair coin flipped 40 n times comes up heads fewer than 5 n times. (Keep in
mind that, in this case, the first traversal by each packet in H must be counted as con-
tributing to T .) Letting Z be the number of heads in 40 n fair coin flips, we now apply
the Chernoff bound (4.5) to prove

Pr( T ≥ 40 n | H ≤ 5 n )≤Pr( Z ≤ 5 n )≤e−^20 n (3/4)
(^2) / 2
≤ 2 −^5 n.
We conclude that
Pr( T ≥ 40 n )≤Pr( T ≥ 40 n | H ≤ 5 n )+Pr( H ≥ 5 n )≤ 2 −^5 n +^1.
There are no more than 2 N 3 n −^1 ≤ n 2 n 3 n possible delay sequences, since a sequence
can start in any one of the 2 N edges of the network, and by Definition4.5,if ei is the
i th edge in the sequence, there are only three possible assignments for ei + 1. Thus, the
probability that, in the execution of Phase I, there is a delay sequence with T ≥ 40 n is
bounded above (using the union bound) by
n 2 n 3 n 2 −^5 n +^1 ≤ O ( N −^1 ).
Since Phase III is entirely similar to Phase I and since Phase II also finishes in O ( n )
steps with probability 1− O ( N −^1 ), we have that the three-phase routing algorithm fin-
ishes in O ( n ) steps with probability 1− O ( N −^1 ).

4.7 Exercises
Exercise 4.1: Alice and Bob play checkers often. Alice is a better player, so the proba-
bility that she wins any given game is 0.6, independent of all other games. They decide
to play a tournament of n games. Bound the probability that Alice loses the tournament
using a Chernoff bound.

Exercise 4.2: We have a standard six-sided die. Let X be the number of times that a 6
occurs over n throws of the die. Let p be the probability of the event X ≥ n /4. Compare
the best upper bounds on p that you can obtain using Markov’s inequality, Chebyshev’s
inequality, and Chernoff bounds.

Exercise 4.3: (a) Determine the moment generating function for the binomial random
variable B ( n , p ).

4.7 exercises
(b) Let X be a B ( n , p ) random variable and Y a B ( m , p ) random variable, where X
and Y are independent. Use part (a) to determine the moment generating function of
X + Y.
(c) What can we conclude from the form of the moment generating function of
X + Y?

Exercise 4.4: Determine the probability of obtaining 55 or more heads when flipping
a fair coin 100 times by an explicit calculation, and compare this with the Chernoff
bound. Do the same for 550 or more heads in 1000 flips.

Exercise 4.5: We plan to conduct an opinion poll to find out the percentage of people
in a community who want its president impeached. Assume that every person answers
either yes or no. If the actual fraction of people who want the president impeached is
p , we want to find an estimate X of p such that

Pr(| X − p |≤ε p )> 1 −δ
foragivenεandδ, with 0<ε,δ<1.
We query N people chosen independently and uniformly at random from the com-
munity and output the fraction of them who want the president impeached. How large
should N be for our result to be a suitable estimator of p? Use Chernoff bounds, and
express N in terms of p ,ε, andδ. Calculate the value of N from your bound ifε= 0. 1
andδ= 0 .05 and if you know that p is between 0.2 and 0.8.

Exercise 4.6: (a) In an election with two candidates using paper ballots, each vote is
independently misrecorded with probability p = 0 .02. Use a Chernoff bound to give
an upper bound on the probability that more than 4% of the votes are misrecorded in
an election of 1,000,000 ballots.
(b) Assume that a misrecorded ballot always counts as a vote for the other candidate.
Suppose that candidate A received 510,000 votes and that candidate B received 490,000
votes. Use Chernoff bounds to upper bound the probability that candidate B wins the
election owing to misrecorded ballots. Specifically, let X be the number of votes for
candidate A that are misrecorded and let Y be the number of votes for candidate B that
are misrecorded. Bound Pr(( X > k )∪( Y < n)) for suitable choices of k and n.

Exercise 4.7: Throughout the chapter we implicitly assumed the following extension
of the Chernoff bound. Prove that it is true.
Let X =

∑ n
i = 1 Xi , where the Xi are independent 0–1 random variables. Letμ=
E [ X ]. Choose anyμ L andμ H such thatμ L ≤μ≤μ H. Then, for anyδ>0,

Pr( X ≥(1+δ)μ H )≤
(
eδ
(1+δ)(1+δ)
)μ H
.
Similarly, for any 0<δ<1,

Pr( X ≤(1−δ)μ L )≤
(
e−δ
(1−δ)(1−δ)
)μ L
.
chernoff and hoeffding bounds
Exercise 4.8: We show how to construct a random permutationπon [1, n ], given a
black box that outputs numbers independently and uniformly at random from [1, k ]
where k ≥ n. If we compute a function f [1, n ]→[1, k ] with f ( i )= f ( j )for i = j ,
this yields a permutation; simply output the numbers [1, n ] according to the order of
the f ( i ) values. To construct such a function f , do the following for j = 1 ,..., n : choose
f ( j ) by repeatedly obtaining numbers from the black box and setting f ( j ) to the first
number found such that f ( j )= f ( i )for i < j.
Prove that this approach gives a permutation chosen uniformly at random from all
permutations. Find the expected number of calls to the black box that are needed when
k = n and k = 2 n. For the case k = 2 n , argue that the probability that each call to the
black box assigns a value of f ( j ) to some j is at least 1/2. Based on this, use a Chernoff
bound to bound the probability that the number of calls to the black box is at least 4 n.

Exercise 4.9: Suppose that we can obtain independent samples X 1 , X 2 ,...of a random
variable X and that we want to use these samples to estimate E [ X ]. Using t samples,
we use

(∑ t
i = 1 Xi
)
/ t for our estimate of E [ X ]. We want the estimate to be withinε E [ X ]
from the true value of E [ X ] with probability at least 1−δ. We may not be able to
use Chernoff’s bound directly to bound how good our estimate is if X is not a 0–1
random variable, and we do not know its moment generating function. We develop
an alternative approach that requires only having a bound on the variance of X. Let
r =

√
Var [ X ]/ E [ X ].
(a) Show using Chebyshev’s inequality that O ( r^2 /ε^2 δ) samples are sufficient to solve
the problem.
(b) Suppose that we need only a weak estimate that is withinε E [ X ]of E [ X ] with
probability at least 3/4. Argue that O ( r^2 /ε^2 ) samples are enough for this weak
estimate.
(c) Show that, by taking the median of O (log(1/δ)) weak estimates, we can obtain an
estimate withinε E [ X ]of E [ X ] with probability at least 1−δ. Conclude that we
need only O (( r^2 log(1/δ))/ε^2 ) samples.

Exercise 4.10: A casino is testing a new class of simple slot machines. Each game, the
player puts in $1, and the slot machine is supposed to return either $3 to the player with
probability 4/25, $100 with probability 1/200, or nothing with all remaining probabil-
ity. Each game is supposed to be independent of other games.
The casino has been surprised to find in testing that the machines have lost $10,000
over the first million games. Derive a Chernoff bound for the probability of this event.
You may want to use a calculator or program to help you choose appropriate values as
you derive your bound.

Exercise 4.11: Consider a collection X 1 ,..., Xn of n independent integers chosen
uniformly from the set{ 0 , 1 , 2 }. Let X =

∑ n
i = 1 Xi and 0<δ<1. Derive a Chernoff
bound for Pr( X ≥(1+δ) n ) and Pr( X ≤(1−δ) n ).

Exercise 4.12: Consider a collection X 1 ,..., Xn of n independent geometrically dis-
tributed random variables with mean 2. Let X =

∑ n
i = 1 Xi andδ>0.
4.7 exercises
(a) Derive a bound on Pr( X ≥(1+δ)(2 n )) by applying the Chernoff bound to a
sequence of (1+δ)(2 n ) fair coin tosses.
(b) Directly derive a Chernoff bound on Pr( X ≥(1+δ)(2 n )) using the moment gen-
erating function for geometric random variables.
(c) Which bound is better?

Exercise 4.13: Let X 1 ,..., Xn be independent Poisson trials such that Pr( Xi =1)= p.
Let X =

∑ n
i = 1 Xi , so that E [ X ]= pn. Let
F ( x , p )= x ln( x / p )+(1− x )ln((1− x )/(1− p )).
(a) Show that, for 1≥ x > p ,

Pr( X ≥ xn )≤e− nF ( x , p ).
(b) Show that, when 0< x , p <1, we have F ( x , p )−2( x − p )^2 ≥0. ( Hint: Take the
second derivative of F ( x , p )−2( x − p )^2 with respect to x .)
(c) Using parts (a) and (b), argue that

Pr( X ≥( p +ε) n )≤e−^2 n ε
2
.
(d) Use symmetry to argue that

Pr( X ≤( p −ε) n )≤e−^2 n ε
2
,
and conclude that
Pr(| X − pn |≥ε n )≤2e−^2 n ε
2
.
Exercise 4.14: Modify the proof of Theorem4.4to show the following bound for
a weighted sum of Poisson trials. Let X 1 ,..., Xn be independent Poisson trials such
that Pr( Xi )= pi and let a 1 ,..., an be real numbers in [0,1]. Let X =

∑ n
i = 1 aiXi and
μ= E [ X ]. Then the following Chernoff bound holds: for anyδ>0,

Pr( X ≥(1+δ)μ)≤
(
eδ
(1+δ)(1+δ)
)μ
.
Prove a similar bound for the probability that X ≤(1−δ)μfor any 0<δ<1.

Exercise 4.15: Let X 1 ,..., Xn be independent random variables such that

Pr( Xi = 1 − pi )= pi and Pr( Xi =− pi )= 1 − pi.
Let X =

∑ n
i = 1 Xi. Prove
Pr(| X |≥ a )≤2e−^2 a
(^2) / n
.
Hint: You may need to assume the inequality
pi eλ(1− pi )+(1− pi )e−λ pi ≤eλ
(^2) / 8
.
This inequality is difficult to prove directly.

chernoff and hoeffding bounds
Exercise 4.16: Let X 1 ,..., Xn be independent Poisson trials such that Pr( Xi =1)=
pi. Let X =

∑ n
i = 1 aiXi andμ= E [ X ]. Use the result of Exercise4.15to prove that if
| ai |≤1 for all 1≤ i ≤ n , then for any 0<δ<1,

Pr(| X −μ|≥δμ)≤2e−^2 δ
(^2) μ (^2) / n
.
Exercise 4.17: Suppose that we have n jobs to distribute among m processors. For
simplicity, we assume that m divides n. A job takes 1 step with probability p and k > 1
steps with probability 1− p. Use Chernoff bounds to determine upper and lower
bounds (that hold with high probability) on when all jobs will be completed if we
randomly assign exactly n / m jobs to each processor.
Exercise 4.18: In many wireless communication systems, each receiver listens on a
specific frequency. The bit b ( t ) sent at time t is represented by a 1 or−1. Unfortunately,
noise from other nearby communications can affect the receiver’s signal. A simplified
model of this noise is as follows. There are n other senders, and the i th has strength
pi ≤1. At any time t , the i th sender is also trying to send a bit bi ( t ) that is represented
by 1 or−1. The receiver obtains the signal s ( t ) given by
s ( t )= b ( t )+
∑ n
i = 1
pibi ( t ).
If s ( t ) is closer to 1 than−1, the receiver assumes that the bit sent at time t was a 1;
otherwise, the receiver assumes that it was a−1.
Assume that all the bits bi ( t ) can be considered independent, uniform random vari-
ables. Give a Chernoff bound to estimate the probability that the receiver makes an
error in determining b ( t ).
Exercise 4.19: Recall that a function f is said to be convex if, for any x 1 , x 2 and for
0 ≤λ≤1,
f (λ x 1 +(1−λ) x 2 )≤λ f ( x 1 )+(1−λ) f ( x 2 ).
(a) Let Z be a random variable that takes on a (finite) set of values in the interval [0,1],
and let p = E [ Z ]. Define the Bernoulli random variable X by Pr( X =1)= p and
Pr( X =0)= 1 − p. Show that E [ f ( Z )]≤ E [ f ( X )] for any convex function f.
(b) Use the fact that f ( x )=e tx is convex for any t ≥0 to obtain a Chernoff bound for
the sum of n independent random variables with distribution Z as in part (a), based
on a Chernoff bound for independent Poisson trials.
Exercise 4.20: Prove Theorem4.14.
Exercise 4.21: We prove that the Randomized Quicksort algorithm sorts a set of
n numbers in time O ( n log n ) with high probability. Consider the following view of
Randomized Quicksort. Every point in the algorithm where it decides on a pivot ele-
ment is called a node. Suppose the size of the set to be sorted at a particular node is s.
The node is called good if the pivot element divides the set into two parts, each of size
not exceeding 2 s /3. Otherwise the node is called bad. The nodes can be thought of as

4.7 exercises
forming a tree in which the root node has the whole set to be sorted and its children
have the two sets formed after the first pivot step and so on.

(a) Show that the number of good nodes in any path from the root to a leaf in this tree
is not greater than c log 2 n , where c is some positive constant.
(b) Show that, with high probability (greater than 1− 1 / n^2 ), the number of nodes in
a given root to leaf path of the tree is not greater than c ′log 2 n , where c ′is another
constant.
(c) Show that, with high probability (greater than 1− 1 / n ), the number of nodes in
the longest root to leaf path is not greater than c ′log 2 n .( Hint: How many nodes
are there in the tree?)
(d) Use your answers to show that the running time of Quicksort is O ( n log n ) with
probability at least 1− 1 / n.

Exercise 4.22: Consider the bit-fixing routing algorithm for routing a permutation on
the n -cube. Suppose that n is even. Write each source node s as the concatenation of
two binary strings as and bs each of length n /2. Let the destination of s ’s packet be
the concatenation of bs and as. Show that this permutation causes the bit-fixing routing
algorithm to take m

(√
N
)
steps.
Exercise 4.23: Consider the following modification to the bit-fixing routing algorithm
for routing a permutation on the n -cube. Suppose that, instead of fixing the bits in order
from 1 to n , each packet chooses a random order (independent of other packets’ choices)
and fixes the bits in that order. Show that there is a permutation for which this algorithm
requires 2 m( n )steps with high probability.

Exercise 4.24: Assume that we use the randomized routing algorithm for the n -cube
network (Algorithm4.2) to route a total of up to p 2 n packets, where each node is the
source of no more than p packets and each node is the destination of no more than p
packets.

(a) Give a high-probability bound on the run-time of the algorithm.
(b) Give a high-probability bound on the maximum number of packets at any node at
any step of the execution of the routing algorithm.

Exercise 4.25: Show that the expected number of packets that traverse any edge on
the path of a given packet when routing a random permutation on the wrapped butterfly
network of N = n 2 n nodes is m( n^2 ).

Exercise 4.26: In this exercise, we design a randomized algorithm for the following
packet routing problem. We are given a network that is an undirected connected graph
G , where nodes represent processors and the edges between the nodes represent wires.
We are also given a set of N packets to route. For each packet we are given a source
node, a destination node, and the exact route (path in the graph) that the packet should
take from the source to its destination. (We may assume that there are no loops in the

chernoff and hoeffding bounds
path.) In each time step, at most one packet can traverse an edge. A packet can wait at
any node during any time step, and we assume unbounded queue sizes at each node.
A schedule for a set of packets specifies the timing for the movement of packets
along their respective routes. That is, it specifies which packet should move and which
should wait at each time step. Our goal is to produce a schedule for the packets that
tries to minimize the total time and the maximum queue size needed to route all the
packets to their destinations.

(a) The dilation d is the maximum distance traveled by any packet. The congestion c is
the maximum number of packets that must traverse a single edge during the entire
course of the routing. Argue that the time required for any schedule should be at
least m( c + d ).
(b) Consider the following unconstrained schedule, where many packets may traverse
an edge during a single time step. Assign each packet an integral delay chosen ran-
domly, independently, and uniformly from the interval [1,α c /log( Nd )], whereα
is a constant. A packet that is assigned a delay of x waits in its source node for x time
steps; then it moves on to its final destination through its specified route without
ever stopping. Give an upper bound on the probability that more than O (log( Nd ))
packets use a particular edge e at a particular time step t.
(c) Again using the unconstrained schedule of part (b), show that the probability that
more than O (log( Nd )) packets pass through any edge at any time step is at most
1 /( Nd ) for a sufficiently largeα.
(d) Use the unconstrained schedule to devise a simple randomized algorithm that, with
high probability, produces a schedule of length O ( c + d log( Nd )) using queues of
size O (log( Nd )) and following the constraint that at most one packet crosses an
edge per time step.

chapter five

Balls, Bins, and Random Graphs

In this chapter, we focus on one of the most basic of random processes: m balls are
thrown randomly into n bins, each ball landing in a bin chosen independently and uni-
formly at random. We use the techniques we have developed previously to analyze this
process and develop a new approach based on what is known as the Poisson approx-
imation. We demonstrate several applications of this model, including a more sophis-
ticated analysis of the coupon collector’s problem and an analysis of the Bloom filter
data structure. After introducing a closely related model of random graphs, we show an
efficient algorithm for finding a Hamiltonian cycle on a random graph with sufficiently
many edges. Even though finding a Hamiltonian cycle is NP-hard in general, our result
shows that, for a randomly chosen graph, the problem is solvable in polynomial time
with high probability.

5.1 Example: The Birthday Paradox
Sitting in lecture, you notice that there are 30 people in the room. Is it more likely that
some two people in the room share the same birthday or that no two people in the room
share the same birthday?
We can model this problem by assuming that the birthday of each person is a ran-
dom day from a 365-day year, chosen independently and uniformly at random for each
person. This is obviously a simplification; for example, we assume that a person’s birth-
day is equally likely to be any day of the year, we avoid the issue of leap years, and we
ignore the possibility of twins! As a model, however, it has the virtue of being easy to
understand and analyze.
One way to calculate this probability is to directly count the configurations where
two people do not share a birthday. It is easier to think about the configurations where
people do not share a birthday than about configurations where some two people do.

Thirty days must be chosen from the 365; there are

(
365
30
)
ways to do this. These 30
days can be assigned to the people in any of the 30! possible orders. Hence there are(
365
30

)
30! configurations where no two people share the same birthday, out of the 365^30
balls, bins, and random graphs
ways the birthdays could occur. Thus, the probability is
(
365
30

)
30!
36530
. (5.1)
We can also calculate this probability by considering one person at a time. The first
person in the room has a birthday. The probability that the second person has a different
birthday is (1− 1 /365). The probability that the third person in the room then has
a birthday different from the first two, given that the first two people have different
birthdays, is (1− 2 /365). Continuing on, the probability that the k th person in the
room has a different birthday than the first k −1, assuming that the first k −1have
different birthdays, is (1−( k −1)/365). So the probability that 30 people all have
different birthdays is the product of these terms, or
(
1 −

1
365
)
·
(
1 −
2
365
)
·
(
1 −
3
365
)
···
(
1 −
29
365
)
.
You can check that this matches the expression (5.1).
Calculations reveal that (to four decimal places) this product is 0.2937, so when
30 people are in the room there is more than a 70% chance that two share the same
birthday. A similar calculation shows that only 23 people need to be in the room before
it is more likely than not that two people share a birthday.
More generally, if there are m people and n possible birthdays then the probability
that all m have different birthdays is
(
1 −

1
n
)
·
(
1 −
2
n
)
·
(
1 −
3
n
)
···
(
1 −
m − 1
n
)
=
m ∏− 1
j = 1
(
1 −
j
n
)
.
Using that 1− k / n ≈e− k / n when k is small compared to n , we see that if m is small
compared to n then

m ∏− 1
j = 1
(
1 −
j
n
)
≈
m ∏− 1
j = 1
e− j / n
=exp
⎧
⎨
⎩
−
m ∑− 1
j = 1
j
n
⎫
⎬
⎭
=e− m ( m −1)/^2 n
≈e− m
(^2) / 2 n
.
Hence the value for m at which the probability that m people all have different birthdays
is 1/2 is approximately given by the equation
m^2
2 n
=ln 2,
or m =

√
2 n ln 2. For the case n =365, this approximation gives m = 22 .49 to two
decimal places, matching the exact calculation quite well.

5.2 Balls into Bins
Quite tight and formal bounds can be established using bounds in place of the
approximations just derived, an option that is considered in Exercise5.3. The follow-
ing simple arguments, however, give loose bounds and good intuition. Let us consider
each person one at a time, and let Ek be the event that the k th person’s birthday does
not match any of the birthdays of the first k −1 people. Then the probability that the
first k people fail to have distinct birthdays is

Pr( E ̄ 1 ∪ E ̄ 2 ∪···∪ E ̄ k )≤
∑ k
i = 1
Pr( E ̄ i )
≤
∑ k
i = 1
i − 1
n
=
k ( k −1)
2 n
If k ≤

√
n this probability is less than 1/2, so with
⌊√
n
⌋
people the probability is at
least 1/2 that all birthdays will be distinct.
Now assume that the first

⌈√
n
⌉
people all have distinct birthdays. Each person after
that has probability at least

√
n / n = 1 /
√
n of having the same birthday as one of these
first

⌈√
n
⌉
people. Hence the probability that the next
⌈√
n
⌉
people all have different
birthdays than the first

⌈√
n
⌉
people is at most
(
1 −
1
√
n
)√ n 
<
1
e
<
1
2
Hence, once there are 2

⌈√
n
⌉
people, the probability is at most 1/e that all birthdays
will be distinct.

5.2. Balls into Bins
5.2.1 The Balls-and-Bins Model
The birthday paradox is an example of a more general mathematical framework that
is often formulated in terms of balls and bins. We have m balls that are thrown into
n bins, with the location of each ball chosen independently and uniformly at random
from the n possibilities. What does the distribution of the balls in the bins look like?
The question behind the birthday paradox is whether or not there is a bin with two
balls.
There are several interesting questions that we could ask about this random process.
For example, how many of the bins are empty? How many balls are in the fullest bin?
Many of these questions have applications to the design and analysis of algorithms.
Our analysis of the birthday paradox showed that, if m balls are randomly placed
into n bins then, for some m = m

(√
n
)
, at least one of the bins is likely to have more
than one ball in it. Another interesting question concerns the maximum number of
balls in a bin, or the maximum load. Let us consider the case where m = n , so that

balls, bins, and random graphs
the number of balls equals the number of bins and the average load is 1. Of course the
maximum possible load is n , but it is very unlikely that all n balls land in the same bin.
We seek an upper bound that holds with probability tending to 1 as n grows large. We
can show that the maximum load is more than 3 ln n /ln ln n with probability at most
1 / n for sufficiently large n via a direct calculation and a union bound. This is a very
loose bound; although the maximum load is in fact m(ln n /ln ln n ) with probability
close to 1 (as we show later), the constant factor 3 we use here is chosen to simplify
the argument and could be reduced with more care.

Lemma 5.1: When n balls are thrown independently and uniformly at random into n
bins, the probability that the maximum load is more than 3ln n /ln ln nisatmost 1 / n
for n sufficiently large.

Proof: The probability that bin 1 receives at least M balls is at most

(
n
M
)(
1
n
) M
.
This follows from a union bound; there are

( n
M
)
distinct sets of M balls, and for any set
of M balls the probability that all land in bin 1 is (1/ n ) M. We now use the inequalities

(
n
M
)(
1
n
) M
≤
1
M!
≤
(
e
M
) M
.
Here the second inequality is a consequence of the following general bound on facto-
rials: since

kk
k!
<
∑∞
i = 0
ki
i!
=e k ,
we have

k !>
(
k
e
) k
.
Applying a union bound again allows us to find that, for M ≥3ln n /ln ln n , the prob-
ability that any bin receives at least M balls is bounded above by

n
(
e
M
) M
≤ n
(
elnln n
3ln n
)3ln n /ln ln n
≤ n
(
ln ln n
ln n
)3ln n /ln ln n
=eln n (eln ln ln n −ln ln n )3ln n /ln ln n
=e−2ln n +3(ln n )(ln ln ln n )/ln ln n
≤
1
n
for n sufficiently large.  /theta

5.3 The Poisson Distribution
5.2.2 Application: Bucket Sort
Bucket sort is an example of a sorting algorithm that, under certain assumptions on
the input, breaks the m( n log n ) lower bound for standard comparison-based sorting.
For example, suppose that we have a set of n = 2 m elements to be sorted and that each
element is an integer chosen independently and uniformly at random from the range
[0, 2 k ), where k ≥ m. Using Bucket sort, we can sort the numbers in expected time
O ( n ). Here the expectation is over the choice of the random input, since Bucket sort is
a completely deterministic algorithm.
Bucket sort works in two stages. In the first stage, we place the elements into n
buckets. The j th bucket holds all elements whose first m binary digits correspond to
the number j. For example, if n = 210 , bucket 3 contains all elements whose first 10
binary digits are 0000000011. When j < n, the elements of the j th bucket all come
before the elements in the nth bucket in the sorted order. Assuming that each element
can be placed in the appropriate bucket in O (1) time, this stage requires only O ( n )
time. Because of the assumption that the elements to be sorted are chosen uniformly,
the number of elements that land in a specific bucket follows a binomial distribution
B ( n , 1 / n ). Buckets can be implemented using linked lists.
In the second stage, each bucket is sorted using any standard quadratic time algo-
rithm (such as Bubblesort or Insertion sort). Concatenating the sorted lists from each
bucket in order gives us the sorted order for the elements. It remains to show that the
expected time spent in the second stage is only O ( n ).
The result relies on our assumption regarding the input distribution. Under the uni-
form distribution, Bucket sort falls naturally into the balls and bins model: the elements
are balls, buckets are bins, and each ball falls uniformly at random into a bin.
Let Xj be the number of elements that land in the j th bucket. The time to sort the j th
bucket is then at most c ( Xj )^2 for some constant c. The expected time spent sorting in
the second stage is at most

E
⎡
⎣
∑ n
j = 1
c ( Xj )^2
⎤
⎦= c
∑ n
j = 1
E [ Xj^2 ]= cn E [ X 12 ],
where the first equality follows from the linearity of expectations and the second fol-
lows from symmetry, as E [ X^2 j ] is the same for all buckets.
Since X 1 is a binomial random variable B ( n , 1 / n ), using the results of Section3.2.1
yields

E [ X 12 ]=
n ( n −1)
n^2
+ 1 = 2 −
1
n
< 2.
Hence the total expected time spent in the second stage is at most 2 cn , so Bucket sort
runs in expected linear time.

5.3. The Poisson Distribution
We now consider the probability that a given bin is empty in the balls and bins model
with m balls and n bins as well as the expected number of empty bins. For the first bin

balls, bins, and random graphs
to be empty, it must be missed by all m balls. Since each ball hits the first bin with
probability 1/ n , the probability the first bin remains empty is
(
1 −

1
n
) m
≈e− m / n ;
of course, by symmetry this probability is the same for all bins. If Xi is a random variable
that is 1 when the i th bin is empty and 0 otherwise, then E [ Xi ]=(1− 1 / n ) m. Let X be
a random variable that represents the number of empty bins. Then, by the linearity of
expectations,

E [ X ]= E
[ n
∑
i = 1
Xi
]
=
∑ n
i = 1
E [ Xi ]= n
(
1 −
1
n
) m
≈ n e− m / n.
Thus, the expected fraction of empty bins is approximately e− m / n. This approximation
is very good even for moderately size values of m and n , and we use it frequently
throughout this chapter.
We can generalize the preceding argument to find the expected fraction of bins with
r balls for any constant r. The probability that a given bin has r balls is
(
m
r

)(
1
n
) r (
1 −
1
n
) m − r
=
1
r!
m ( m −1)···( m − r +1)
nr
(
1 −
1
n
) m − r
.
When m and n are large compared to r , the second factor on the right-hand side is
approximately ( m / n ) r , and the third factor is approximately e− m / n. Hence the proba-
bility pr that a given bin has r balls is approximately

pr ≈
e− m / n ( m / n ) r
r!
, (5.2)
and the expected number of bins with exactly r balls is approximately npr. We formalize
this relationship in Section5.3.1.
The previous calculation naturally leads us to consider the following distribution.

Definition 5.1: A discrete Poisson random variable X with parameter μ is given by
the following probability distribution on j = 0 , 1 , 2 ,...:

Pr( X = j )=
e−μμ j
j!
.
(Note that Poisson random variables differ from Poisson trials, discussed in Sec-
tion4.2.1.)
Let us verify that the definition gives a proper distribution in that the probabilities
sum to 1:

∑∞
j = 0
Pr( X = j )=
∑∞
j = 0
e−μμ j
j!
=e−μ
∑∞
j = 0
μ j
j!
= 1 ,
where we have used the Taylor expansion e x =

∑∞
j = 0 ( xj / j !).
5.3the poisson distribution
Next we show that the expectation of this random variable isμ:
E [ X ]=
∑∞
j = 0
j Pr( X = j )
=
∑∞
j = 1
j
e−μμ j
j!
=μ
∑∞
j = 1
e−μμ j −^1
( j −1)!
=μ
∑∞
j = 0
e−μμ j
j!
=μ.
In the context of throwing m balls into n bins, the distribution of the number of balls in
a bin is approximately Poisson withμ= m / n , which is exactly the average number of
balls per bin, as one might expect.
An important property of Poisson distributions is given in the following lemma.

Lemma 5.2: The sum of a finite number of independent Poisson random variables is
a Poisson random variable.

Proof: We consider two independent Poisson random variables X and Y with means
μ 1 andμ 2 ; the case of more random variables is simply handled by induction. Now

Pr( X + Y = j )=
∑ j
k = 0
Pr(( X = k )∩( Y = j − k ))
=
∑ j
k = 0
e−μ^1 μ k 1
k!
e−μ^2 μ( 2 j − k )
( j − k )!
=
e−(μ^1 +μ^2 )
j!
∑ j
k = 0
j!
k !( j − k )!
μ k 1 μ( 2 j − k )
=
e−(μ^1 +μ^2 )
j!
∑ j
k = 0
(
j
k
)
μ k 1 μ( 2 j − k )
=
e−(μ^1 +μ^2 )(μ 1 +μ 2 ) j
j!
.
In the last equality we used the binomial theorem to simplify the summation.  /theta

We can also prove Lemma5.2using moment generating functions.
Lemma 5.3: The moment generating function of a Poisson random variable with
parameter μ is

Mx ( t )=eμ(e
t −1)
.
balls, bins, and random graphs
Proof: For any t ,

E [e tX ]=
∑∞
k = 0
e−μμ k
k!
e tk =eμ(e
t −1)∑∞
k = 0
e−μe
t
(μe t ) k
k!
=eμ(e
t −1)
.
 /theta
Given two independent Poisson random variables X and Y with meansμ 1 andμ 2 ,we
apply Theorem4.3to prove

MX + Y ( t )= MX ( t )· MY ( t )=e(μ^1 +μ^2 )(e
t −1)
,
which is the moment generating function of a Poisson random variable with meanμ 1 +
μ 2. By Theorem4.2, the moment generating function uniquely defines the distribution,
and hence the sum X + Y is a Poisson random variable with meanμ 1 +μ 2.
We can also use the moment generating function of the Poisson distribution to prove
that E [ X^2 ]=λ(λ+1) and Var [ X ]=λ(see Exercise5.5).
Next we develop a Chernoff bound for Poisson random variables that we will use
later in this chapter.

Theorem 5.4: Let X be a Poisson random variable with parameter μ.

1. If x >μ , then

Pr( X ≥ x )≤
e−μ(eμ) x
xx
;
2. If x <μ , then

Pr( X ≤ x )≤
e−μ(eμ) x
xx
.
3. For δ> 0 ,

Pr( X ≥(1+δ)μ)≤
(
eδ
(1+δ)(1+δ)
)μ
;
4. For 0 <δ< 1 ,

Pr( X ≤(1−δ)μ)≤
(
e−δ
(1−δ)(1−δ)
)μ
.
Proof: For any t >0 and x >μ,

Pr( X ≥ x )=Pr(e tX ≥e tx )≤
E [e tX ]
e tx
.
Plugging in the expression for the moment generating function of the Poisson distribu-
tion, we have

Pr( X ≥ x )≤eμ(e
t −1)− xt
.
Choosing t =ln( x /μ)>0gives

Pr( X ≥ x )≤e x −μ− x ln( x /μ)
=
e−μ(eμ) x
xx
.
5.3the poisson distribution
For any t <0 and x <μ,

Pr( X ≤ x )=Pr(e tX ≥e tx )≤
E [e tX ]
e tx
Hence

Pr( X ≤ x )≤eμ(e
t −1)− xt
Choosing t =ln( x /μ)<0, it follows that

Pr( X ≤ x )≤e x −μ− x ln( x /μ)
=
e−μ(eμ) x
xx
The alternate forms of the bound given in parts 3 and 4 follow immediately from parts
1 and 2.  /theta

5.3.1 Limit of the Binomial Distribution
We have shown that, when throwing m balls randomly into n bins, the probability pr
that a bin has r balls is approximately the Poisson distribution with mean m / n. In gen-
eral, the Poisson distribution is the limit distribution of the binomial distribution with
parameters n and p , when n is large and p is small. More precisely, we have the follow-
ing limit result.

Theorem 5.5: Let Xnbe a binomial random variable with parameters n and p, where
p is a function of n and lim n →∞ np =λ is a constant that is independent of n. Then,
for any fixed k,

n lim→∞Pr( Xn = k )=
e−λλ k
k!
.
This theorem directly applies to the balls-and-bins scenario. Consider the situation
where there are m balls and n bins, where m is a function of n and lim m →∞ m / n =λ.
Let Xm be the number of balls in a specific bin. Then Xm is a binomial random variable
with parameters m and 1/ n. Theorem5.5thus applies and says that

m lim→∞Pr( Xm = r )=
e− m / n ( m / n ) r
r!
,
matching the approximation of Eqn. (5.2).
Before proving Theorem5.5, we describe some of its applications. Distributions of
this type arise frequently and are often modeled by Poisson distributions. For example,
consider the number of spelling or grammatical mistakes in a book, including this book.
One model for such mistakes is that each word is likely to have an error with some very
small probability p. The number of errors is then a binomial random variable with large
n and small p that can therefore be treated as a Poisson random variable. As another
example, consider the number of chocolate chips inside a chocolate chip cookie. One
possible model is to split the volume of the cookie into a large number of small disjoint
compartments, so that a chip lands in each compartment with some probability p. With
this model, the number of chips in a cookie roughly follows a Poisson distribution.

balls, bins, and random graphs
We will see similar applications of the Poisson distribution in continuous settings in
Chapter 8.

Proof of Theorem5.5: We can write

Pr( Xn = k )=
(
n
k
)
pk (1− p ) n − k.
In what follows, we make use of the bound that, for| x |≤1,

e x (1− x^2 )≤ 1 + x ≤e x , (5.3)
which follows from the Taylor series expansion of e x. (This is left as Exercise5.7.)
Then

Pr( Xn = k )≤
nk
k!
pk
(1− p ) n
(1− p ) k
≤
( np ) k
k!
e− pn
1 − pk
=
e− pn ( np ) k
k!
1
1 − pk
.
The second line follows from the first by Eqn. (5.3) and the fact that (1− p ) k ≥ 1 − pk
for k ≥0. Also,

Pr( Xn = k )≥
( n − k +1) k
k!
pk (1− p ) n
≥
(( n − k +1) p ) k
k!
e− pn (1− p^2 ) n
≥
e− pn (( n − k +1) p ) k
k!
(1− p^2 n ),
where in the second inequality we applied Eqn. (5.3) with x =− p.
Combining, we have

e− pn ( np ) k
k!
1
1 − pk
≥Pr( Xn = k )≥
e− pn (( n − k +1) p ) k
k!
(1− p^2 n ).
In the limit, as n approaches infinity, p approaches zero because the limiting value of
pn is the constantλ. Hence 1/(1− pk ) approaches 1, 1− p^2 n approaches 1, and the
difference between ( n − k +1) p and np approaches 0. It follows that

n lim→∞
e− pn ( np ) k
k!
1
1 − pk
=
e−λλ k
k!
and

n lim→∞
e− pn (( n − k +1) p ) k
k!
(1− p^2 n )=
e−λλ k
k!
.
Since lim n →∞Pr( Xn = k ) lies between these two values, the theorem follows.  /theta

5.4 The Poisson Approximation
5.4. The Poisson Approximation
The main difficulty in analyzing balls-and-bins problems is handling the dependencies
that naturally arise in such systems. For example, if we throw m balls into n bins and find
that bin 1 is empty, then it is less likely that bin 2 is empty because we know that the m
balls must now be distributed among n −1 bins. More concretely: if we know the num-
ber of balls in the first n −1 bins, then the number of balls in the last bin is completely
determined. The loads of the various bins are not independent, and independent random
variables are generally much easier to analyze, since we can apply Chernoff bounds. It
is therefore useful to have a general way to circumvent these sorts of dependencies.
We have already shown that, after throwing m balls independently and uniformly at
random into n bins, the distribution of the number of balls in a given bin is approxi-
mately Poisson with mean m / n. We would like to say that the joint distribution of the
number of balls in all the bins is well approximated by assuming the load at each bin is
an independent Poisson random variable with mean m / n. This would allow us to treat
bin loads as independent random variables. We show here that we can do this when
we are concerned with sufficiently rare events. Specifically, we show in Corollary5.9
that taking the probability of an event using this Poisson approximation for all of the
bins and multiplying it by e

√
m gives an upper bound for the probability of the event
when m balls are thrown into n bins. For rare events, this extra e

√
m factor will not be
significant. To achieve this result, we now introduce some technical machinery.
Suppose that m balls are thrown into n bins independently and uniformly at random,
and let Xi ( m )be the number of balls in the i th bin, where 1≤ i ≤ n. Let Y 1 ( m ),..., Yn ( m )be
independent Poisson random variables with mean m / n. We derive a useful relationship
between these two sets of random variables. Tighter bounds for specific problems can
often be obtained with more detailed analysis, but this approach is quite general and
easy to apply.
The difference between throwing m balls randomly and assigning each bin a number
of balls that is Poisson distributed with mean m / n is that, in the first case, we know there
are m balls in total, whereas in the second case we know only that m is the expected
number of balls in all of the bins. But suppose when we use the Poisson distribution
we end up with m balls. In this case, we do indeed have that the distribution is the same
as if we threw m balls into n bins randomly.

Theorem 5.6: The distribution of ( Y 1 ( m ),..., Yn ( m )) conditioned on

∑
iY
( m )
i = k is the
same as ( X 1 ( k ),..., Xn ( k )) , regardless of the value of m.

Proof: When throwing k balls into n bins, the probability that ( X 1 ( k ),..., Xn ( k ))=
( k 1 ,..., kn ) for any k 1 ,..., kn satisfying

∑
iki = k is given by
(
k
k 1 ; k 2 ;...; kn
)
nk
=
k!
( k 1 !)( k 2 !)···( kn !) nk
.
Now, for any k 1 ,..., kn with
∑
iki = k , consider the probability that
(
Y 1 ( m ),..., Yn ( m )
)
=( k 1 ,..., kn )
balls, bins, and random graphs
conditioned on ( Y 1 ( m ),..., Yn ( m )) satisfying

∑
iY
( m )
i = k :
Pr
(
(
Y 1 ( m ),..., Yn ( m )
)
=( k 1 ,..., kn )
∣
∣
∑ n
i = 1
Yi ( m )= k
)
=
Pr
((
Y 1 ( m )= k 1
)
∩
(
Y 1 ( m )= k 2
)
∩···∩
(
Yn ( m )= kn
))
Pr
(∑ n
i = 1 Y
( m )
i = k
).
The probability that Yi ( m )= ki is e− m / n ( m / n ) ki / ki !, since the Yi ( m )are independent Pois-
son random variables with mean m / n. Also, by Lemma5.2, the sum of the Yi ( m )is itself
a Poisson random variable with mean m. Hence

Pr
((
Y 1 ( m )= k 1
)
∩
(
Y 1 ( m )= k 2
)
∩···∩
(
Yn ( m )= kn
))
Pr
(∑ n
i = 1 Y
( m )
i = k
) =
∏ n
i = 1 e
− m / n ( m / n ) ki / ki!
e− mmk / k!
=
k!
( k 1 !)( k 2 !)···( kn !) nk
,
proving the theorem.  /theta

With this relationship between the two distributions, we can prove strong results about
any function on the loads of the bins.

Theorem 5.7: Let f ( x 1 ,..., xn ) be a nonnegative function. Then

E
[
f
(
X 1 ( m ),..., Xn ( m )
)]
≤e
√
m E
[
f
(
Y 1 ( m ),..., Yn ( m )
)]
. (5.4)
Proof: We have that

E
[
f
(
Y 1 ( m ),..., Yn ( m )
)]
=
∑∞
k = 0
E
[
f
(
Y 1 ( m ),..., Yn ( m )
)∣∣∑ n
i = 1
Yi ( m )= k
]
Pr
( n
∑
i = 1
Yi ( m )= k
)
≥ E
[
f
(
Y 1 ( m ),..., Yn ( m )
)∣∣∑ n
i = 1
Yi ( m )= m
]
Pr
( n
∑
i = 1
Yi ( m )= m
)
= E [ f
(
X 1 ( m ),..., Xn ( m )
)
]Pr
(∑
Yi ( m )= m
)
,
where the last equality follows from the fact that the joint distribution of the∑ Yi ( m )given
n
i = 1 Y

( m )
i = m is exactly that of the X
( m )
i , as shown in Theorem5.6. Since
∑ n
i = 1 Y
( m )
i
is Poisson distributed with mean m ,wenowhave

E
[
f
(
Y 1 ( m ),..., Yn ( m )
)]
≥ E
[
f
(
X 1 ( m ),..., Xn ( m )
)] mm e− m
m!
.
We use the following loose bound on m !, which we prove as Lemma5.8:
m !<e
√
m
(
m
e
) m
.
This yields

E
[
f
(
Y 1 ( m ),..., Yn ( m )
)]
≥ E
[
f
(
X 1 ( m ),..., Xn ( m )
)] 1
e
√
m
,
and the theorem is proven.  /theta

5.4 the poisson approximation
We prove the upper bound we used for factorials, which closely matches the loose lower
bound we used in Lemma5.1.

Lemma 5.8:

n !≤e
√
n
(
n
e
) n
. (5.5)

Proof: We use the fact that

ln( n !)=
∑ n
i = 1
ln i.
We first claim that, for i ≥2,
∫ i

i − 1
ln xdx ≥
ln( i −1)+ln i
2
.
This follows from the fact that ln x is concave, since its second derivative is− 1 / x^2 ,
which is always negative. Therefore,

∫ n
1
ln xdx ≥
∑ n
i = 1
ln i −
ln n
2
or, equivalently,

n ln n − n + 1 ≥ln( n !)−
ln n
2
.
The result now follows simply by exponentiating.  /theta

Theorem5.7holds for any nonnegative function on the number of balls in the bins. In
particular, if the function is the indicator function that is 1 if some event occurs and 0
otherwise, then the theorem gives bounds on the probability of events. Let us call the
scenario in which the number of balls in the bins are taken to be independent Poisson
random variables with mean m / n the Poisson case, and the scenario where m balls are
thrown into n bins independently and uniformly at random the exact case.

Corollary 5.9: Any event that takes place with probability p in the Poisson case takes
place with probability at most p e

√
m in the exact case.
Proof: Let f be the indicator function of the event. In this case, E [ f ] is just the
probability that the event occurs, and the result follows immediately from Theorem
5.7.  /theta

This is a quite powerful result. It says that any event that happens with small proba-
bility in the Poisson case also happens with small probability in the exact case, where
balls are thrown into bins. Since in the analysis of algorithms we often want to show
that certain events happen with small probability, this result says that we can utilize an

balls, bins, and random graphs
analysis of the Poisson approximation to obtain a bound for the exact case. The Pois-
son approximation is easier to analyze because the numbers of balls in each bin are
independent random variables.^1
We can actually do even a little bit better in many natural cases. Part of the proof of
the following theorem is outlined in Exercises5.14and5.15.

Theorem 5.10: Let f ( x 1 ,..., xn ) be a nonnegative function such that
E [ f ( X 1 ( m ),..., Xn ( m ))] is either monotonically increasing or monotonically decreasing
in m. Then

E
[
f
(
X 1 ( m ),..., Xn ( m )
)]
≤ 2 E
[
f
(
Y 1 ( m ),..., Yn ( m )
)]
. (5.6)
The following corollary is immediate.

Corollary 5.11: Let E be an event whose probability is either monotonically increas-
ing or monotonically decreasing in the number of balls. If E has probability p in the
Poisson case, then E has probability at most 2 p in the exact case.

To demonstrate the utility of this corollary, we again consider the maximum load prob-
lem for the case m = n. We have shown via a union bound argument that the maximum
load is at most 3 ln n /ln ln n with high probability. Using the Poisson approximation,
we prove the following almost-matching lower bound on the maximum load.

Lemma 5.12: When n balls are thrown independently and uniformly at random into
n bins, the maximum load is at least ln n /ln ln n with probability at least 1 − 1 / n for n
sufficiently large.

Proof: In the Poisson case, the probability that bin 1 has load at least M =ln n /ln ln n
is at least 1/e M !, which is the probability it has load exactly M. In the Poisson case,
all bins are independent, so the probability that no bin has load at least M is at
most
(
1 −

1
e M!
) n
≤e− n /(e M !).
We now need to choose M so that e− n /(e M !)≤ n −^2 , for then (by Theorem5.7)wewill
have that the probability that the maximum load is not at least M in the exact case is at
most e

√
n / n^2 < 1 / n. This will give the lemma. Because the maximum load is clearly
monotonically increasing in the number of balls, we could also apply the slightly better
Theorem5.10, but this would not affect the argument substantially.
It therefore suffices to show that M !≤ n /2e ln n , or equivalently that ln M !≤ln n −
ln ln n −ln(2e). From our bound of Eqn. (5.5), it follows that

M !≤e
√
M
(
M
e
) M
≤ M
(
M
e
) M
(^1) There are other ways to handle the dependencies in the balls-and-bins model. In Chapter 13 we describe a more
general way to deal with dependencies (using martingales) that applies here. Also, there is a theory of negative
dependence that applies to balls-and-bins problems that also allows these dependencies to be dealt with nicely.

5.4 the poisson approximation
when n (and hence M =ln n /ln ln n ) are suitably large. Hence, for n suitably large,

ln M !≤ M ln M − M +ln M
=
ln n
ln ln n
(ln ln n −ln ln ln n )−
ln n
ln ln n
+(ln ln n −ln ln ln n )
≤ln n −
ln n
ln ln n
≤ln n −ln ln n −ln(2e),
where in the last two inequalities we have used the fact that ln ln n = o (ln n /ln ln n ). /theta

5.4.1 ∗ Example: Coupon Collector’s Problem, Revisited
The coupon collector’s problem introduced in Section2.4.1can be thought of as a balls-
and-bins problem. Recall that in this problem there are n different types of coupons,
each cereal box yields a coupon chosen independently and uniformly at random from
the n types, and you need to buy cereal boxes until you collect one of each coupon.
If we think of coupons as bins and cereal boxes as balls, the question becomes: If
balls are thrown independently and uniformly at random into bins, how many balls are
thrown until all bins have at least one ball? We showed in Section2.4.1that the expected
number of cereal boxes necessary is nH ( n )≈ n ln n ; in Section3.3.1we showed that, if
there are n ln n + cn cereal boxes, then the probability that not all coupons are collected
is at most e− c. These results translate immediately to the balls-and-bins setting. The
expected number of balls that must be thrown before each bin has at least one ball is
nH ( n ), and when n ln n + cn balls are thrown the probability that not all bins have at
least one ball is e− c.
We have seen in Chapter 4 that Chernoff bounds yield concentration results for sums
of independent 0–1 random variables. We will use here a Chernoff bound for the Pois-
son distribution to obtain much stronger results for the coupon collector’s problem.

Theorem 5.13: Let X be the number of coupons observed before obtaining one of each
of n types of coupons. Then, for any constant c,

n lim→∞Pr[ X > n ln n + cn ]=^1 −e−e
− c
.
This theorem states that, for large n , the number of coupons required should be very
close to n ln n. For example, over 98% of the time the number of coupons required lies
between n ln n − 4 n and n ln n + 4 n. This is an example of a sharp threshold, where
the random variable is closely concentrated around its mean.

Proof: We look at the problem as a balls-and-bins problem. We begin by considering
the Poisson approximation, and then demonstrate that the Poisson approximation gives
the correct answer in the limit. For the Poisson approximation, we suppose that the
number of balls in each bin is a Poisson random variable with mean ln n + c , so that
the expected total number of balls is m = n ln n + cn. The probability that a specific
bin is empty is then

e−(ln n + c )=
e− c
n
.
balls, bins, and random graphs
Since all bins are independent under the Poisson approximation, the probability that no
bin is empty is
(
1 −

e− c
n
) n
≈e−e
− c
.
The last approximation is appropriate in the limit as n grows large, so we apply it here.
To show the Poisson approximation is accurate, we undertake the following steps.
Consider the experiment where each bin has a Poisson number of balls, each with mean
ln n + c. LetEbe the event that no bin is empty, and let X be the number of balls thrown.
We have seen that

lim
n →∞
Pr(E)=e−e
− c
.
We use Pr(E) by splitting it as follows:

Pr(E)=Pr
(
E∩
(
| X − m |≤
√
2 m ln m
))
+Pr
(
E∩
(
| X − m |>
√
2 m ln m
))
=Pr
(
E|| X − m |≤
√
2 m ln m
)
·Pr
(
| X − m |≤
√
2 m ln m
)
+Pr
(
E|| X − m |>
√
2 m ln m
)
·Pr
(
| X − m |>
√
2 m ln m
)
. (5.7)
This representation proves helpful once we establish two facts. First, we show that
Pr

(
| X − m |>
√
2 m ln m
)
is o (1); that is, the probability that in the Poisson case the
number of balls thrown deviates significantly from its mean m is o (1). This guarantees
that the second term in the summation on the right of Eqn. (5.7)is o (1). Second, we
show that
∣∣
Pr

(
E|| X − m |≤
√
2 m ln m
)
−Pr(E| X = m )
∣∣
= o (1).
That is, the difference between our experiment coming up with exactly m balls or just
almost m balls makes an asymptotically negligible difference in the probability that
every bin has a ball. With these two facts, Eqn. (5.7) becomes

Pr(E)=Pr
(
E|| X − m |≤
√
2 m ln m
)
·Pr
(
| X − m |≤
√
2 m ln m
)
+Pr
(
E|| X − m |>
√
2 m ln m
)
·Pr
(
| X − m |>
√
2 m ln m
)
=Pr
(
E|| X − m |≤
√
2 m ln m
)
·(1− o (1))+ o (1)
=Pr(E| X = m )(1− o (1))+ o (1),
and hence

n lim→∞Pr(E)= n lim→∞Pr(E| X = m ).
But from Theorem5.6, the quantity on the right is equal to the probability that every
bin has at least one ball when m balls are thrown randomly, since conditioning on m
total balls with the Poisson approximation is equivalent to throwing m balls randomly
into the n bins. As a result, the theorem follows once we have shown these two facts.
To show that Pr

(
| X − m |>
√
2 m ln m
)
is o (1), consider that X is a Poisson ran-
dom variable with mean m , since it is a sum of independent Poisson random variables.
We use the Chernoff bound for the Poisson distribution (Theorem5.4) to bound this

5.5 Application: Hashing
probability, writing the bound as

Pr( X ≥ x )≤e x − m − x ln( x / m ).
For x = m +

√
2 m ln m , we use that ln(1+ z )≥ z − z^2 /2for z ≥0toshow
Pr
(
X > m +
√
2 m ln m
)
≤e
√
2 m ln m −( m +
√
2 m ln m )ln(1+
√
2ln m / m )
≤e
√ 2 m ln m −( m +√ 2 m ln m )(√2ln m / m −ln m / m )
=e−ln m +
√ 2 m ln m (ln m / m )
= o (1).
A similar argument holds if x < m ,soPr

(
| X − m |>
√
2 m ln m
)
= o (1).
We now show the second fact, that
∣∣
Pr
(
E|| X − m |≤
√
2 m ln m
)
−Pr(E| X = m )
∣∣
= o (1).
Note that Pr(E| X = k ) is increasing in k , since this probability corresponds to the
probability that all bins are nonempty when k balls are thrown independently and uni-
formly at random. The more balls that are thrown, the more likely all bins are nonempty.
It follows that

Pr
(
E| X = m −
√
2 m ln m
)
≤Pr
(
E|| X − m |≤
√
2 m ln m
)
≤Pr
(
E| X = m +
√
2 m ln m
)
Hence we have the bound
∣∣
Pr(E|| X − m |≤

√
2 m ln m
)
−Pr(E| X = m )
∣∣
≤Pr
(
E| X = m +
√
2 m ln m
)
−Pr
(
E| X = m −
√
2 m ln m
)
,
and we show the right-hand side is o (1). This is the difference between the proba-
bility that all bins receive at least one ball when m −

√
2 m ln m balls are thrown and
when m +

√
2 m ln m balls are thrown. This difference is equivalent to the probability of
the following experiment: we throw m −

√
2 m ln m balls and there is still at least one
empty bin, but after throwing an additional 2

√
2 m ln m balls, all bins are nonempty.
In order for this to happen, there must be at least one empty bin after m −

√
2 m ln m
balls; the probability that one of the next 2

√
(^2 m ln m balls covers this bin is at most
2

√
2 m ln m
)
/ n = o (1) by the union bound. Hence this difference is o (1) as well.  /theta
5.5. Application: Hashing
5.5.1 Chain Hashing
The balls-and-bins-model is also useful for modeling hashing. For example, consider
the application of a password checker, which prevents people from using common,
easily cracked passwords by keeping a dictionary of unacceptable passwords. When
a user tries to set up a password, the application would like to check if the requested
password is part of the unacceptable set. One possible approach for a password checker
would be to store the unacceptable passwords alphabetically and do a binary search on
the dictionary to check if a proposed password is unacceptable. A binary search would
require /eta(log m ) time for m words.

balls, bins, and random graphs
Another possibility is to place the words into bins and then search the appropriate bin
for the word. The words in a bin would be represented by a linked list. The placement
of words into bins is accomplished by using a hash function. A hash function f from a
universe U into a range [0, n −1] can be thought of as a way of placing items from the
universe into n bins. Here the universe U would consist of possible password strings.
The collection of bins is called a hash table. This approach to hashing is called chain
hashing, since items that fall in the same bin are chained together in a linked list.
Using a hash table turns the dictionary problem into a balls-and-bins problem. If our
dictionary of unacceptable passwords consists of m words and the range of the hash
function is [0, n −1], then we can model the distribution of words in bins with the
same distribution as m balls placed randomly in n bins. We are making a rather strong
assumption by presuming that our hash function maps words into bins in a fashion
that appears random, so that the location of each word is independent and identically
distributed. There is a great deal of theory behind designing hash functions that appear
random, and we will not delve into that theory here. We simply model the problem by
assuming that hash functions are random. In other words, we assume that (a) for each
x ∈ U , the probability that f ( x )= j is 1/ n (for 0≤ j ≤ n −1) and that (b) the values
of f ( x ) for each x are independent of each other. Notice that this does not mean that
every evaluation of f ( x ) yields a different random answer! The value of f ( x )isfixed
for all time; it is just equally likely to take on any value in the range.
Let us consider the search time when there are n bins and m words. To search for an
item, we first hash it to find the bin that it lies in and then search sequentially through
the linked list for it. If we search for a word that is not in our dictionary, the expected
number of words in the bin the word hashes to is m / n. If we search for a word that is in
our dictionary, the expected number of other words in that word’s bin is ( m −1)/ n ,so
the expected number of words in the bin is 1+( m −1)/ n. If we choose n = m bins for
our hash table, then the expected number of words we must search through in a bin is
constant. If the hashing takes constant time, then the total expected time for the search
is constant.
The maximum time to search for a word, however, is proportional to the maximum
number of words in a bin. We have shown that when n = m this maximum load is
 /eta(ln n /ln ln n ) with probability close to 1, and hence with high probability this is the
maximum search time in such a hash table. While this is still faster than the required
time for standard binary search, it is much slower than the average, which can be a
drawback for many applications.
Another drawback of chain hashing can be wasted space. If we use n bins for n items,
several of the bins will be empty, potentially leading to wasted space. The space wasted
can be traded off against the search time by making the average number of words per
bin larger than 1.

5.5.2 Hashing: Bit Strings
If we want to save space instead of time, we can use hashing in another way. Again, we
consider the problem of keeping a dictionary of unsuitable passwords. Assume that a
password is restricted to be eight ASCII characters, which requires 64 bits (8 bytes) to

5.5application: hashing
represent. Suppose we use a hash function to map each word into a 32-bit string. This
string will serve as a short fingerprint for the word; just as a fingerprint is a succinct way
of identifying people, the fingerprint string is a succinct way of identifying a word. We
keep the fingerprints in a sorted list. To check if a proposed password is unacceptable,
we calculate its fingerprint and look for it on the list, say by a binary search.^2 If the
fingerprint is on the list, we declare the password unacceptable.
In this case, our password checker may not give the correct answer! It is possible for
a user to input an acceptable password, only to have it rejected because its fingerprint
matches the fingerprint of an unacceptable password. Hence there is some chance that
hashing will yield a false positive : it may falsely declare a match when there is not an
actual match. The problem is that – unlike fingerprints for human beings – our finger-
prints do not uniquely identify the associated word. This is the only type of mistake this
algorithm can make; it does not allow a password that is in the dictionary of unsuitable
passwords. In the password application, allowing false positives means our algorithm
is overly conservative, which is probably acceptable. Letting easily cracked passwords
through, however, would probably not be acceptable.
To place the problem in a more general context, we describe it as an approximate
set membership problem. Suppose we have a set S ={ s 1 , s 2 ,..., sm }of m elements
from a large universe U. We would like to represent the elements in such a way that
we can quickly answer queries of the form “is x an element of S ?” We would also like
the representation to take as little space as possible. In order to save space, we would
be willing to allow occasional mistakes in the form of false positives. Here the unal-
lowable passwords correspond to our set S.
How large should the range of the hash function used to create the fingerprints be?
Specifically, if we are working with bits, how many bits should we use to create a
fingerprint? Obviously, we want to choose the number of bits that gives an acceptable
probability for a false positive match. The probability that an acceptable password has a
fingerprint that is different from any specific unallowable password in S is (1− 1 / 2 b ).
It follows that if the set S has size m and if we use b bits for the fingerprint, then
the probability of a false positive for an acceptable password is 1−(1− 1 / 2 b ) m ≥
1 −e− m /^2 b. If we want this probability of a false positive to be less than a constant c ,
we need
e− m /^2
b
≥ 1 − c ,
which implies that
b ≥log 2
m
ln(1/(1− c ))
.
That is, we need b = m(log 2 m ) bits. On the other hand, if we use b =2 log 2 m bits,
then the probability of a false positive falls to
1 −
(
1 −
1
m^2
) m
<
1
m
.
(^2) In this case the fingerprints will be uniformly distributed over all 32-bit strings. There are faster algorithms
for searching over sets of numbers with this distribution, just as Bucket sort allows faster sorting than standard
comparison-based sorting when the elements to be sorted are from a uniform distribution, but we will not concern
ourselves with this point here.

balls, bins, and random graphs
In our example, if our dictionary has 2^16 = 65 ,536 words, then using 32 bits when
hashing yields a false positive probability of just less than 1/ 65 ,536.

5.5.3 Bloom Filters
We can generalize the hashing ideas of Sections5.5.1and5.5.2to achieve more inter-
esting trade-offs between the space required and the false positive probability. The
resulting data structure for the approximate set membership problem is called a Bloom
filter.
A Bloom filter consists of an array of n bits, A [0] to A [ n −1], initially all set
to 0. A Bloom filter uses k independent random hash functions h 1 ,..., hk with range
{ 0 ,..., n − 1 }. We make the usual assumption for analysis that these hash functions
map each element in the universe to a random number independently and uniformly
over the range{ 0 ,..., n − 1 }. Suppose that we use a Bloom filter to represent a set
S ={ s 1 , s 2 ,..., sm }of m elements from a large universe U. For each element s ∈ S ,
the bits A [ hi ( s )] are set to 1 for 1≤ i ≤ k. A bit location can be set to 1 multiple times,
but only the first change has an effect. To check if an element x is in S , we check whether
all array locations A [ hi ( x )] for 1≤ i ≤ k are set to 1. If not, then clearly x is not a mem-
ber of S , because if x were in S then all locations A [ hi ( x )] for 1≤ i ≤ k would be set
to 1 by construction. If all A [ hi ( x )] are set to 1, we assume that x is in S , although
we could be wrong. We would be wrong if all of the positions A [ hi ( x )] were set to 1
by elements of S even though x is not in the set. Hence Bloom filters may yield false
positives. Figure5.1shows an example.
The probability of a false positive for an element not in the set – the false positive
probability – can be calculated in a straightforward fashion, given our assumption that
the hash functions are random. After all the elements of S are hashed into the Bloom
filter, the probability that a specific bit is still 0 is
(
1 −

1
n
) km
≈e− km / n.
We let p =e− km / n. To simplify the analysis, let us temporarily assume that a fraction
p of the entries are still 0 after all of the elements of S are hashed into the Bloom
filter.
The probability of a false positive is then
⎛
⎝ 1 −

(
1 −
1
n
) km ⎞
⎠
k
≈(1−e− km / n ) k =(1− p ) k.
We let f =(1−e− km / n ) k =(1− p ) k. From now on, for convenience we use the
asymptotic approximations p and f to represent (respectively) the probability that a
bit in the Bloom filter is 0 and the probability of a false positive.
Suppose that we are given m and n and wish to optimize the number of hash func-
tions k in order to minimize the false positive probability f. There are two competing
forces: using more hash functions gives us more chances to find a 0-bit for an element
that is not a member of S , but using fewer hash functions increases the fraction of 0-bits

5.5application: hashing
Figure 5.1: Example of how a Bloom filter functions.
in the array. The optimal number of hash functions that minimizes f as a function of k
is easily found taking the derivative. Let g = k ln(1−e− km / n ), so that f =e g and mini-
mizing the false positive probability f is equivalent to minimizing g with respect to k .We
find

dg
dk
=ln(1−e− km / n )+
km
n
e− km / n
1 −e− km / n
.
It is easy to check that the derivative is zero when k =(ln 2)·( n / m ) and that this
point is a global minimum. In this case the false positive probability f is (1/2) k ≈
(0.6185) n / m. The false positive probability falls exponentially in n / m , the number of
bits used per item. In practice, of course, k must be an integer, so the best possible
choice of k may lead to a slightly higher false positive rate.
A Bloom filter is like a hash table, but instead of storing set items we simply use one
bit to keep track of whether or not an item hashed to that location. If k =1, we have
just one hash function and the Bloom filter is equivalent to a hashing-based fingerprint
system, where the list of the fingerprints is stored in a 0–1 bit array. Thus Bloom filters
can be seen as a generalization of the idea of hashing-based fingerprints. As we saw
when using fingerprints, to get even a small constant probability of a false positive
required m(log m ) fingerprint bits per item. In many practical applications, m(log m )
bits per item can be too many. Bloom filters allow a constant probability of a false
positive while keeping n / m , the number of bits of storage required per item, constant.
For many applications, the small space requirements make a constant probability of

balls, bins, and random graphs
error acceptable. For example, in the password application, we may be willing to accept
false positive rates of 1% or 2%.
Bloom filters are highly effective even if n = cm for a small constant c , such as
c =8. In this case, when k =5or k =6 the false positive probability is just over 0.02.
This contrasts with the approach of hashing each element into /eta(log m ) bits. Bloom
filters require significantly fewer bits while still achieving a very good false positive
probability.
It is also interesting to frame the optimization another way. Consider f , the proba-
bility of a false positive, as a function of p .Wefind

f =(1− p ) k
=(1− p )(−ln p )( n / m )
=(e−ln( p )ln(1− p )) n / m. (5.8)
From the symmetry of this expression, it is easy to check that p = 1 /2 minimizes the
false positive probability f. Hence the optimal results are achieved when each bit of the
Bloom filter is 0 with probability 1/2. An optimized Bloom filter looks like a random
bit string.
To conclude, we reconsider our assumption that the fraction of entries that are still 0
after all of the elements of S are hashed into the Bloom filter is p. Each bit in the array
can be thought of as a bin, and hashing an item is like throwing a ball. The fraction of
entries that are still 0 after all of the elements of S are hashed is therefore equivalent to
the fraction of empty bins after mk balls are thrown into n bins. Let X be the number of
such bins when mk balls are thrown. The expected fraction of such bins is

p ′=
(
1 −
1
n
) km
The events of different bins being empty are not independent, but we can apply
Corollary5.9, along with the Chernoff bound of Eqn. (4.6), to obtain

Pr(| X − np ′|≥ε n )≤2e
√
n e− n ε
(^2) / 3 p ′
Actually, Corollary5.11applies as well, since the number of 0-entries – which corre-
sponds to the number of empty bins – is monotonically decreasing in the number of
balls thrown. The bound tells us that the fraction of empty bins is close to p ′(when
n is reasonably large) and that p ′is very close to p. Our assumption that the fraction
of 0-entries in the Bloom filter is p is therefore quite accurate for predicting actual
performance.

5.5.4 Breaking Symmetry
As our last application of hashing, we consider how hashing provides a simple way
to break symmetry. Suppose that n users want to utilize a resource, such as time on a
supercomputer. They must use the resource sequentially, one at a time. Of course, each
user wants to be scheduled as early as possible. How can we decide a permutation of
the users quickly and fairly?

5.6 Random Graphs
If each user has an identifying name or number, hashing provides one possible solu-
tion. Hash each user’s identifier into 2 b bits, and then take the permutation given by
the sorted order of the resulting numbers. That is, the user whose identifier gives the
smallest number when hashed comes first, and so on. For this approach to work, we do
not want two users to hash to the same value, since then we must decide again how to
order these users.
If b is sufficiently large, then with high probability the users will all obtain distinct
hash values. One can analyze the probability that two hash values collide by using the
analysis from Section5.1for the birthday paradox; hash values correspond to birthdays.
We here use a simpler analysis based just on using a union bound. There are

( n
2
)
pairs
of users. The probability that any specific pair has the same hash value is 1/ 2 b. Hence
the probability that any pair has the same hash value is at most

( n
2
) 1
2 b
=
n ( n −1)
2 b +^1
Choosing b =3 log 2 n −1 guarantees success with probability at least 1− 1 / n.
This solution is extremely flexible, making it useful for many situations in dis-
tributed computing. For example, new users can easily be added into the schedule
at any time, as long as they do not hash to the same number as another scheduled
user.
A related problem is leader election. Suppose that instead of trying to order all of
the users, we simply want to fairly choose a leader from them. Again, if we have a
suitably random hash function then we can simply take the user whose hash value is
the smallest. An analysis of this scheme is left as Exercise5.26.

5.6. Random Graphs
5.6.1 Random Graph Models
There are many NP-hard computational problems defined on graphs: Hamiltonian
cycle, independent set, vertex cover, and so forth. One question worth asking is whether
these problems are hard for most inputs or just for a relatively small fraction of all
graphs. Random graph models provide a probabilistic setting for studying such ques-
tions.
Most of the work on random graphs has focused on two closely related models, Gn , p
and Gn , N .In Gn , p we consider all undirected graphs on n distinct verticesv 1 ,v 2 ,...,v n.
A graph with a given set of m edges has probability

pm (1− p )(
n 2 )− m
.
One way to generate a random graph in Gn , p is to consider each of the

( n
2
)
possible edges
in some order and then independently add each edge to the graph with probability p.

balls, bins, and random graphs
The expected number of edges in the graph is therefore

( n
2
)
p , and each vertex has
expected degree ( n −1) p.
In the Gn , N model, we consider all undirected graphs on n vertices with exactly N

edges. There are

(
( n 2 )
N
)
possible graphs, each selected with equal probability. One way
to generate a graph uniformly from the graphs in Gn , N is to start with a graph with no
edges. Choose one of the

( n
2
)
possible edges uniformly at random and add it to the edges
in the graph. Now choose one of the remaining

( n
2
)
−1 possible edges independently
and uniformly at random and add it to the graph. Similarly, continue choosing one of
the remaining unchosen edges independently and uniformly at random until there are
N edges.
The Gn , p and Gn , N models are related; when p = N /

( n
2
)
, the number of edges in a
random graph in Gn , p is concentrated around N , and conditioned on a graph from Gn , p
having N edges, that graph is uniform over all the graphs from Gn , N. The relationship
is similar to the relationship between throwing m balls into n bins and having each bin
have a Poisson distributed number of balls with mean m / n.
Here, for example is one way of formalizing the relationship between the Gn , p and
Gn , N models. A graph property is a property that holds for a graph regardless of how
the vertices are labeled, so it holds for all possible isomorphisms of the graph. We say
that a graph property is monotone increasing if whenever the property holds for G =
( V , E ) it holds also for any graph G ′=( V , E ′) with E ⊆ E ′; monotone decreasing graph
properties are defined similarly. For example, the property that a graph is connected
is a monotone increasing graph property, as is the property that a graph contains a
connected component of at least k vertices for any particular value of k. The property
that a graph is a tree, however, is not a monotone graph property, although the property
that the graph contains no cycles is a monotone decreasing graph property. We have
the following lemma:

Lemma 5.14: For a given monotone increasing graph property let P ( n , N ) be the
probability that the property holds for a graph in Gn , Nand P ( n , p ) the probability that
it holds for a graph in Gn , p.Letp +=(1+ /theta) N /

( n
2
)
and p −=(1− /theta) N /
( n
2
)
for a
constant 1 > /theta> 0_. Then_

P ( n , p −)−e− O ( N )≤ P ( n , N )≤ P ( n , p +)+e− O ( N ).
Proof: Let X be a random variable giving the number of edges that occur when a graph
is chosen from Gn , p −. Conditioned on X = k , a random graph from Gn , p −is equivalent
to a graph from Gn , k , since the k edges chosen are equally likely to be any subset of k
edges. Hence

P ( n , p −)=
∑( n^2 )
k = 0
P ( n , k )Pr( X = k ).
In particular,

P ( n , p −)=
∑
k ≤ N
P ( n , k )Pr( X = k )+
∑
k > N
P ( n , k )Pr( X = k ).
5.6random graphs
Also, for a monotone increasing graph property, P ( n , k )≤ P ( n , N )for k ≤ N. Hence

P ( n , p −)≤Pr( X ≤ N ) P ( n , N )+Pr( X > N )≤ P ( n , N )+Pr( X > N ).
However, Pr(( X > N ) can be bounded by a standard Chernoff bound; X is the sum of
n
2

)
independent Bernoulli random variables, and hence by Theorem4.4
Pr( X > N )=Pr
(
X >
1
1 − /theta
E [ X ]
)
≤Pr( X >(1+ /theta) E [ X ])≤e−(1− /theta) /theta
(^2) N / 3
Here we have used that 1 −^1  /theta> 1 + /thetafor 0< /theta<1.
Similarly,
P ( n , p +)=

∑
k < N
P ( n , k )Pr( X = k )+
∑
k ≥ N
P ( n , k )Pr( X = k ),
so

P ( n , p +)≥Pr( X ≥ N ) P ( n , N )≥ P ( n , N )−Pr( X < N ).
By Theorem4.5

Pr( X > N )=Pr
(
X <
1
1 + /theta
E [ X ]
)
≤Pr
(
X <
(
1 +
 /theta
2
)
E [ X ]
)
≤e−(1+ /theta) /theta
(^2) N / 8
,
where here we have used that 1 +^1  /theta< 1 − /theta/2for0< /theta<1.  /theta
A similar result holds for monotone decreasing properties. Another formalization of
the relationship between the graph models is given as Exercise5.18.
There are many additional similarities between random graphs and the balls-and-
bins model. Throwing edges into the graph as in the Gn , N model is like throwing balls
into bins. However, since each edge has two endpoints, each edge is like throwing
two balls at once into two different bins. The pairing defined by the edges adds a rich
structure that does not exist in the balls-and-bins model. Yet we can often utilize the
relation between the two models to simplify analysis in random graph models. For
example, in the coupon collector’s problem we found that when we throw n ln n + cn
balls, the probability that there are no empty bins converges to e−e− c as n grows to
infinity. Similarly, we have the following theorem for random graphs, which is left as
Exercise5.20.
Theorem 5.15: Let N =^12 ( n ln n + cn ). Then the probability that there are no isolated
vertices ( vertices with degree 0) in Gn , Nconverges to e−e
− c
as n grows to infinity.

5.6.2 Application: Hamiltonian Cycles in Random Graphs
A Hamiltonian path in a graph is a path that traverses each vertex exactly once. A
Hamiltonian cycle is a cycle that traverses each vertex exactly once. We show an inter-
esting connection between random graphs and balls-and-bins problems by analyzing a
simple and efficient algorithm for finding Hamiltonian cycles in random graphs. The
algorithm is randomized, and its probabilistic analysis is over both the input distribu-
tion and the random choices of the algorithm. Finding a Hamiltonian cycle in a graph

balls, bins, and random graphs
Figure 5.2: The rotation of the pathv 1 ,v 2 ,v 3 ,v 4 ,v 5 ,v 6 with the edge (v 6 ,v 3 ) yields a new path
v 1 ,v 2 ,v 3 ,v 6 ,v 5 ,v 4.

is an NP-hard problem. However, our analysis of this algorithm shows that finding a
Hamiltonian cycle is not hard for suitably randomly selected graphs, even though it
may be hard to solve in general.
Our algorithm will make use of a simple operation called a rotation. Let G be an
undirected graph. Suppose that

P =v 1 ,v 2 ,...,v k
is a simple path in G and that (v k ,v i ) is an edge of G. Then

P ′=v 1 ,v 2 ,...,v i ,v k ,v k − 1 ,...,v i + 2 ,v i + 1
is also a simple path, which we refer to as the rotation of P with the rotation edge
(v k ,v i ); see Figure5.2.
We first consider a simple, natural algorithm that proves challenging to analyze. We
assume that our input is presented as a list of adjacent edges for each vertex in the graph,
with the edges of each list being given in a random order according to independent and
uniform random permutations. Initially, the algorithm chooses an arbitrary vertex to
start the path; this is the initial head of the path. The head is always one of the endpoints
of the path. From this point on, the algorithm either “grows” the path deterministically
from the head, or rotates the path – as long as there is an adjacent edge remaining on
the head’s list. See Algorithm5.1.
The difficulty in analyzing this algorithm is that, once the algorithm views some
edges in the edge lists, the distribution of the remaining edges is conditioned on the
edges the algorithm has already seen. We circumvent this difficulty by considering a
modified algorithm that, though less efficient, avoids this conditioning issue and so is
easier to analyze for the random graphs we consider. See Algorithm5.2. Each ver-
tex v keeps two lists. The list used-edges(v) contains edges adjacent to v that have
been used in the course of the algorithm while v was the head; initially this list is
empty. The list unused-edges(v) contains other edges adjacent to v that have not been
used.
We initially analyze the algorithm assuming a specific model for the initial unused-
edges lists. We subsequently relate this model to the Gn , p model for random graphs.
Assume that each of the n −1 possible edges connected to a vertex v is initially on the
unused-edges list for vertex v independently with some probability q. We also assume
these edges are in a random order. One way to think of this is that, before beginning
the algorithm, we create the unused-edges list for each vertex v by inserting each pos-
sible edge (v, u ) with probability q ; we think of the corresponding graph G as being
the graph including all edges that were inserted on some unused-edges list. Notice that

5.6random graphs
Hamiltonian Cycle Algorithm:

Input: A graph G =( V , E ) with n vertices.

Output: A Hamiltonian cycle, or failure.

1. Start with a random vertex as the head of the path.
2. Repeat the following steps until the rotation edge closes a Hamiltonian cycle or
the unused-edges list of the head of the path is empty:
(a) Let the current path be P =v 1 ,v 2 ,...,v k , wherev k is the head, and let
(v k , u ) be the first edge in the head’s list.
(b) Remove (v k , u ) from the head’s list and u ’s list.
(c) If u =v i for 1≤ i ≤ k , add u =v k + 1 to the end of the path and make it the
head.
(d) Otherwise, if u =v i , rotate the current path with (v k ,v i ) and setv i + 1 to be
the head. (This step closes the Hamiltonian path if k = n and the chosen
edge is (v n ,v 1 ).)
3. Return a Hamiltonian cycle if one was found or failure if no cycle was found.

Algorithm 5.1: Hamiltonian cycle algorithm.
Modified Hamiltonian Cycle Algorithm:

Input: A graph G =( V , E ) with n vertices and associated edge lists.

Output: A Hamiltonian cycle, or failure.

1. Start with a random vertex as the head of the path.
2. Repeat the following steps until the rotation edge closes a Hamiltonian cycle or
the unused-edges list of the head of the path is empty:
(a) Let the current path be P =v 1 ,v 2 ,...,v k , withv k being the head.
(b) Execute i, ii, or iii with probabilities 1/ n ,|used-edges(v k )|/ n , and
1 − 1 / n −|used-edges(v k )|/ n , respectively:
i. Reverse the path, and makev 1 the head.
ii. Choose uniformly at random an edge from used-edges(v k ); if the edge
is (v k ,v i ), rotate the current path with (v k ,v i ) and setv i + 1 to be the head.
(If the edge is (v k ,v k − 1 ), then no change is made.)
iii. Select the first edge from unused-edges(v k ), call it (v k , u ). If u =v i for
1 ≤ i ≤ k , add u =v k + 1 to the end of the path and make it the head.
Otherwise, if u =v i , rotate the current path with (v k ,v i ) and setv i + 1
to be the head. (This step closes the Hamiltonian path if k = n and the
chosen edge is (v n ,v 1 ).)
(c) Update the used-edges and unused-edges lists appropriately.
3. Return a Hamiltonian cycle if one was found or failure if no cycle was found.

Algorithm 5.2: Modified Hamiltonian cycle algorithm.
balls, bins, and random graphs
this means an edge (v, u ) could initially be on the unused-edges list for v but not for
u. Also, when an edge (v, u ) is first used in the algorithm, if v is the head then it is
removed just from the unused-edges list of v ; if the edge is on the unused-edges list for
u , it remains on this list.
By choosing the rotation edge from either the used-edges list or the unused-edges
list with appropriate probabilities and then reversing the path with some small proba-
bility in each step, we modify the rotation process so that the next head of the list is
chosen uniformly at random from among all vertices of the graph. Once we establish
this property, the progress of the algorithm can be analyzed through a straightforward
application of our analysis of the coupon collector’s problem.
The modified algorithm appears wasteful; reversing the path or rotating with one of
the used edges cannot increase the path length. Also, we may not be taking advantage
of all the possible edges of G at each step. The advantage of the modified algorithm is
that it proves easier to analyze, owing to the following lemma.

Lemma 5.16: Suppose the modified Hamiltonian cycle algorithm is run on a graph
chosen using the described model. LetVtbe the head vertex after the t th step. Then, for
any vertex u, as long as at the t th step there is at least one unused edge available at the
head vertex,

Pr( Vt + 1 = u | Vt = ut , Vt − 1 = ut − 1 , ..., V 0 = u 0 )= 1 / n.
That is, the head vertex can be thought of as being chosen uniformly at random from
all vertices at each step, regardless of the history of the process.

Proof: Consider the possible cases when the path is P =v 1 ,v 2 ,...,v k.
The only wayv 1 can become the head is if the path is reversed, so Vt + 1 =v 1 with
probability 1/ n.
If u =v i + 1 is a vertex that lies on the path and (v k ,v i ) is in used-edges(v k ), then the
probability that Vt + 1 = u is

|used-edges(v k )|
n
1
|used-edges(v k )|
=
1
n
.
If u is not covered by one of the first two cases then we use the fact that, when
an edge is chosen from unused-edges(v k ), the adjacent vertex is uniform over all
the n −|used-edges(v k )|−1 remaining vertices. This follows from the principle of
deferred decisions. Our initial setup required the unused-edges list forv k to be con-
structed by including each possible edge with probability q and randomizing the order
of the list. This is equivalent to choosing X neighboring vertices forv k , where X is
a B ( n − 1 , q ) random variable and the X vertices are chosen uniformly at random
without replacement. Becausev k ’s list was determined independently from the lists
of the other vertices, the history of the algorithm tells us nothing about the remaining
edges in unused-edges(v k ), and the principle of deferred decisions applies. Hence any
edge inv k ’s unused-edges list that we have not seen is by construction equally likely
to connect to any of the n −|used-edges(v k )|−1 remaining possible neighboring
vertices.

5.6random graphs
If u =v i + 1 is a vertex on the path but (v k ,v i ) is not in used-edges(v k ), then the
probability that Vt + 1 = u is the probability that the edge (v k ,v i ) is chosen from unused-
edges(v k ) as the next rotation edge, which is
(
1 −

1
n
−
|used-edges(v k )|
n
)(
1
n −|used-edges(v k )|− 1
)
=
1
n
. (5.9)
Finally, if u is not on the path, then the probability that Vt + 1 = u is the probability that
the edge (v k + 1 , u ) is chosen from unused-edges(v k ). But this has the same probability
as in Eqn. (5.9).  /theta

For Algorithm5.2, the problem of finding a Hamiltonian path looks exactly like the
coupon collector’s problem; the probability of finding a new vertex to add to the path
when there are k vertices left to be added is k / n. Once all the vertices are on the
path, the probability that a cycle is closed in each rotation is 1/ n. Hence, if no list
of unused-edges is exhausted then we can expect a Hamiltonian path to be formed in
about O ( n ln n ) rotations, with about another O ( n ln n ) rotations to close the path to
form a Hamiltonian cycle. More concretely, we can prove the following theorem.

Theorem 5.17: Suppose the input to the modified Hamiltonian cycle algorithm ini-
tially has unused-edge lists where each edge (v, u ) with u =v is placed on v’s list
independently with probability q ≥20 ln n / n. Then the algorithm successfully finds a
Hamiltonian cycle in O ( n ln n ) iterations of the repeat loop (step 2) with probability
1 − O ( n −^1 ).

Note that we did not assume that the input random graph has a Hamiltonian cycle. A
corollary of the theorem is that, with high probability, a random graph chosen in this
way has a Hamiltonian cycle.

Proof of Theorem5.17: Consider the following two events.

E 1 : The algorithm ran for 3 n ln n steps with no unused-edges list becoming empty, but
it failed to construct a Hamiltonian cycle.
E 2 : At least one unused-edges list became empty during the first 3 n ln n iterations of
the loop.

For the algorithm to fail, either eventE 1 orE 2 must occur. We first bound the proba-
bility ofE 1. Lemma5.16implies that, as long as there is no empty unused-edges list in
the first 3 n ln n iterations of step 2 of Algorithm5.2, in each iteration the next head of
the path is uniform among the n vertices of the graph. To boundE 1 , we therefore con-
sider the probability that more than 3 n ln n iterations are required to find a Hamiltonian
cycle when the head is chosen uniformly at random each iteration.
The probability that the algorithm takes more than 2 n ln n iterations to find a Hamil-
tonian path is exactly the probability that a coupon collector’s problem on n types
requires more than 2 n ln n coupons. The probability that any specific coupon type has
not been found among 2 n ln n random coupons is
(
1 −

1
n
) 2 n ln n
≤e−2ln n =
1
n^2
.
balls, bins, and random graphs
By the union bound, the probability that any coupon type is not found is at most
1 / n.
In order to complete a Hamiltonian path to a cycle the path must close, which it does
at each step with probability 1/ n. Hence the probability that the path does not become
a cycle within the next n ln n iterations is
(
1 −

1
n
) n ln n
≤e−ln n =
1
n
.
Thus we have shown that

Pr(E 1 )≤
2
n
.
Next we bound Pr(E 2 ), the probability that an unused-edges list is empty in the first
3 n ln n iterations. We consider two subevents as follows.

E 2 a : At least 9 ln n edges were removed from the unused-edges list of at least one vertex
in the first 3 n ln n iterations of the loop.
E 2 b : At least one vertex had fewer than 10 ln n edges initially in its unused-edges list.

ForE 2 to occur, eitherE 2 a orE 2 b must occur. Hence

Pr(E 2 )≤Pr(E 2 a )+Pr(E 2 b ).
Let us first bound Pr(E 2 a ). Exactly one edge is used in each iteration of the loop.
From the proof of Lemma5.16we have that, at each iteration, the probability that a
given vertex v is the head of the path is 1/ n ,independently at each step. Hence the
number of times X that v is the head during the first 3 n ln n steps is a binomial random
variable B (3 n ln n , 1 / n ), and this dominates the number of edges taken from v ’s unused-
edges list.
Using the Chernoff bound of Eqn. (4.1) withδ=2 andμ=3ln n for the binomial
random variable B (3 n ln n , 1 / n ), we have

Pr( X ≥9ln n )≤
(
e^2
27
)3ln n
≤
1
n^2
.
By taking a union bound over all vertices, we find Pr(E 2 a )≤ 1 / n.
Next we bound Pr(E 2 b ). The expected number of edges Y initially in a vertex’s
unused-edges list is at least ( n −1) q ≥(20( n −1)ln n )/ n ≥19 ln n for sufficiently
large n. Using Chernoff bounds again (Eqn. (4.5)), the probability that any vertex ini-
tially has 10 ln n edges or fewer on its list is at most

Pr( Y ≤10 ln n )≤e−(19 ln n )(9/19)
(^2) / 2
≤

1
n^2
,
and by the union bound the probability that any vertex has too few adjacent edges is at
most 1/ n. Thus,

Pr(E 2 b )≤
1
n
5.7 Exercises
and hence

Pr(E 2 )≤
2
n
.
In total, the probability that the algorithm fails to find a Hamiltonian cycle in 3 n ln n
iterations is bounded by

Pr(E 1 )+Pr(E 2 )≤
4
n
.  /theta
We did not make an effort to optimize the constants in the proof. There is, how-
ever, a clear trade-off; with more edges, one could achieve a lower probability of
failure.
We are left with showing how our algorithm can be applied to graphs in Gn , p .We
show that, as long as p is known, we can partition the edges of the graph into edge lists
that satisfy the requirements of Theorem5.17.

Corollary 5.18: By initializing edges on the unused-edges lists appropriately, Algo-
rithm5.2will find a Hamiltonian cycle on a graph chosen randomly from Gn , pwith
probability 1 − O (1/ n ) whenever p ≥(40 ln n )/ n.

Proof: We partition the edges of our input graph from Gn , p as follows. Let q ∈[0,1] be
such that p = 2 q − q^2. Consider any edge ( u ,v) in the input graph. We execute exactly
one of the following three possibilities: with probability q (1− q )/(2 q − q^2 ) we place
the edge on u ’s unused-edges list but not on v ’s; with probability q (1− q )/(2 q − q^2 )we
initially place the edge on v ’s unused-edges list but not on u ’s; and with the remaining
probability q^2 /(2 q − q^2 ) the edge is placed on both unused-edges lists.
Now, for any possible edge ( u ,v), the probability that it is initially placed in the
unused-edges list for v is

p
(
q (1− q )
2 q − q^2
+
q^2
2 q − q^2
)
= q.
Moreover, the probability that an edge ( u ,v) is initially placed on the unused-edges
list for both u and v is pq^2 /(2 q − q^2 )= q^2 , so these two placements are indepen-
dent events. Since each edge ( u ,v) is treated independently, this partitioning fulfills
the requirements of Theorem5.17provided the resulting q is at least 20 ln n / n. When
p ≥(40 ln n )/ n we have q ≥ p / 2 ≥(20 ln n )/ n , and the result follows.  /theta

In Exercise5.27, we consider how to use Algorithm5.2even in the case where p is not
known in advance, so that the edge lists must be initialized without knowledge of p.

5.7. Exercises
Exercise 5.1: For what values of n is (1+ 1 / n ) n within 1% of e? Within 0.0001%
of e? Similarly, for what values of n is (1− 1 / n ) n within 1% of 1/e? Within 0.0001%?

balls, bins, and random graphs
Exercise 5.2: Suppose that Social Security numbers were issued uniformly at random,
with replacement. That is, your Social Security number would consist of just nine ran-
domly generated digits, and no check would be made to ensure that the same number
was not issued twice. Sometimes, the last four digits of a Social Security number are
used as a password. How many people would you need to have in a room before it was
more likely than not that two had the same last four digits? How many numbers could
be issued before it would be more likely than not that there is a duplicate number? How
would you answer these two questions if Social Security numbers had 13 digits? Try
to give exact numerical answers.

Exercise 5.3: Suppose that balls are thrown randomly into n bins. Show, for some
constant c 1 , that if there are c 1

√
n balls then the probability that no two land in the
same bin is at most 1/e. Similarly, show for some constant c 2 (and sufficiently large n )
that, if there are c 2

√
n balls, then the probability that no two land in the same bin is at
least 1/2. Make these constants as close to optimal as possible. Hint: You may want to
use the facts that

e− x ≥ 1 − x
and

e− x − x
2
≤ 1 − x for x ≤
1
2
.
Exercise 5.4: In a lecture hall containing 100 people, you consider whether or not
there are three people in the room who share the same birthday. Explain how to calculate
this probability exactly, using the same assumptions as in our previous analysis.

Exercise 5.5: Use the moment generating function of the Poisson distribution to com-
pute the second moment and the variance of the distribution.

Exercise 5.6: Let X be a Poisson random variable with meanμ, representing the num-
ber of errors on a page of this book. Each error is independently a grammatical error
with probability p and a spelling error with probability 1− p .If Y and Z are random
variables representing the number of grammatical and spelling errors (respectively) on
a page of this book, prove that Y and Z are Poisson random variables with meansμ p
andμ(1− p ), respectively. Also, prove that Y and Z are independent.

Exercise 5.7: Use the Taylor expansion

ln(1+ x )= x −
x^2
2
+
x^3
3
−
x^4
4
+···
to prove that, for any x with| x |≤1,

e x (1− x^2 )≤ 1 + x ≤e x.
Exercise 5.8: Suppose that n balls are thrown independently and uniformly at random
into n bins.

5.7 exercises
(a) Find the conditional probability that bin 1 has one ball given that exactly one ball
fell into the first three bins.
(b) Find the conditional expectation of the number of balls in bin 1 under the condition
that bin 2 received no balls.
(c) Write an expression for the probability that bin 1 receives more balls than bin 2.

Exercise 5.9: Our analysis of Bucket sort in Section5.2.2assumed that n elements
were chosen independently and uniformly at random from the range [0, 2 k ). Suppose
instead that n elements are chosen independently from the range [0, 2 k ) according to
a distribution with the property that any number x ∈[0, 2 k ) is chosen with probability
at most a / 2 k for some fixed constant a >0. Show that, under these conditions, Bucket
sort still requires linear expected time.

Exercise 5.10: Consider the probability that every bin receives exactly one ball when
n balls are thrown randomly into n bins.

(a) Give an upper bound on this probability using the Poisson approximation.
(b) Determine the exact probability of this event.
(c) Show that these two probabilities differ by a multiplicative factor that equals the
probability that a Poisson random variable with parameter n takes on the value n.
Explain why this is implied by Theorem5.6.

Exercise 5.11: Consider throwing m balls into n bins, and for convenience let the
bins be numbered from 0 to n −1. We say there is a k-gap starting at bin i if bins
i , i + 1 ,..., i + k −1 are all empty.

(a) Determine the expected number of k -gaps.
(b) Prove a Chernoff-like bound for the number of k -gaps. ( Hint: If you let Xi = 1
when there is a k -gap starting at bin i , then there are dependencies between Xi and
Xi + 1 ; to avoid these dependencies, you might consider Xi and Xi + k .)

Exercise 5.12: The following problem models a simple distributed system wherein
agents contend for resources but “back off” in the face of contention. Balls represent
agents, and bins represent resources.
The system evolves over rounds. Every round, balls are thrown independently and
uniformly at random into n bins. Any ball that lands in a bin by itself is served and
removed from consideration. The remaining balls are thrown again in the next round.
We begin with n balls in the first round, and we finish when every ball is served.

(a) If there are b balls at the start of a round, what is the expected number of balls at
the start of the next round?
(b) Suppose that every round the number of balls served was exactly the expected
number of balls to be served. Show that all the balls would be served in O (log log n )
rounds. ( Hint: If xj is the expected number of balls left after j rounds, show and
use that xj + 1 ≤ x^2 j / n .)

balls, bins, and random graphs
Exercise 5.13: Suppose that we vary the balls-and-bins process as follows. For conve-
nience let the bins be numbered from 0 to n −1. There are log 2 n players. Each player
randomly chooses a starting location nuniformly from [0, n −1] and then places one
ball in each of the bins numbered nmod n , n+1mod n ,..., n+ n /log 2 n −1mod n.
Argue that the maximum load in this case is only O (log log n /log log log n ) with prob-
ability that approaches 1 as n →∞.

Exercise 5.14: We prove that if Z is a Poisson random variable of meanμ, where
μ≥1 is an integer, then Pr( Z ≥μ)≥ 1 /2.

(a) Show that Pr( Z =μ+ h )≥Pr( Z =μ− h −1) for 0≤ h ≤μ−1.
(b) Using part (a), argue that Pr( Z ≥μ)≥ 1 /2.
(c) Show that Pr( Z ≤μ)≤ 1 /2 for all integersμfrom 1 to 10 by explicitly performing
the calculation. (This is in fact true for all integersμ≥1, but it is more difficult to
prove.)

Exercise 5.15: (a) In Theorem5.7we showed that, for any nonnegative functions f ,

E
[
f
(
Y 1 ( m ),..., Yn ( m )
)]
≥ E
[
f
(
X 1 ( m ),..., Xn ( m )
)]
Pr
(∑
Yi ( m )= m
)
.
Prove that if E [ f ( X 1 ( m ),..., Xn ( m ))] is monotonically increasing in m , then

E
[
f
(
Y 1 ( m ),..., Yn ( m )
)]
≥ E
[
f
(
X 1 ( m ),..., Xn ( m )
)]
Pr
(∑
Yi ( m )≥ m
)
,
again under the condition that f is nonnegative. Make a similar statement for the case
when E [ f ( X 1 ( m ),..., Xn ( m ))] is monotonically decreasing in m.
(b) Using part (a) and Exercise5.14, prove Theorem5.10 for the case that
E [ f ( X 1 ( m ),..., Xn ( m ))] is monotonically increasing.

Exercise 5.16: We consider another way to obtain Chernoff-like bounds in the setting
of balls and bins without using Theorem5.7. Consider n balls thrown randomly into
n bins. Let Xi =1 if the i th bin is empty and 0 otherwise. Let X =

∑ n
i = 1 Xi. Let Yi ,
i = 1 ,..., n , be independent Bernoulli random variables that are 1 with probability
p =(1− 1 / n ) n. Let Y =

∑ n
i = 1 Yi.
(a) Show that E [ X 1 X 2 ··· Xk ]≤ E [ Y 1 Y 2 ··· Yk ] for any k ≥1.
(b) Show that E [e tX ]≤ E [e tY ] for all t ≥0. ( Hint: Use the expansion for e x and com-
pare E [ Xk ]to E [ Yk ].)
(c) Derive a Chernoff bound for Pr( X ≥(1+δ) E [ X ]).

Exercise 5.17: Let G be a random graph generated using the Gn , p model.

(a) A clique of k vertices in a graph is a subset of k vertices such that all
( k
2
)
edges
between these vertices lie in the graph. For what value of p , as a function of n ,is
the expected number of cliques of five vertices in G equal to 1?
(b) A K 3 , 3 graph is a complete bipartite graph with three vertices on each side. In other
words, it is a graph with six vertices and nine edges; the six distinct vertices are
arranged in two groups of three, and the nine edges connect each of the nine pairs

5.7 exercises
of vertices with one vertex in each group. For what value of p , as a function of n ,
is the expected number of K 3 , 3 subgraphs of G equal to 1?
(c) For what value of p , as a function of n , is the expected number of Hamiltonian
cycles in the graph equal to 1?
Exercise 5.18: Theorem5.7shows that any event that occurs with small probability
in the balls-and-bins setting where the number of balls in each bin is an independent
Poisson random variable also occurs with small probability in the standard balls-and-
bins model. Prove a similar statement for random graphs: Every event that happens
with small probability in the Gn , p model also happens with small probability in the
Gn , N model for N =

( n
2
)
p.
Exercise 5.19: An undirected graph on n vertices is disconnected if there exists a set
of k < n vertices such that there is no edge between this set and the rest of the graph.
Otherwise, the graph is said to be connected. Show that there exists a constant c such
that if N ≥ cn log n then, with probability 1− o (1), a graph randomly chosen from Gn , N
is connected.

Exercise 5.20: Prove Theorem5.15.

Exercise 5.21: (a) Let f ( n ) be the expected number of random edges that must be
added before an empty undirected graph with n vertices becomes connected. (Con-
nectedness is defined in Exercise5.19.) That is, suppose that we start with a graph on
n vertices with zero edges and then repeatedly add an edge, chosen uniformly at ran-
dom from all edges not currently in the graph, until the graph becomes connected. If
Xn represents the number of edges added, then f ( n )= E [ Xn ].
Write a program to estimate f ( n ) for a given value of n. Your program should track
the connected components of the graph as you add edges until the graph becomes con-
nected. You will probably want to use a disjoint set data structure, a topic covered in
standard undergraduate algorithms texts. You should try n =100, 200, 300, 400, 500,
600, 700, 800, 900, and 1000. Repeat each experiment 100 times, and for each value of
n compute the average number of edges needed. Based on your experiments, suggest a
function h ( n ) that you think is a good estimate for f ( n ).
(b) Modify your program for the problem in part (a) so that it also keeps track of
isolated vertices. Let g ( n ) be the expected number of edges added before there are no
more isolated vertices. What seems to be the relationship between f ( n ) and g ( n )?

Exercise 5.22: In hashing with open addressing, the hash table is implemented as an
array and there are no linked lists or chaining. Each entry in the array either contains
one hashed item or is empty. The hash function defines, for each key k ,a probesequence
h ( k ,0), h ( k ,1),...of table locations. To insert the key k , we first examine the sequence
of table locations in the order defined by the key’s probe sequence until we find an
empty location; then we insert the item at that position. When searching for an item in
the hash table, we examine the sequence of table locations in the order defined by the
key’s probe sequence until either the item is found or we have found an empty location

balls, bins, and random graphs
in the sequence. If an empty location is found, this means the item is not present in the
table.
An open-address hash table with 2 n entries is used to store n items. Assume that the
table location h ( k , j ) is uniform over the 2 n possible table locations and that all h ( k , j )
are independent.

(a) Show that, under these conditions, the probability of an insertion requiring more
than k probes is at most 2− k.
(b) Show that, for i = 1 , 2 ,..., n , the probability that the i th insertion requires more
than 2 log n probes is at most 1/ n^2.

Let the random variable Xi denote the number of probes required by the i th insertion.
You have shown in part (b) that Pr( Xi >2 log n )≤ 1 / n^2. Let the random variable X =
max 1 ≤ i ≤ nXi denote the maximum number of probes required by any of the n insertions.

(c) Show that Pr( X >2 log n )≤ 1 / n.
(d) Show that the expected length of the longest probe sequence is E [ X ]= O (log n ).

Exercise 5.23: Bloom filters can be used to estimate set differences. Suppose you have
a set X and I have a set Y , both with n elements. For example, the sets might represent
our 100 favorite songs. We both create Bloom filters of our sets, using the same number
of bits m and the same k hash functions. Determine the expected number of bits where
our Bloom filters differ as a function of m , n , k , and| X ∩ Y |. Explain how this could be
used as a tool to find people with the same taste in music more easily than comparing
lists of songs directly.

Exercise 5.24: Suppose that we wanted to extend Bloom filters to allow deletions as
well as insertions of items into the underlying set. We could modify the Bloom filter
to be an array of counters instead of an array of bits. Each time an item is inserted
into a Bloom filter, the counters given by the hashes of the item are increased by one.
To delete an item, one can simply decrement the counters. To keep space small, the
counters should be a fixed length, such as 4 bits.
Explain how errors can arise when using fixed-length counters. Assuming a setting
where one has at most n elements in the set at any time, m counters, k hash functions,
and counters with b bits, explain how to bound the probability that an error occurs over
the course of t insertions or deletions.

Exercise 5.25: Suppose that you built a Bloom filter for a dictionary of words with
m = 2 b bits. A co-worker building an application wants to use your Bloom filter but
has only 2 b −^1 bits available. Explain how your colleague can use your Bloom filter to
avoid rebuilding a new Bloom filter using the original dictionary of words.

Exercise 5.26: For the leader election problem alluded to in Section5.5.4,wehave n
users, each with an identifier. The hash function takes as input the identifier and outputs
a b -bit hash value, and we assume that these values are independent and uniformly
distributed. Each user hashes its identifier, and the leader is the user with the smallest

5.8 An Exploratory Assignment
hash value. Give lower and upper bounds on the number of bits b necessary to ensure
that a unique leader is successfully chosen with probability p. Make your bounds as
tight as possible.
Exercise 5.27: Consider Algorithm5.2, the modified algorithm for finding Hamilto-
nian cycles. We have shown that the algorithm can be applied to find a Hamiltonian
cycle with high probability in a graph chosen randomly from Gn , p , when p is known
and sufficiently large, by initially placing edges in the edge lists appropriately. Argue
that the algorithm can similarly be applied to find a Hamiltonian cycle with high prob-
ability on a graph chosen randomly from Gn , N when N = c 1 n ln n for a suitably large
constant c 1. Argue also that the modified algorithm can be applied even when p is not
known in advance as long as p is at least c 2 ln n / n for a suitably large constant c 2.
5.8. An Exploratory Assignment
Part of the research process in random processes is first to understand what is going on
at a high level and then to use this understanding in order to develop formal mathemat-
ical proofs. In this assignment, you will be given several variations on a basic random
process. To gain insight, you should perform experiments based on writing code to
simulate the processes. (The code should be very short, a few pages at most.) After
the experiments, you should use the results of the simulations to guide you to make
conjectures and prove statements about the processes. You can apply what you have
learned up to this point, including probabilistic bounds and analysis of balls-and-bins
problems.
Consider a complete binary tree with N = 2 n −1 nodes. Here n is the depth of the
tree. Initially, all nodes are unmarked. Over time, via processes that we shall describe,
nodes becomes marked.
All of the processes share the same basic form. We can think of the nodes as having
unique identifying numbers in the range of [1, N ]. Each unit of time, I send you the
identifier of a node. When you receive a sent node, you mark it. Also, you invoke the
following marking rule, which takes effect before I send out the next node.
 /thetaIf a node and its sibling are marked, its parent is marked.
 /thetaIf a node and its parent are marked, the other sibling is marked.
The marking rule is applied recursively as much as possible before the next node is
sent. For example, in Figure5.3, the marked nodes are filled in. The arrival of the node
labeled by an X will allow you to mark the remainder of the nodes, as you apply the
marking rule first up and then down the tree. Keep in mind that you always apply the
marking rule as much as possible.
Now let us consider the different ways in which I might be sending you the nodes.
Process 1: Each unit of time, I send the identifier of a node chosen independently and
uniformly at random from all of the N nodes. Note that I might send you a node that
is already marked, and in fact I may send a useless node that I have already sent.

balls, bins, and random graphs
X
Figure 5.3: The arrival of X causes all other nodes to be marked.
Process 2: Each unit of time I send the identifier of a node chosen uniformly at random
from those nodes that Ihave not yet sent. Again, a node that has already been marked
might arrive, but each node will be sent at most once.
Process 3: Each unit of time I send the identifier of a node chosen uniformly at random
from those nodes that you have not yet marked.
We want to determine how many time steps are needed before all the nodes are
marked for each of these processes. Begin by writing programs to simulate the sending
processes and the marking rule. Run each process ten times for each value of n in the
range [10,20]. Present the data from your experiments in a clear, easy-to-read fashion
and explain your data suitably. A tip: You may find it useful to have your program print
out the last node that was sent before the tree became completely marked.

1. For the first process, prove that the expected number of nodes sent is m( N log N ).
How well does this match your simulations?
2. For the second process, you should find that almost all N nodes must be sent before
the tree is marked. Show that, with constant probability, at least N − 2

√
N nodes
must be sent.
3. The behavior of the third process might seem a bit unusual. Explain it with a proof.

After answering these questions, you may wish to consider other facts you could prove
about these processes.

chapter six

The Probabilistic Method

The probabilistic method is a way of proving the existence of objects. The under-
lying principle is simple: to prove the existence of an object with certain properties,
we demonstrate a sample space of objects in which the probability is positive that a
randomly selected object has the required properties. If the probability of selecting an
object with the required properties is positive, then the sample space must contain such
an object, and therefore such an object exists. For example, if there is a positive proba-
bility of winning a million-dollar prize in a raffle, then there must be at least one raffle
ticket that wins that prize.
Although the basic principle of the probabilistic method is simple, its application to
specific problems often involves sophisticated combinatorial arguments. In this chapter
we study a number of techniques for constructing proofs based on the probabilistic
method, starting with simple counting and averaging arguments and then introducing
two more advanced tools, the Lovász local lemma and the second moment method.
In the context of algorithms we are generally interested in explicit constructions
of objects, not merely in proofs of existence. In many cases the proofs of existence
obtained by the probabilistic method can be converted into efficient randomized con-
struction algorithms. In some cases, these proofs can be converted into efficient deter-
ministic construction algorithms; this process is called derandomization , since it con-
verts a probabilistic argument into a deterministic one. We give examples of both
randomized and deterministic construction algorithms arising from the probabilistic
method.

6.1 The Basic Counting Argument
To prove the existence of an object with specific properties, we construct an appropriate
probability spaceSof objects and then show that the probability that an object inS
with the required properties is selected is strictly greater than 0.
For our first example, we consider the problem of coloring the edges of a graph
with two colors so that there are no large cliques with all edges having the same color.

the probabilistic method
Let Kn be a complete graph (with all

( n
2
)
edges) on n vertices. A clique of k vertices in
Kn is a complete subgraph Kk.

Theorem 6.1: If

( n
k
)
2 −
( k
2
)
(^1) < 1 then it is possible to color the edges of K
nwith two
colors so that it has no monochromatic Kksubgraph.
Proof: Define a sample space consisting of all possible colorings of the edges of Kn
using two colors. There are 2
( n
2
)
possible colorings, so if one is chosen uniformly at
random then the probability of choosing each coloring in our probability space is 2−

( n
2
)
.
A nice way to think about this probability space is: if we color each edge of the graph
independently, with each edge taking each of the two colors with probability 1/2, then
we obtain a random coloring chosen uniformly from this sample space. That is, we flip
an independent fair coin to determine the color of each edge.
Fix an arbitrary ordering of all of the

( n
k
)
different k -vertex cliques of Kn , and for
i = 1 ,...,

( n
k
)
let Ai be the event that clique i is monochromatic. Once the first edge
in clique i is colored, the remaining

( k
2
)
−1 edges must all be given the same color. It
follows that

Pr( Ai )= 2 −
( k
2
)
(^1).
Using a union bound then yields
Pr
⎛
⎜⎝
( n
k
)
⋃
i = 1
Ai
⎞
⎟⎠≤
( n
k
)
∑
i = 1
Pr( Ai )=
(
n
k
)
2 −
( k
2
)
(^1) < 1 ,
where the last inequality follows from the assumptions of the theorem. Hence
Pr
⎛
⎜
⎝
( n
k
)
⋂
i = 1
Ai
⎞
⎟
⎠=^1 −Pr
⎛
⎜
⎝
( n
k
)
⋃
i = 1
Ai
⎞
⎟
⎠>^0.
Since the probability of choosing a coloring with no monochromatic k -vertex clique
from our sample space is strictly greater than 0, there must exist a coloring with no
monochromatic k -vertex clique.  /theta

As an example, consider whether the edges of K 1000 can be 2-colored in such a way
that there is no monochromatic K 20. Our calculations are simplified if we note that, for
n ≤ 2 k /^2 and k ≥3,
(
n
k

)
2 −
( k
2
)
(^1) ≤ nk
k!
2 −( k ( k −1)/2)+^1
≤
2 k /^2 +^1
k!
< 1.
Observing that for our example n = 1000 ≤ 210 = 2 k /^2 , we see that by Theorem6.1
there exists a 2-coloring of the edges of K 1000 with no monochromatic K 20.

6.2 the expectation argument
Can we use this proof to design an efficient algorithm to construct such a coloring?
Let us consider a general approach that gives a randomized construction algorithm.
First, we require that we can efficiently sample a coloring from the sample space. In
this case sampling is easy, because we can simply color each edge independently with
a randomly chosen color. In general, however, there might not be an efficient sampling
algorithm.
If we have an efficient sampling algorithm, the next question is: How many sam-
ples must we generate before obtaining a sample that satisfies our requirements? If
the probability of obtaining a sample with the desired properties is p and if we sam-
ple independently at each trial, then the number of samples needed before finding a
sample with the required properties is a geometric random variable with expectation
1 / p. Hence we need that 1/ p be polynomial in the problem size in order to have an
algorithm that finds a suitable sample in polynomial expected time.
If p = 1 − o (1), then sampling once gives a Monte Carlo construction algorithm
that is incorrect with probability o (1). In our specific example of finding a coloring on
a graph of 1000 vertices with no monochromatic K 20 , we know that the probability that
a random coloring has a monochromatic K 20 is at most
220 /^2 +^1
20!
< 8. 5 · 10 −^16.
Hence we have a Monte Carlo algorithm with a small probability of failure.
If we want a Las Vegas algorithm – that is, one that always gives a correct construc-
tion – then we need a third ingredient. We require a polynomial time procedure for
verifying that a sample object satisfies the requirements; then we can test samples until
we find one that does so. An upper bound on the expected time for this construction
can be found by multiplying together the expected number of samples 1/ p by the sum
of an upper bound on the time to generate each sample and an upper bound on the time
to check each sample.^1 For the coloring problem, there is a polynomial time verifica-
tion algorithm when k is a constant: simply check all
( n
k
)
cliques and make sure they
are not monochromatic. It does not seem that this approach can be extended to yield
polynomial time algorithms when k grows with n.
6.2. The Expectation Argument
As we have seen, in order to prove that an object with certain properties exists, we
can design a probability space from which an element chosen at random yields an
object with the desired properties with positive probability. A similar and sometimes
easier approach for proving that such an object exists is to use an averaging argument.
The intuition behind this approach is that, in a discrete probability space, a random
variable must with positive probability assume at least one value that is no greater
than its expectation and at least one value that is not smaller than its expectation.
(^1) Sometimes the time to generate or check a sample may itself be a random variable. In this case, Wald’s equation
(discussed in Chapter 13 ) may apply.

the probabilistic method
For example, if the expected value of a raffle ticket is $3, then there must be at least
one ticket that ends up being worth no more than $3 and at least one that ends up being
worth no less than $3.
More formally, we have the following lemma.

Lemma 6.2: Suppose we have a probability space S and a random variable X defined
on S such that E [ X ]=μ. Then Pr( X ≥μ)> 0 and Pr( X ≤μ)> 0_._

Proof: We have

μ= E [ X ]=
∑
x
x Pr( X = x ),
where the summation ranges over all values in the range of X .IfPr( X ≥μ)=0, then

μ=
∑
x
x Pr( X = x )=
∑
x <μ
x Pr( X = x )<
∑
x <μ
μPr( X = x )=μ,
giving a contradiction. Similarly, if Pr( X ≤μ)=0 then

μ=
∑
x
x Pr( X = x )=
∑
x >μ
x Pr( X = x )>
∑
x >μ
μPr( X = x )=μ,
again yielding a contradiction.  /theta

Thus, there must be at least one instance in the sample space ofSfor which the value
of X is at leastμand at least one instance for which the value of X is no greater thanμ.

6.2.1 Application: Finding a Large Cut
We consider the problem of finding a large cut in an undirected graph. A cut is a parti-
tion of the vertices into two disjoint sets, and the value of a cut is the weight of all edges
crossing from one side of the partition to the other. Here we consider the case where
all edges in the graph have the same weight 1. The problem of finding a maximum cut
is NP-hard. Using the probabilistic method, we show that the value of the maximum
cut must be at least 1/2 the number of edges in the graph.

Theorem 6.3: Given an undirected graph G with m edges, there is a partition of V
into two disjoint sets A and B such that at least m / 2 edges connect a vertex in A to a
vertex in B. That is, there is a cut with value at least m / 2_._

Proof: Construct sets A and B by randomly and independently assigning each vertex
to one of the two sets. Let e 1 ,..., em be an arbitrary enumeration of the edges of G.
For i = 1 ,..., m , define Xi such that

Xi =
{
1 if edge i connects A to B ,
0 otherwise.
The probability that edge ei connects a vertex in A to a vertex in B is 1/2, and thus

E [ Xi ]=
1
2
.
6.2 the expectation argument
Let C ( A , B ) be a random variable denoting the value of the cut corresponding to the
sets A and B. Then

E [ C ( A , B )]= E
[ m
∑
i = 1
Xi
]
=
∑ m
i = 1
E [ Xi ]= m ·
1
2
=
m
2
Since the expectation of the random variable C ( A , B )is m /2, there exists a partition A
and B with at least m /2 edges connecting the set A to the set B.  /theta

We can transform this argument into an efficient algorithm for finding a cut with value
at least m /2. We first show how to obtain a Las Vegas algorithm. In Section6.3,we
show how to construct a deterministic polynomial time algorithm.
It is easy to randomly choose a partition as described in the proof. The expectation
argument does not give a lower bound on the probability that a random partition has a
cut of value at least m /2. To derive such a bound, let

p =Pr
(
C ( A , B )≥
m
2
)
,
and observe that C ( A , B )≤ m. Then

m
2
= E [ C ( A , B )]
=
∑
i < m / 2
i Pr( C ( A , B )= i )+
∑
i ≥ m / 2
i Pr( C ( A , B )= i )
≤(1− p )
(
m
2
− 1
)
+ pm ,
which implies that

p ≥
1
m / 2 + 1
The expected number of samples before finding a cut with value at least m /2 is therefore
just m / 2 +1. Testing to see if the value of the cut determined by the sample is at least
m /2 can be done in polynomial time simply by counting the edges crossing the cut. We
therefore have a Las Vegas algorithm for finding the cut.

6.2.2 Application: Maximum Satisfiability
We can apply a similar argument to the maximum satisfiability (MAXSAT) problem.
In a logical formula, a literal is either a Boolean variable or the negation of a Boolean
variable. We use x to denote the negation of the variable x. A satisfiability (SAT) prob-
lem, or a SAT formula, is a logical expression that is the conjunction (AND) of a set
of clauses, where each clause is the disjunction (OR) of literals. For example, the fol-
lowing expression is an instance of SAT:

( x 1 ∨ x 2 ∨ x 3 )∧( x 1 ∨ x 3 )∧( x 1 ∨ x 2 ∨ x 4 )∧( x 4 ∨ x 3 )∧( x 4 ∨ x 1 ).
the probabilistic method
A solution to an instance of a SAT formula is an assignment of the variables to the
values True and False so that all the clauses are satisfied. That is, there is at least one
true literal in each clause. For example, assigning x 1 to True, x 2 to False, x 3 to False,
and x 4 to True satisfies the preceding SAT formula. In general, determining if a SAT
formula has a solution is NP-hard.
A related goal, given a SAT formula, is satisfying as many of the clauses as pos-
sible. In what follows, let us assume that no clause contains both a variable and its
complement, since in this case the clause is always satisfied.

Theorem 6.4: Given a set of m clauses, let kibe the number of literals in the i th clause
for i = 1 ,..., m. Let k =min mi = 1 ki. Then there is a truth assignment that satisfies at
least

∑ m
i = 1
(1− 2 − ki )≥ m (1− 2 − k )
clauses.

Proof: Assign values independently and uniformly at random to the variables. The
probability that the i th clause with ki literals is satisfied is at least (1− 2 − ki ). The
expected number of satisfied clauses is therefore at least

∑ m
i = 1
(1− 2 − ki )≥ m (1− 2 − k ),
and there must be an assignment that satisfies at least that many clauses.  /theta

The foregoing argument can also be easily transformed into an efficient randomized
algorithm; the case where all ki = k is left as Exercise6.1.

6.3 Derandomization Using Conditional Expectations
The probabilistic method can yield insight into how to construct deterministic algo-
rithms. As an example, we apply the method of conditional expectations in order to
derandomize the algorithm of Section6.2.1for finding a large cut.
Recall that we find a partition of the n vertices V of a graph into sets A and B by
placing each vertex independently and uniformly at random in one of the two sets. This
gives a cut with expected value E [ C ( A , B )]≥ m /2. Now imagine placing the vertices
deterministically, one at a time, in an arbitrary orderv 1 ,v 2 ,...,v n. Let xi be the set
wherev i is placed (so xi is either A or B ). Suppose that we have placed the first k
vertices, and consider the expected value of the cut if the remaining vertices are then
placed independently and uniformly into one of the two sets. We write this quantity
as E [ C ( A , B )| x 1 , x 2 ,..., xk ]; it is the conditional expectation of the value of the cut
given the locations x 1 , x 2 ,..., xk of the first k vertices. We show inductively how to
place the next vertex so that

E [ C ( A , B )| x 1 , x 2 ,..., xk ]≤ E [ C ( A , B )| x 1 , x 2 ,..., xk + 1 ].
6.3derandomization using conditional expectations
It follows that

E [ C ( A , B )]≤ E [ C ( A , B )| x 1 , x 2 ,..., xn ].
The right-hand side is the value of the cut determined by our placement algorithm,
since if x 1 , x 2 ,..., xn are all determined then we have a cut of the graph. Hence our
algorithm returns a cut whose value is at least E [ C ( A , B )]≥ m /2.
The base case in the induction is

E [ C ( A , B )| x 1 ]= E [ C ( A , B )],
which holds by symmetry because it does not matter where we place the first vertex.
We now prove the inductive step, that

E [ C ( A , B )| x 1 , x 2 ,..., xk ]≤ E [ C ( A , B )| x 1 , x 2 ,..., xk + 1 ]. (6.1)
Consider placingv k + 1 randomly, so that it is placed in A or B with probability 1/2 each,
and let Yk + 1 be a random variable representing the set where it is placed. Then

E [ C ( A , B )| x 1 , x 2 ,..., xk ]=
1
2
E [ C ( A , B )| x 1 , x 2 ,..., xk , Yk + 1 = A ]
+
1
2
E [ C ( A , B )| x 1 , x 2 ,..., xk , Yk + 1 = B ].
It follows that

max
(
E [ C ( A , B )| x 1 , x 2 ,..., xk , Yk + 1 = A ], E [ C ( A , B )| x 1 , x 2 ,..., xk , Yk + 1 = B ]
)
≥ E [ C ( A , B )| x 1 , x 2 ,..., xk ].
Therefore, all we have to do is compute the two quantities E [ C ( A , B )| x 1 , x 2 ,...,
xk , Yk + 1 = A ] and E [ C ( A , B )| x 1 , x 2 ,..., xk , Yk + 1 = B ] and then place thev k + 1 in the
set that yields the larger expectation. Once we do this, we will have a placement satis-
fying

E [ C ( A , B )| x 1 , x 2 ,..., xk ]≤ E [ C ( A , B )| x 1 , x 2 ,..., xk + 1 ].
To compute E [ C ( A , B )| x 1 , x 2 ,..., xk , Yk + 1 = A ], note that the conditioning gives
the placement of the first k +1 vertices. We can therefore compute the number of edges
among these vertices that contribute to the value of the cut. For all other edges, the
probability that it will later contribute to the cut is 1/2, since this is the probability
its two endpoints end up on different sides of the cut. By linearity of expectations,
E [ C ( A , B )| x 1 , x 2 ,..., xk , Yk + 1 = A ] is the number of edges crossing the cut whose
endpoints are both among the first k +1 vertices, plus half of the remaining edges. This
is easy to compute in linear time. The same is true for E [ C ( A , B )| x 1 , x 2 ,..., xk , Yk + 1 =
B ].
In fact, from this argument, we see that the larger of the two quantities is determined
just by whetherv k + 1 has more neighbors in A or in B. All edges that do not havev k + 1
as an endpoint contribute the same amount to the two expectations. Our derandomized
algorithm therefore has the following simple form: Take the vertices in some order.
Place the first vertex arbitrarily in A. Place each successive vertex to maximize the
number of edges crossing the cut. Equivalently, place each vertex on the side with

the probabilistic method
fewer neighbors, breaking ties arbitrarily. This is a simple greedy algorithm, and our
analysis shows that it always guarantees a cut with at least m /2 edges.

6.4 Sample and Modify
Thus far we have used the probabilistic method to construct random structures with the
desired properties directly. In some cases it is easier to work indirectly, breaking the
argument into two stages. In the first stage we construct a random structure that does not
have the required properties. In the second stage we then modify the random structure
so that it does have the required property. We give two examples of this sample-and-
modify technique.

6.4.1 Application: Independent Sets
An independent set in a graph G is a set of vertices with no edges between them. Find-
ing the largest independent set in a graph is an NP-hard problem. The following the-
orem shows that the probabilistic method can yield bounds on the size of the largest
independent set of a graph.

Theorem 6.5: LetG =( V , E ) beaconnectedgraphonnverticeswithm ≥ n / 2 edges.
Then G has an independent set with at least n^2 / 4 m vertices.

Proof: Let d = 2 m / n ≥1 be the average degree of the vertices in G. Consider the
following randomized algorithm.

1. Delete each vertex of G (together with its incident edges) independently with prob-
ability 1− 1 / d.
2. For each remaining edge, remove it and one of its adjacent vertices.

The remaining vertices form an independent set, since all edges have been removed.
This is an example of the sample-and-modify technique. We first sample the vertices,
and then we modify the remaining graph.
Let X be the number of vertices that survive the first step of the algorithm. Since
the graph has n vertices and since each vertex survives with probability 1/ d , it follows
that

E [ X ]=
n
d
.
Let Y be the number of edges that survive the first step. There are nd /2 edges in
the graph, and an edge survives if and only if its two adjacent vertices survive.
Thus

E [ Y ]=
nd
2
(
1
d
) 2
=
n
2 d
.
The second step of the algorithm removes all the remaining edges and at most Y
vertices. When the algorithm terminates, it outputs an independent set of size at least

6.5 The Second Moment Method
X − Y , and

E [ X − Y ]=
n
d
−
n
2 d
=
n
2 d
The expected size of the independent set generated by the algorithm is at least n / 2 d ,
so the graph has an independent set with at least n / 2 d = n^2 / 4 m vertices.  /theta

6.4.2 Application: Graphs with Large Girth
As another example we consider the girth of a graph, which is the length of its smallest
cycle. Intuitively we expect dense graphs to have small girth. We can show, however,
that there are dense graphs with relatively large girth.

Theorem 6.6: For any integer k ≥ 3 , for n sufficiently large there is a graph with n
nodes, at least^14 n^1 +^1 / kedges, and girth at least k.

Proof: We first sample a random graph G ∈ Gn , p with p = n^1 / k −^1. Let X be the number
of edges in the graph. Then

E [ X ]= p
(
n
2
)
=
1
2
(
1 −
1
n
)
n^1 / k +^1.
Let Y be the number of cycles in the graph of length at most k −1. Any specific
possible cycle of length i , where 3≤ i ≤ k −1, occurs with probability pi. Also, there
are

( n
i
)( i −1)!
2 possible cycles of length i ; to see this, first consider choosing the i vertices,
then consider the possible orders, and finally keep in mind that reversing the order
yields the same cycle. Hence,

E [ Y ]=
∑ k −^1
i = 3
(
n
i
)
( i −1)!
2
pi ≤
∑ k −^1
i = 3
nipi =
∑ k −^1
i = 3
ni / k < kn ( k −1)/ k.
We modify the original randomly chosen graph G by eliminating one edge from each
cycle of length up to k −1. The modified graph therefore has girth at least k. When n
is sufficiently large, the expected number of edges in the resulting graph is

E [ X − Y ]≥
1
2
(
1 −
1
n
)
n^1 / k +^1 − kn ( k −1)/ k ≥
1
4
n^1 / k +^1.
Hence there exists a graph with at least^14 n^1 +^1 / k edges and girth at least k.  /theta

6.5. The Second Moment Method
The second moment method is another useful way to apply the probabilistic method.
The standard approach typically makes use of the following inequality, which is easily
derived from Chebyshev’s inequality.

Theorem 6.7: If X is an integer-valued random variable, then

Pr( X =0)≤
Var [ X ]
( E [ X ])^2
. (6.2)
the probabilistic method
Proof:

Pr( X =0)≤Pr(| X − E [ X ]|≥ E [ X ])≤
Var [ X ]
( E [ X ])^2
.  /theta
6.5.1 Application: Threshold Behavior in Random Graphs
The second moment method can be used to prove the threshold behavior of certain
random graph properties. That is, in the Gn , p model it is often the case that there is a
threshold function f such that: (a) when p is just less than f ( n ), almost no graph has the
desired property; whereas (b) when p is just larger than f ( n ), almost every graph has
the desired property. We present here a relatively simple example.

Theorem 6.8: In Gn , p, suppose that p = f ( n ) , where f ( n )= o ( n −^2 /^3 ). Then, for any
ε> 0 and for sufficiently large n, the probability that a random graph chosen from Gn , p
has a clique of four or more vertices is less than ε. Similarly, if f ( n )=ω( n −^2 /^3 ) then,
for sufficiently large n, the probability that a random graph chosen from Gn , pdoes not
have a clique with four or more vertices is less than ε.

Proof: We first consider the case in which p = f ( n ) and f ( n )= o ( n −^2 /^3 ). Let
C 1 ,..., C ( n 4 )be an enumeration of all the subsets of four vertices in G. Let

Xi =
{
1if Ci is a 4-clique,
0 otherwise.
Let

X =
∑( n^4 )
i = 1
Xi ,
so that

E [ X ]=
(
n
4
)
p^6.
In this case E [ X ]= o (1), which means that E [ X ]<εfor sufficiently large n. Since X is
a nonnegative integer-valued random variable, it follows that Pr( X ≥1)≤ E [ X ]<ε.
Hence, the probability that a random graph chosen from Gn , p has a clique of four or
more vertices is less thanε.
We now consider the case when p = f ( n ) and f ( n )=ω( n −^2 /^3 ). In this case,
E [ X ]→∞as n grows large. This in itself is not sufficient to conclude that, with high
probability, a graph chosen random from Gn , p has a clique of at least four vertices. We
can, however, use Theorem6.7to prove that Pr( X =0)= o (1) in this case. To do so
we must show that Var [ X ]= o (( E [ X ])^2 ). Here we shall bound the variance directly;
an alternative approach is given as Exercise6.12.
We begin with the following useful formula.

Lemma 6.9: LetYi,i = 1 ,..., m,be0–1randomvariables,andletY =

∑ m
i = 1 Yi.Then
Var [ Y ]≤ E [ Y ]+
∑
1 ≤ i , j ≤ m ; i = j
Cov ( Yi , Yj ).
6.6 The Conditional Expectation Inequality
Proof: For any sequence of random variables Y 1 ,..., Ym ,

Var
[ m
∑
i = 1
Yi
]
=
∑ m
i = 1
Var [ Yi ]+
∑
1 ≤ i , j ≤ m ; i = j
Cov ( Yi , Yj ).
This is the generalization of Theorem3.2to m variables.
When Yi is a 0–1 random variable, E [ Yi^2 ]= E [ Yi ] and so

Var [ Yi ]= E
[
Yi^2
]
−( E [ Yi ])^2 ≤ E [ Yi ],
giving the lemma.  /theta

We wish to compute

Var [ X ]= Var
⎡
⎣
∑( n^4 )
i = 1
Xi
⎤
⎦.
Applying Lemma6.9, we see that we need to consider the covariance of the Xi .If
| Ci ∩ Cj |=0 then the corresponding cliques are disjoint, and it follows that Xi and Xj
are independent. Hence, in this case, E [ XiXj ]− E [ Xi ] E [ Xj ]=0. The same is true if
| Ci ∩ Cj |=1.
If| Ci ∩ Cj |=2, then the corresponding cliques share one edge. For both cliques to
be in the graph, the eleven corresponding edges must appear in the graph. Hence, in
this case E [ XiXj ]− E [ Xi ] E [ Xj ]≤ E [ XiXj ]≤ p^11. There are

( n
6
)
ways to choose the six
vertices and

( 6
2 ; 2 ; 2
)
ways to split them into Ci and Cj (because we choose two vertices
for Ci ∩ Cj , two for Ci alone, and two for Cj alone).
If| Ci ∩ Cj |=3, then the corresponding cliques share three edges. For both cliques
to be in the graph, the nine corresponding edges must appear in the graph. Hence, in
this case E [ XiXj ]− E [ Xi ] E [ Xj ]≤ E [ XiXj ]≤ p^9. There are

( n
5
)
ways to choose the five
vertices, and

( 5
3 ; 1 ; 1
)
ways to split them into Ci and Cj.
Finally, recall again that E [ X ]=
( n
4
)
p^6 and p = f ( n )=ω( n −^2 /^3 ). Therefore,
Var [ X ]≤

(
n
4
)
p^6 +
(
n
6
)(
6
2 ; 2 ; 2
)
p^11 +
(
n
5
)(
5
3 ; 1 ; 1
)
p^9 = o ( n^8 p^12 )= o (( E [ X ])^2 ),
since

( E [ X ])^2 =
((
n
4
)
p^6
) 2
= /eta( n^8 p^12 ).
Theorem6.7now applies, showing that Pr( X =0)= o (1) and thus the second part of
the theorem.  /theta

6.6. The Conditional Expectation Inequality
For a sum of Bernoulli random variables, we can derive an alternative to the second
moment method that is often easier to apply.

the probabilistic method
Theorem 6.10: Let X =

∑ n
i = 1 Xi, where each Xiis a 0–1 random variable. Then
Pr( X >0)≥
∑ n
i = 1
Pr( Xi =1)
E [ X | Xi =1]
. (6.3)
Notice that the Xi need not be independent for Eqn. (6.3) to hold.

Proof: Let Y = 1 / X if X >0, with Y =0 otherwise. Then

Pr( X >0)= E [ XY ].
However,

E [ XY ]= E
[ n
∑
i = 1
XiY
]
=
∑ n
i = 1
E [ XiY ]
=
∑ n
i = 1
(
E [ XiY | Xi =1] Pr( Xi =1)+ E [ XiY | Xi =0] Pr( Xi =0)
)
=
∑ n
i = 1
E [ Y | Xi =1] Pr( Xi =1)
=
∑ n
i = 1
E [1/ X | Xi =1] Pr( Xi =1)
≥
∑ n
i = 1
Pr( Xi =1)
E [ X | Xi =1]
.
The key step is from the third to the fourth line, where we use conditional expectations
in a fruitful way by taking advantage of the fact that E [ XiY | Xi =0]=0. The last line
makes use of Jensen’s inequality, with the convex function f ( x )= 1 / x.  /theta

We can use Theorem6.10to give an alternate proof of Theorem6.8. Specifically, if
p = f ( n )=ω( n −^2 /^3 ), we use Theorem6.10to show that, for any constantε>0 and
for sufficiently large n , the probability that a random graph chosen from Gn , p does not
have a clique with four or more vertices is less thanε.

As in the proof of Theorem6.8, let X =
∑( n 4 )
i = 1 Xi , where Xi is 1 if the subset of four
vertices Ci is a 4-clique and 0 otherwise. For a specific Xj ,wehavePr( Xj =1)= p^6.
Using the linearity of expectations, we compute

E [ X | Xj =1]= E
⎡
⎣
∑( n^4 )
i = 1
Xi
∣∣
Xj = 1
⎤
⎦=
∑( n^4 )
i = 1
E [ Xi | Xj =1].
Conditioning on Xj =1, we now compute E [ Xi | Xj =1] by using that, for a 0–1 ran-
dom variable,

E [ Xi | Xj =1]=Pr( Xi = 1 | Xj =1).
6.7 The Lovász Local Lemma
There are
( n − 4
4
)
sets of vertices Ci that do not intersect Cj. Each corresponding Xi
is 1 with probability p^6. Similarly, Xi =1 with probability p^6 for the 4

( n − 4
3
)
sets Ci that
have one vertex in common with Cj.
For the remaining cases, we have Pr( Xi = 1 | Xj =1)= p^5 for the 6

( n − 4
2
)
sets Ci
that have two vertices in common with Cj and Pr( Xi = 1 | Xj =1)= p^3 for the 4

( n − 4
1
)
sets Ci that have three vertices in common with Cj. Summing, we have

E [ X | Xj =1]=

∑( n^4 )
i = 1
E [ Xi | Xj =1]
= 1 +
(
n − 4
4
)
p^6 + 4
(
n − 4
3
)
p^6 + 6
(
n − 4
2
)
p^5 + 4
(
n − 4
1
)
p^3.
Applying Theorem6.10yields

Pr( X >0)≥
( n
4
)
p^6
1 +
(
n − 4
4
)
p^6 + 4
(
n − 4
3
)
p^6 + 6
(
n − 4
2
)
p^5 + 4
(
n − 4
1
)
p^3
,
which approaches 1 as n grows large when p = f ( n )=ω( n −^2 /^3 ).

6.7. The Lovász Local Lemma
One of the most elegant and useful tools in applying the probabilistic method is the
Lovász Local Lemma. Let E 1 ,..., En be a set of bad events in some probability space.
We want to show that there is an element in the sample space that is not included in
any of the bad events.
This would be easy to do if the events were mutually independent. Recall
that events E 1 , E 2 ,..., En are mutually independent if and only if, for any subset
I ⊆[1, n ],

Pr
(
⋂
i ∈ I
Ei
)
=
∏
i ∈ I
Pr( Ei ).
Also, if E 1 ,..., En are mutually independent then so are E ̄ 1 ,..., E ̄ n. (This was left as
Exercise1.20.) If Pr( Ei )<1 for all i , then

Pr
( n
⋂
i = 1
E ̄ i
)
=
∏ n
i = 1
Pr( E ̄ i )> 0 ,
and there is an element of the sample space that is not included in any bad event.
Mutual independence is too much to ask for in many arguments. The Lovász local
lemma generalizes the preceding argument to the case where the n events are not mutu-
ally independent but the dependency is limited. Specifically, following from the defi-
nition of mutual independence, we say that an event En + 1 is mutually independent of

the probabilistic method
the events E 1 , E 2 ,..., En if, for any subset I ⊆[1, n ],

Pr
⎛
⎝ En + 1
∣∣⋂
j ∈ I
Ej
⎞
⎠=Pr( En + 1 ).
The dependency between events can be represented in terms of a dependency graph.

Definition 6.1: A dependency graph for a set of events E 1 ,..., Enis a graph G =
( V , E ) such thatV ={ 1 ,..., n } and, for i = 1 ,..., n, event Eiis mutually independent
of the events { Ej |( i , j )∈/ E } .The degree of the dependency graph is the maximum
degree of any vertex in the graph.

We discuss first a special case, the symmetric version of the Lovász Local Lemma,
which is more intuitive and is sufficient for most algorithmic applications.

Theorem 6.11 [ Lovász Local Lemma ] : Let E 1 ,..., Enbe a set of events, and assume
that the following hold:

1. for all i, Pr( Ei )≤ p;
2. the degree of the dependency graph given by E 1 ,..., Enis bounded by d;
3. 4 dp ≤ 1_._

Then

Pr
( n
⋂
i = 1
E ̄ i
)
> 0.
Proof: Let S ⊂{ 1 ,..., n }. We prove by induction on s = 0 ,..., n −1 that, if| S |≤ s ,
then for all k ∈/ S we have

Pr
⎛
⎝ Ek
∣∣⋂
j ∈ S
E ̄ j
⎞
⎠≤ 2 p.
For this expression to be well-defined when S is not empty, we need Pr

(⋂
j ∈ SE ̄ j
)
>0.
The base case s =0 follows from the assumption that Pr( Ek )≤ p. To perform the
inductive step, we first show that Pr

(⋂
j ∈ SE ̄ j
)
This is true when s =1, because
Pr( E ̄ j )≥ 1 − p >0. For s >1, without loss of generality let S ={ 1 , 2 ,..., s }. Then
Pr
( s
⋂
i = 1
E ̄ i
)
=
∏ s
i = 1
Pr
⎛
⎝ E ̄ i
∣
∣
i ⋂− 1
j = 1
E ̄ j
⎞
⎠
=
∏ s
i = 1
⎛
⎝ 1 −Pr
⎛
⎝ Ei
∣∣⋂ i −^1
j = 1
E ̄ j
⎞
⎠
⎞
⎠
≥
∏ s
i = 1
(1− 2 p )> 0.
In obtaining the last line we used the induction hypothesis.

6.7 the lovász local lemma
For the rest of the induction, let S 1 ={ j ∈ S |( k , j )∈ E }and S 2 = S − S 1 .If S 2 = S
then Ek is mutually independent of the events E ̄ i , i ∈ S , and

Pr
⎛
⎝ Ek
∣
∣
⋂
j ∈ S
E ̄ j
⎞
⎠=Pr( Ek )≤ p.
We continue with the case| S 2 |< s. It will be helpful to introduce the following nota-
tion. Let FS be defined by

FS =
⋂
j ∈ S
E ̄ j ,
and similarly define FS 1 and FS 2. Notice that FS = FS 1 ∩ FS 2.
Applying the definition of conditional probability yields

Pr( Ek | FS )=
Pr( Ek ∩ FS )
Pr( FS )
. (6.4)
Applying the definition of conditional probability to the numerator of Eqn. (6.4), we
obtain

Pr( Ek ∩ FS )=Pr( Ek ∩ FS 1 ∩ FS 2 )
=Pr( Ek ∩ FS 1 | FS 2 )Pr( FS 2 ).
The denominator can be written as

Pr( FS )=Pr( FS 1 ∩ FS 2 )
=Pr( FS 1 | FS 2 )Pr( FS 2 ).
Canceling the common factor, which we have already shown to be nonzero, yields

Pr( Ek | FS )=
Pr( Ek ∩ FS 1 | FS 2 )
Pr( FS 1 | FS 2 )
. (6.5)
Note that Eqn. (6.5) is valid even when S 2 =∅.
Since the probability of an intersection of events is bounded by the probability of
any one of the events and since Ek is independent of the events in S 2 , we can bound the
numerator of Eqn. (6.5)by

Pr( Ek ∩ FS 1 | FS 2 )≤Pr( Ek | FS 2 )=Pr( Ek )≤ p.
Because| S 2 |<| S |= s , we can apply the induction hypothesis to
Pr( Ei | FS 2 )=Pr
⎛
⎝ Ei ∣∣
⋂
j ∈ S 2
E ̄ j
⎞
⎠.
the probabilistic method
Using also the fact that| S 1 |≤ d , we establish a lower bound on the denominator of
Eqn. (6.5) as follows:

Pr( FS 1 | FS 2 )=Pr
⎛
⎝
⋂
i ∈ S 1
E ̄ i
∣
∣
⋂
j ∈ S 2
E ̄ j
⎞
⎠
≥ 1 −
∑
i ∈ S 1
Pr
⎛
⎝ Ei
∣∣⋂
j ∈ S 2
E ̄ j
⎞
⎠
≥ 1 −
∑
i ∈ S 1
2 p
≥ 1 − 2 pd
≥
1
2
Using the upper bound for the numerator and the lower bound for the denominator,
we prove the induction:

Pr( Ek | FS )=
Pr( Ek ∩ FS 1 | FS 2 )
Pr( FS 1 | FS 2 )
≤
p
1 / 2
= 2 p.
The theorem follows from

Pr
( n
⋂
i = 1
E ̄ i
)
=
∏ n
i = 1
Pr
⎛
⎝ E ̄ i
∣
∣
i ⋂− 1
j = 1
E ̄ j
⎞
⎠
=
∏ n
i = 1
⎛
⎝ 1 −Pr
⎛
⎝ Ei
∣∣⋂ i −^1
j = 1
E ̄ j
⎞
⎠
⎞
⎠
≥
∏ n
i = 1
(1− 2 p )> 0.
 /theta
6.7.1 Application: Edge-Disjoint Paths
Assume that n pairs of users need to communicate using edge-disjoint paths on a given
network. Each pair i = 1 ,..., n can choose a path from a collection Fi of m paths. We
show using the Lovász local lemma that, if the possible paths do not share too many
edges, then there is a way to choose n edge-disjoint paths connecting the n pairs.

Theorem 6.12: If any path in Fishares edges with no more than k paths in Fj, where
i = j and 8 nk / m ≤ 1 , then there is a way to choose n edge-disjoint paths connecting
the n pairs.

Proof: Consider the probability space defined by each pair choosing a path indepen-
dently and uniformly at random from its set of m paths. Define Ei , j to represent the

6.7 the lovász local lemma
event that the paths chosen by pairs i and j share at least one edge. Since a path in Fi
shares edges with no more than k paths in Fj ,

p =Pr( Ei , j )≤
k
m
Let d be the degree of the dependency graph. Since event Ei , j is independent of all
events Ei ′, j ′when i ′∈{/ i , j }and j ′∈{/ i , j },wehave d < 2 n. Since

4 dp <
8 nk
m
≤ 1 ,
all of the conditions of the Lovász Local Lemma are satisfied, proving

Pr
⎛
⎝
⋂
i = j
E ̄ i , j
⎞
⎠> 0.
Hence, there is a choice of paths such that the n paths are edge disjoint.  /theta

6.7.2 Application: Satisfiability
As a second example, we return to the satisfiability question. For the k -satisfiability ( k -
SAT) problem, the formula is restricted so that each clause has exactly k literals. Again,
we assume that no clause contains both a literal and its negation, as these clauses are
trivial. We prove that any k -SAT formula in which no variable appears in too many
clauses has a satisfying assignment.

Theorem 6.13: If no variable in a k-SAT formula appears in more than T = 2 k / 4 k
clauses, then the formula has a satisfying assignment.

Proof: Consider the probability space defined by giving a random assignment to the
variables. For i = 1 ,..., m , let Ei denote the event that the i th clause is not satisfied by
the random assignment. Since each clause has k literals,

Pr( Ei )= 2 − k.
The event Ei is mutually independent of all of the events related to clauses that do
not share variables with clause i. Because each of the k variables in clause i can appear
in no more than T = 2 k / 4 k clauses, the degree of the dependency graph is bounded by
d ≤ kT ≤ 2 k −^2.
In this case,

4 dp ≤ 4 · 2 k −^22 − k ≤ 1 ,
so we can apply the Lovász Local Lemma to conclude that

Pr
( m
⋂
i = 1
E ̄ i
)
> 0 ;
hence there is a satisfying assignment for the formula.  /theta

the probabilistic method
6.8 ∗ Explicit Constructions Using the Local Lemma
The Lovász Local Lemma proves that a random element in an appropriately defined
sample space has a nonzero probability of satisfying our requirement. However, this
probability might be too small for an algorithm that is based on simple sampling. The
number of objects that we need to sample before we find an element that satisfies our
requirements might be exponential in the problem size.
In a number of interesting applications, the existential result of the Lovász Local
Lemma can be used to derive efficient construction algorithms. Although the details
differ in the specific applications, many known algorithms are based on a common two-
phase scheme. In the first phase, a subset of the variables of the problem are assigned
random values; the remaining variables are deferred to the second stage. The subset of
variables that are assigned values in the first stage is chosen so that:

1. using the Local Lemma, one can show that the random partial solution fixed in the
first phase can be extended to a full solution of the problem without modifying any
of the variables fixed in the first phase; and
2. the dependency graph H between events defined by the variables deferred to the
second phase has, with high probability, only small connected components.

When the dependency graph consists of connected components, a solution for the
variables of one component can be found independently of the other components. Thus,
the first phase of the two-phase algorithm breaks the original problem into smaller
subproblems. Each of the smaller subproblems can then be solved independently in the
second phase by an exhaustive search.

6.8.1 Application: A Satisfiability Algorithm
We demonstrate this technique in an algorithm for finding a satisfying assignment for
a k -SAT formula. The explicit construction result will be significantly weaker than the
existence result proven in the previous section. In particular, we obtain a polynomial
time algorithm only for the case when k is a constant. This result is still interesting, since
for k ≥3 the problem of k -satisfiability is NP-complete. For notational convenience we
treat here only the case where k is an even constant with k ≥12; the case where k is a
sufficiently large odd constant is similar.
Consider a k -SAT formulaF, with k an even constant, such that each variable
appears in no more than T = 2 α k clauses for some constantα>0 determined in the
proof. Let x 1 ,..., x nbe the nvariables and C 1 ,..., Cm the m clauses ofF.
Following the outline suggested in Section6.8, our algorithm for finding a satisfying
assignment forFhas two phases. Some of the variables are fixed at the first phase,
and the remaining variables are deferred to the second phase. While executing the first
phase, we call a clause Cidangerous if both the following conditions hold:

1. k /2 literals of the clause Ci have been fixed; and
2. Ci is not yet satisfied.

6.8 ∗ explicit constructions using the local lemma
Phase I can be described as follows. Consider the variables x 1 ,..., x nsequentially.
If xi is not in a dangerous clause, assign it independently and uniformly at random a
value in{ 0 , 1 }.
A clause is a surviving clause if it is not satisfied by the variables fixed in phase I.
Note that a surviving clause has no more than k /2 of its variables fixed in the first
phase. A deferred variable is a variable that was not assigned a value in the first phase.
In phase II, we use exhaustive search in order to assign values to the deferred variables
and so complete a satisfying assignment for the formula.
In the next two lemmas we show that:

1. the partial solution computed in phase I can be extended to a full satisfying assign-
ment ofF, and
2. with high probability, the exhaustive search in phase II is completed in time that is
polynomial in m.

Lemma 6.14: There is an assignment of values to the deferred variables such that all
the surviving clauses are satisfied.

Proof: Let H =( V , E ) be a graph on m nodes, where V ={ 1 ,..., m }, and let ( i , j )∈
E if and only if Ci ∩ Cj =∅. That is, H is the dependency graph for the original prob-
lem. Let H ′=( V ′, E ′) be a graph with V ′⊆ V and E ′⊆ E such that (a) i ∈ V ′if and
only if Ci is a surviving clause and (b) ( i , j )∈ E ′if and only if Ci and Cj share a deferred
variable. In the following discussion we do not distinguish between node i and clause i.
Consider the probability space defined by assigning a random value in{ 0 , 1 }inde-
pendently to each deferred variable. The assignment of values to the nondeferred vari-
ables in phase I, together with the random assignment of values to the deferred vari-
ables, defines an assignment to all the nvariables. For i = 1 ,..., m , let Ei be the event
that surviving clause Ci is not satisfied by this assignment. Associate the event Ei with
node i in V ′. The graph H ′is then the dependency graph for this set of events.
A surviving clause has at least k /2 deferred variables, so

p =Pr( Ei )≤ 2 − k /^2.
A variable appears in no more than T clauses; therefore, the degree of the dependency
graph is bounded by

d = kT ≤ k 2 α k.
For any k ≥12, there is a corresponding suitably small constantα>0sothat

4 dp = 4 k 2 α k 2 − k /^2 ≤ 1
and so, by the Lovász Local Lemma, there is an assignment for the deferred variables
that – together with the assignment of values to variables in phase I – satisfies the
formula.  /theta

The assignment of values to a subset of the variables in phase I partitions the problem
into as many as m independent subformulas, so that each deferred variable appears
in only one subformula. The subformulas are given by the connected components of
H ′. If we can show that each connected component in H ′has size O (log m ), then each

the probabilistic method
subformula will have no more than O ( k log m ) deferred variables. An exhaustive search
of all the possible assignments for all variables in each subformula can then be done in
polynomial time. Hence we focus on the following lemma.

Lemma 6.15: All connected components in H ′ are of size O (log m ) with probability
1 − o (1).

Proof: Consider a connected component R of r vertices in H .If R is a connected com-
ponent in H ′, then all its r nodes are surviving clauses. A surviving clause is either
a dangerous clause or it shares at least one deferred variable with a dangerous clause
(i.e., it has a neighbor in H ′that is a dangerous clause). The probability that a given
clause is dangerous is at most 2− k /^2 , since exactly k /2 of its variables were given ran-
dom values in phase I yet none of these values satisfied the clause. The probability that
a given clause survives is the probability that either this clause or at least one of its
direct neighbors is dangerous, which is bounded by

( d +1)2− k /^2 ,
where again d = kT >1.
If the survival of individual clauses were independent events then we would be in
excellent shape. However, from our description here it is evident that such events are
not independent. Instead, we identify a subset of the vertices in R such that the survival
of the clauses represented by the vertices of this subset are independent events. A 4-tree
S of a connected component R in H is defined as follows:

1. S is a rooted tree;
2. any two nodes in S are at distance at least 4 in H ;
3. there can be an edge in S only between two nodes with distance exactly 4 between
them in H ;
4. any node of R is either in S or is at distance 3 or less from a node in S.

Considering the nodes in a 4-tree proves useful because the event that a node u in
a 4-tree survives and the event that another node v in a 4-tree survives are actually
independent. Any clause that could cause u to survive has distance at least 2 from
any clause that could cause v to survive. Clauses at distance 2 share no variables, and
hence the events that they are dangerous are independent. We can take advantage of
this independence to conclude that, for any 4-tree S , the probability that the nodes in
the 4-tree survive is at most

(( d +1)2− k /^2 )| S |.
A maximal 4-tree S of a connected component R is the 4-tree with the largest possible
number of vertices. Since the degree of the dependency graph is bounded by d , there
are no more than

d + d ( d −1)+ d ( d −1)( d −1)≤ d^3 − 1
nodes at distance 3 or less from any given vertex. We therefore claim that a maximal
4-tree of R must have at least r / d^3 vertices. Otherwise, when we consider the vertices
of the maximal 4-tree S and all neighbors within distance 3 or less of these vertices,
we obtain fewer than r vertices. Hence there must be a vertex of distance at least 4

6.9 Lovász Local Lemma: The General Case
from all vertices in S. If this vertex has distance exactly 4 from some vertex in S , then
it can be added to S and thus S is not maximal, yielding a contradiction. If its dis-
tance is larger than 4 from all vertices in S , consider any path that brings it closer to S ;
such a path must eventually pass through a vertex of distance at least 4 from all ver-
tices in S and of distance 4 from some vertex in S , again contradicting the maximality
of S.
To show that with probability 1− o (1) there is no connected component R of size
r ≥ c log 2 m for some constant c in H ′, we show that there is no 4-tree of H of size
r / d^3 that survives with probability 1− o (1). Since a surviving connected component
R would have a maximal 4-tree of size r / d^3 , the absence of such a 4-tree implies the
absence of such a component.
We need to count the number of 4-trees of size s = r / d^3 in H. We can choose the
root of the 4-tree in m ways. A tree with root v is uniquely defined by an Eulerian tour
that starts and ends at v and traverses each edge of the tree twice, once in each direction.
Since an edge of S represents a path of length 4 in H , at each vertex in the 4-tree the
Eulerian path can continue in as many as d^4 different ways, and therefore the number
of 4-trees of size s = r / d^3 in H is bounded by

m ( d^4 )^2 s = md^8 r / d
3
.
The probability that the nodes of each such 4-tree survive in H ′is at most

(( d +1)2− k /^2 ) s =(( d +1)2− k /^2 ) r / d
3
.
Hence the probability that H ′has a connected component of size r is bounded by

md^8 r / d
3
(( d +1)2− k /^2 ) r / d
3
≤ m 2 ( rk / d
(^3) )(8α+ 2 α− 1 /2)
= o (1)
for r ≥ c log 2 m and for a suitably large constant c and a sufficiently small constant
α>0.  /theta
Thus, we have the following theorem.
Theorem 6.16: Consider a k-SAT formula with m clauses, where k ≥ 12 is an even
constant and each variable appears in up to 2 α kclauses for a sufficiently small constant
α> 0_. Then there is an algorithm that finds a satisfying assignment for the formula in
expected time that is polynomial in m._
Proof: As we have described, if the first phase partitions the problem into subformu-
las involving only O ( k log m ) variables, then a solution can be found by solving each
subformula exhaustively in time that is polynomial in m. The probability of the first
phase partitioning the problem appropriately is 1− o (1), so we need only run phase I
a constant number of times on average before obtaining a good partition. The theorem
follows.  /theta

6.9. Lovász Local Lemma: The General Case
For completeness we include the statement and proof of the general case of the Lovász
Local Lemma.

the probabilistic method
Theorem 6.17: Let E 1 ,..., Enbe a set of events in an arbitrary probability space,
and let G =( V , E ) be the dependency graph for these events. Assume there exist
x 1 ,..., xn ∈[0,1] such that, for all 1 ≤ i ≤ n,

Pr( Ei )≤ xi
∏
( i , j )∈ E
(1− xj ).
Then

Pr
( n
⋂
i = 1
E ̄ i
)
≥
∏ n
i = 1
(1− xi ).
Proof: Let S ⊆{ 1 ,..., n }. We prove by induction on s = 0 ,..., n that, if| S |≤ s , then
for all k we have

Pr
⎛
⎝ Ek
∣
∣
⋂
j ∈ S
E ̄ j
⎞
⎠≤ xk.
As in the case of the symmetric version of the Local Lemma, we must be careful that
the conditional probability is well-defined. This follows using the same approach as in
the symmetric case, so we focus on the rest of the induction.
The base case s =0 follows from the assumption that

Pr( Ek )≤ xk
∏
( k , j )∈ E
(1− xj )≤ xk.
For the inductive step, let S 1 ={ j ∈ S |( k , j )∈ E }and S 2 = S − S 1 .If S 2 = S then
Ek is mutually independent of the events E ̄ i , i ∈ S , and

Pr
⎛
⎝ Ek
∣
∣
⋂
j ∈ S
E ̄ j
⎞
⎠=Pr( Ek )≤ xk.
We continue with the case| S 2 |< s. We again use the notation

FS =
⋂
j ∈ S
E ̄ j
and define FS 1 and FS 2 similarly, so that FS = FS 1 ∩ FS 2.
Applying the definition of conditional probability yields

Pr( Ek | FS )=
Pr( Ek ∩ FS )
Pr( FS )
. (6.6)
By once again applying the definition of conditional probability, the numerator of
Eqn. (6.6) can be written as

Pr( Ek ∩ FS )=Pr( Ek ∩ FS 1 | FS 2 )Pr( FS 2 )
and the denominator as

Pr( FS )=Pr( FS 1 | FS 2 )Pr( FS 2 ).
6.9lovász local lemma: the general case
Canceling the common factor then yields

Pr( Ek | FS )=
Pr( Ek ∩ FS 1 | FS 2 )
Pr( FS 1 | FS 2 )
. (6.7)
Since the probability of an intersection of events is bounded by the probability of
each of the events and since Ek is independent of the events in S 2 , we can bound the
numerator of Eqn. (6.7)by

Pr( Ek ∩ FS 1 | FS 2 )≤Pr( Ek | FS 2 )=Pr( Ek )≤ xk
∏
( k , j )∈ E
(1− xj ).
To bound the denominator of Eqn. (6.7), let S 1 ={ j 1 ,..., jr }. Applying the induc-
tion hypothesis, we have

Pr( FS 1 | FS 2 )=Pr
⎛
⎝
⋂
j ∈ S 1
E ̄ j
∣
∣
⋂
j ∈ S 2
E ̄ j
⎞
⎠
=
∏ r
i = 1
⎛
⎝ 1 −Pr
⎛
⎝ Eji
∣∣
( i − 1
⋂
t = 1
E ̄ jt
)
∩
⎛
⎝
⋂
j ∈ S 2
E ̄ j
⎞
⎠
⎞
⎠
⎞
⎠
≥
∏ r
i = 1
(1− xji )
≥
∏
( k , j )∈ E
(1− xj ).
Using the upper bound for the numerator and the lower bound for the denominator,
we can prove the induction hypothesis:

Pr
⎛
⎝ Ek
∣∣⋂
j ∈ S
E ̄ j
⎞
⎠=Pr( Ek | FS )
=
Pr( Ek ∩ FS 1 | FS 2 )
Pr( FS 1 | FS 2 )
≤
xk
∏
∏( k , j )∈ E (1− xj )
( k , j )∈ E (1− xj )
= xk.
The theorem now follows from:
Pr( E ̄ 1 ,..., E ̄ n )=
∏ n
i = 1
Pr( E ̄ i | E ̄ 1 ,..., E ̄ i − 1 )
=
∏ n
i = 1
(1−Pr( Ei | E ̄ 1 ,..., E ̄ i − 1 ))
≥
∏ n
i = 1
(1− xi )> 0.
 /theta
the probabilistic method
6.10 ∗The Algorithmic Lovász Local Lemma
Recently, there have been several advances in extending the Lovász Local Lemma. We
briefly summarize the key points here, and start by looking again to the k -SAT problem
to provide an example of these ideas in action.
We have shown previously that if no variable in a k -SAT formula appears in more
than 2 k /(4 k ) clauses, then the formula has a satisfying assignment, and we have shown
that if each variable appears in no more that 2α k clauses for some constantαa solution
can be found in expected polynomial time. Here we provide an improved result which
again provides a solution in expected polynomial time.

Theorem 6.18: Suppose that every clause in a k-SAT formula shares one or more
variables with at most 2 k −^3 − 1 other clauses. Then a solution for the formula exists
and can be found in expected time that is polynomial in the number of clauses m.

Before starting the proof, we informally describe our algorithm. As before, let
x 1 , x 2 ,..., x nbe the nvariables and C 1 , C 2 ,..., Cm be the m clauses in the formula.
We begin by choosing a random truth assignment (uniformly at random). We then look
for a clause Ci that is unsatisfied; if no such clause exists we are done. If such a clause
exists, we look specifically at the variables in the clause, and randomly choose a new
truth assignment for those variables. Doing so will hopefully “fix” the clause Ci so that
it is satisfied, but it may not; even worse, it may end up causing a clause Cj that shares
a variable with Ci to become unsatisfied. We recursively fix these neighboring clauses,
so that when the recursion is finished, we have that Ci is satisfied and we have not
damaged any clause by making it become unsatisfied. We therefore have improved the
situation by satisfying at least one previously unsatisfied clause. We then continue to
the next unsatisfied clause; we have to do this at most m times.
The underlying question that we need to answer to show that this algorithm works
is how we know that the recursion we have described stops successfully. Perhaps it
simply goes on forever, or for an exponential amount of time. The proof we provide
shows that this cannot be the case through a new type of argument. Specifically, we
show that if such bad recursions occur with non-trivial probability, then one could
compress a random string of n independent, unbiased flips into much fewer than n bits.
That should seem impossible, and it is. While compression is a theme we cover in much
more detail in Chapter 10 , we explain the compression result we need here in the proof
of the theorem. All we need is that a string of r random bits, where each bit is chosen
independently and uniformly at random, cannot be compressed so that the average
length of the representation over all choices of the r random bits is less than r −2.
To see that this must be true, assume the best possible setting for us, where we don’t
have to worry about the “end” of our compressed sequence, but can use each string of
bits of length less than r to represent one of the 2 r possible strings we aim to compress.
That is, we won’t worry that one compressed string might be “0” and another one
might be “00”, in which case it might be hard to distinguish whether “00” was meant
to represent a single compressed string, or two copies of the string represented by “0”.
(Essentially, a compressed string can be terminated for free; this allowance can only

6.10 ∗ the algorithmic lovász local lemma
hurt us in our argument.) Still, each string of s < r bits can only represent a single
possible string of length r. Hence we have available one string of length 0 (the empty
string), two strings of length 1, and so on. There are only 2 r −1 strings of length less
than r ; even if we count only those in computing the average length of the compressed
string, which again can only hurt us, the average length would be at least

∑ r −^1
i = 1
1
2 r − i
· i ≥ r − 2.
The same compression fact naturally holds true for any collection of 2 r equally likely
strings; they do not have to be limited to strings of r random bits.
Given this fact, our proof proceeds as follows.

Proof of Theorem6.18: The algorithm we use is explicitly provided as the k -
Satisfiability Algorithm below.

k -Satisfiability Algorithm (Using Algorithmic LLL):
Input: A collection C 1 , C 2 ,..., Cm of clauses for a k -SAT formula over n variables.
Output: A truth assignment for these variables.
Main routine:
1. Start with a random assignment.
2. While some Ci is not satisfied:
(a) choose the unsatisfied Ci with the smallest value of i ;
(b) enter i in binary usinglog 2 m bits in the history;
(c) use localcorrect on clause Ci.
localcorrect(C):
1. Resample new values for every variable in clause C.
2. While some Cj that shares a variable with C (including possibly C itself) is not
satisfied
(a) choose the unsatisfied Cj sharing a variable with C with the smallest value
of j ;
(b) enter “0” followed by j in binary using k −3 bits in the history;
(c) use localcorrect on clause Cj.
3. Enter “1” in the history.

We note the algorithm produces a history, which we use in the analysis of the algo-
rithm.
It is important to realize that while a clause can become satisfied and unsatisfied
again multiple times through the recursive process, when we return to the main routine
and complete the call to localcorrect, we have satisfied the clause Ci that localcorrect
was called on from the main routine, and further any clause that was previously satisfied
has stayed satisfied because of the recursion. What we wish to show is that the recursive
process has to stop.

the probabilistic method
Our analysis makes use of the fact that our algorithm makes use of a random string.
We provide two different ways to describe how our algorithm runs.
We can think of our algorithm as being described by the random string of bits it uses.
It takes n bits to initially assign random truth values to each of the variables. After that,
it takes k bits to resample values for a clause each time localcorrect is called. Let us refer
to each time localcorrect is called as a round. Then one way to describe our algorithm’s
actions for j rounds is with the random string of n + jk bits used by the algorithm.
But here is another way of describing how our algorithm works. We keep track of the
“history” of the algorithm as shown in the algorithm. The history includes a list of the
clauses that localcorrect is called on by the main routine. The history also includes a list
of the recursive calls to localcorrect, in a slightly non-obvious way. First, we note that
the algorithm uses a flag bit 0 and a flag bit 1 to mark the start and end of recursive calls,
so the algorithm tracks the stack of recursive calls in a natural way. Second, instead of
the natural approach of usinglog 2 m bits to represent the index of the clause in our
recursive calls, the algorithm uses only k −3 bits. We now explain why only k −3 bits
are needed. Since there are at most 2 k −^3 possible clauses that share a variable with the
current clause (including the current clause itself) that could be the next one called, the
clause can be represented by an index of k −3 bits. (Imagine having an ordered list of
the up to 2 k −^3 clauses that share a variable with each clause; we just need the index into
that list.) Finally, our history will also include the current truth assignment of n bits.
Note that the current truth assignment can be thought of as in an separate updatable
storage area for the history; every time the truth assignment is updated, so is this part
of the history.
We now show that when the algorithm has run j rounds, we can recover the random
string of n + jk bits that the algorithm has used from the history we have described.
Start with the current truth assignment, and break the history up, using the flags that
mark invocations of localcorrect. We can use the history to determine the sequence of
recursive calls, and what clauses localcorrect was called on. Then, going backwards
through the history, we know at each step which clause was being resampled. For that
clause to have to be resampled, it must have been unsatisfied previously. But there is
only one setting of the variables that makes a clause unsatisfied, and hence we know
what the truth values for those variables were before the clause was resampled. We
can therefore update the current truth assignment so that it represents the truth assign-
ment before the resampling, and continue backwards through the process. Repeating
this action, we can determine the original truth assignment, and since at each step we
can determine what variable values were changed and what their values were on each
resampling, we recover the whole string of n + jk random bits.
Our history takes at most n + m log 2 m + j ( k −1) bits; here we use the fact that
each resampling uses at most k −1 bits, including the two bits that may be necessary as
flags for the start and end of the recursion given by that resampling. For large enough j ,
our history yields a compressed form of the the random string used to run the algorithm,
since only k −1 bits are used to represent each resampling in the history instead of the
k bits used by the algorithm.
Now suppose there were no truth assignment, in which case the algorithm would
run forever. Then after a large enough number of rounds J , the history will be at most

6.10 ∗ the algorithmic lovász local lemma
n + m log 2 m + J ( k −1) bits, while the random string running the algorithm would
be n + Jk bits. By our result on compressing random strings, we must have

n + m log 2 m + J ( k −1)≥ n + Jk − 2.
Hence

J ≤ m log 2 m + 2.
This contradicts that the algorithm can run forever, so there must be a truth assignment.
Similarly, the number of rounds J is more than m log 2 m + 2 + i with probability
at most 2− i. To see this, suppose the probability of lasting to this round is greater than
2 − i. Again consider the algorithm after J = m log 2 m + 2 + i rounds, so the history
will be at most n + m log 2 m + J ( k −1) bits. The algorithm can also be described
by the n + Jk random bits that led to the current state. As there are at least 2 n + Jk ran-
dom bit strings of this length, and the probability of lasting at least this many rounds
is greater than 2− i by assumption, there are at least 2 n + Jk − i random bit strings associ-
ated with reaching this round. By our result on compressing random strings, it requires
more than n + Jk − i −2 bits on average to represent the at least 2 n + Jk − i random bit
strings associated with reaching this round. But the history, as we have already argued,
provides a representation of these random bit strings, in that we can reconstruct the
algorithm’s random bit string from the history. The number of bits the history uses is
only

n + m log 2 m + J ( k −1)= n + Jk − i − 2 ,
a contradiction.
Since the probability of lasting more than m log 2 m + 2 + i is at most 2− i , we can
bound the expected number of rounds by

m log 2 m + 2 +
∑∞
i = 1
i 2 − i.
The expected number rounds used by the algorithm is thus at most m log 2 m +4.
The work done in each resampling round can easily be made to be polynomial in
m , so the total expected time to find an assigment can be made polynomial in m as
well.  /theta

While already surprising, the proof above can be improved slightly. A more careful
encoding shows that the expected number of rounds required can be reduced to O ( m )
instead of O ( m log m ). This is covered in the Exercise6.21.
The algorithmic approach we have used for the satisfiability problem in the proof of
Theorem6.18can be extended further to obtain an algorithmic version of the Lovász
local lemma, which we now describe. Let us suppose that we have a collection of n
events E 1 , E 2 ,..., En that depend on a collection of nmutually independent variables
y 1 , y 2 ,..., y n. The dependency graph on events has an edge between two events if they
both depend on at least one shared variable yi. The idea is that at each step if there
is an event that is unsatisfied, we resample only the random variables on which that
event depends. As with the k -Satisfiability Algorithm using the algorithmic Lovász

the probabilistic method
Local Lemma, this resampling process has to be ordered carefully to ensure progress.
If the dependencies are not too great, then the right resampling algorithm terminates
with a solution.
The symmetric version is easier to state.

Theorem 6.19: Let E 1 , E 2 ,..., Enbe a set of events in an arbitrary probability space
that are determined by mutually independent random variables y 1 , y 2 ,..., y n , and let
G =( V , E ) be the dependency graph for these events. Suppose the following conditions
hold for values d and p:

1. each event Eiis adjacent to at most d other events in the dependency graph, or
equivalently, there are only d other events that also depend on one or more of the
yjthat Eidepends on;
2. Pr( Ei )≤ p;
3. e p ( d +1)≤ 1_._

Then there exists an assignment of the yiso that the event ∩ ni = 1 E ̄ iholds, and a resam-
pling algorithm with the property that the expected number of times the algorithm
resamples the event Eiin finding such an assignment is at most 1 / d. Hence the expected
total number of resampling steps taken by the algorithm is at most n / d.

However, we also have a corresponding theorem for the asymmetric version.
Theorem 6.20: Let E 1 , E 2 ,..., Enbe a set of events in an arbitrary probability
space that are determined by mutually independent random variables y 1 , y 2 ,..., y n ,
and let G =( V , E ) be the dependency graph for these events. Assume there exist
x 1 , x 2 ,..., xn ∈[0,1] such that, for all 1 ≤ i ≤ n

Pr( Ei )≤ xi
∏
( i , j )∈ E
(1− xj ).
Then there exists an assignment of the yiso that the event ∩ ni = 1 E ̄ iholds, and a resam-
pling algorithm with the property that the expected number of times the algorithm
resamples the event Eiin finding such an assignment is at most xi /(1− xi ). Hence
the expected total number of resampling steps taken by the algorithm is at most ∑
n
i = 1 xi /(1− xi ).
The proofs of these theorems are beyond the scope of the book. Similar to the algo-
rithm for satisfiability based on resampling given above, the proof relies on bounding
the expected number of resamplings that occur over the course of the algorithm.

6.11 Exercises
Exercise 6.1: Consider an instance of SAT with m clauses, where every clause has
exactly k literals.

(a) Give a Las Vegas algorithm that finds an assignment satisfying at least m (1− 2 − k )
clauses, and analyze its expected running time.
6.11exercises
(b) Give a derandomization of the randomized algorithm using the method of condi-
tional expectations.

Exercise 6.2:

(a) Prove that, for every integer n , there exists a coloring of the edges of the complete
graph Kn by two colors so that the total number of monochromatic copies of K 4 is
at most

( n
4
)
2 −^5.
(b) Give a randomized algorithm for finding a coloring with at most

( n
4
)
2 −^5 mono-
chromatic copies of K 4 that runs in expected time polynomial in n.
(c) Show how to construct such a coloring deterministically in polynomial time using
the method of conditional expectations.
Exercise 6.3: Given an n -vertex undirected graph G =( V , E ), consider the following
method of generating an independent set. Given a permutationσof the vertices, define
a subset S (σ) of the vertices as follows: for each vertex i , i ∈ S (σ) if and only if no
neighbor j of i precedes i in the permutationσ.

(a) Show that each S (σ) is an independent set in G.
(b) Suggest a natural randomized algorithm to produceσfor which you can show that
the expected cardinality of S (σ)is
∑ n

i = 1
1
di + 1
,
where di denotes the degree of vertex i.
(c) Prove that G has an independent set of size at least
∑ n
i = 11 /( di +1).
Exercise 6.4: Consider the following two-player game. The game begins with k tokens
placed at the number 0 on the integer number line spanning [0, n ]. Each round, one
player, called the chooser, selects two disjoint and nonempty sets of tokens A and B.
(The sets A and B need not cover all the remaining tokens; they only need to be disjoint.)
The second player, called the remover, takes all the tokens from one of the sets off the
board. The tokens from the other set all move up one space on the number line from
their current position. The chooser wins if any token ever reaches n. The remover wins
if the chooser finishes with one token that has not reached n.

(a) Give a winning strategy for the chooser when k ≥ 2 n.
(b) Use the probabilistic method to show that there must exist a winning strategy for
the remover when k < 2 n.
(c) Explain how to use the method of conditional expectations to derandomize the
winning strategy for the remover when k < 2 n.

Exercise 6.5: We have shown using the probabilistic method that, if a graph G has n
nodes and m edges, then there exists a partition of the n nodes into sets A and B such
that at least m /2 edges cross the partition. Improve this result slightly: show that there
exists a partition such that at least mn /(2 n −1) edges cross the partition.

the probabilistic method
Exercise 6.6: We can generalize the problem of finding a large cut to finding a large
k -cut. A k -cut is a partition of the vertices into k disjoint sets, and the value of a cut is
the weight of all edges crossing from one of the k sets to another. In Section6.2.1we
considered 2-cuts when all edges had the same weight 1, showing via the probabilistic
method that any graph G with m edges has a cut with value at least m /2. Generalize
this argument to show that any graph G with m edges has a k -cut with value at least
( k −1) m / k. Show how to use derandomization (following the argument of Section6.3)
to give a deterministic algorithm for finding such a cut.

Exercise 6.7: A hypergraph H is a pair of sets ( V , E ), where V is the set of vertices
and E is the set of hyperedges. Every hyperedge in E is a subset of V. In particular, an
r -uniform hypergraph is one where the size of each edge is r. For example, a 2-uniform
hypergraph is just a standard graph. A dominating set in a hypergraph H is a set of
vertices S ⊂ V such that e ∩ S =∅for every edge e ∈ E. That is, S hits every edge of
the hypergraph.
Let H =( V , E )bean r -uniform hypergraph with n vertices and m edges. Show
that there is a dominating set of size at most np +(1− p ) rm for every real number
0 ≤ p ≤1. Also, show that there is a dominating set of size at most ( m + n ln r )/ r.

Exercise 6.8: Prove that, for every integer n , there exists a way to 2-color the edges
of Kx so that there is no monochromatic clique of size k when

x = n −
(
n
k
)
21 −
( k
2
)
.
( Hint: Start by 2-coloring the edges of Kn , then fix things up.)

Exercise 6.9: A tournament is a graph on n vertices with exactly one directed edge
between each pair of vertices. If vertices represent players, then each edge can be
thought of as the result of a match between the two players: the edge points to the win-
ner. A ranking is an ordering of the n players from best to worst (ties are not allowed).
Given the outcome of a tournament, one might wish to determine a ranking of the play-
ers. A ranking is said to disagree with a directed edge from y to x if y is ahead of x in
the ranking (since x beat y in the tournament).

(a) Prove that, for every tournament, there exists a ranking that disagrees with at most
50% of the edges.
(b) Prove that, for sufficiently large n , there exists a tournament such that every ranking
disagrees with at least 49% of the edges in the tournament.

Exercise 6.10: A family of subsetsFof{ 1 , 2 ,..., n }is called an antichain if there is
no pair of sets A and B inFsatisfying A ⊂ B.

(a) Give an example ofFwhere|F|=
( n
n / 2 
)
.
(b) Let fk be the number of sets inFwith size k. Show that

∑ n
k = 0
fk
( n
k
)≤ 1.
6.11exercises
( Hint: Choose a random permutation of the numbers from 1 to n , and let Xk =1if
the first k numbers in your permutation yield a set inF.If X =
∑ n
k = 0 Xk , what can
you say about X ?)
(c) Argue that|F|≤
( n
n / 2 
)
for any antichainF.
Exercise 6.11: Consider a graph in Gn , p with n vertices and each pair of vertices
independently connected by an edge with probability p. We prove a threshold for the
existence of triangles in the graph.
Let t 1 ,..., t ( n 3 )be an enumeration of all triplets of three vertices in the graph. Let
Xi =1 if the the three edges of the triplet ti appear in the graph, so that ti forms a triangle

in the graph. Otherwise Xi =0. Let X =

∑( n 3 )
i = 1 Xi.
(a) Compute E [ X ].
(b) Use (a) to show that if pn →0 then Pr( X >0)→0.
(c) Show that Var [ Xi ]≤ p^3.
(d) Show that Cov ( Xi , Xj )= p^5 − p^6 for O ( n^4 ) pairs i = j , otherwise Cov ( Xi , Xj )=

(e) Show that Var [ X ]= O ( n^3 p^3 + n^4 ( p^5 − p^6 )).
(f) Conclude that if p is such that pn →∞then Pr( X =0)→0.

Exercise 6.12: In Section6.5.1, we bounded the variance of the number of 4-cliques
in a random graph in order to demonstrate the second moment method. Show how to
calculate the variance directly by using the equality from Exercise3.9:for X =

∑ n
1 = 1 Xi
the sum of Bernoulli random variables,

E [ X^2 ]=
∑ n
i = 1
Pr( Xi =1) E [ X | Xi =1].
Exercise 6.13: Consider the problem of whether graphs in Gn , p have cliques of con-
stant size k. Suggest an appropriate threshold function for this property. Generalize the
argument used for cliques of size 4, using either the second moment method or the
conditional expectation inequality, to prove that your threshold function is correct for
cliques of size 5.

Exercise 6.14: Consider a graph in Gn , p , with p = c ln n / n. Use the second moment
method or the conditional expectation inequality to prove that if c <1 then, for any
constantε>0 and for n sufficiently large, the graph has isolated vertices with proba-
bility at least 1−ε.

Exercise 6.15: Consider a graph in Gn , p , with p = 1 / n. Let X be the number of trian-
gles in the graph, where a triangle is a clique with three edges. Show that

Pr( X ≥1)≤ 1 / 6
and that

lim
n →∞
Pr( X ≥1)≥ 1 / 7.
( Hint: Use the conditional expectation inequality.)

the probabilistic method
Exercise 6.16: Consider the set-balancing problem of Section4.4. We claim that there
is an n × n matrix A for which‖ A b ̄‖∞is m

(√
n
)
for any choice of ̄ b. For convenience
here we assume that n is even.

(a) We have shown in Eqn. (5.5) that
n !≤e
√
n
(
n
e
) n
.
Using similar ideas, show that
n !≥ a
√
n
(
n
e
) n
for some positive constant a.
(b) Let b 1 , b 2 ,..., bm / 2 all equal 1, and let bm / 2 + 1 , bm / 2 + 2 ,..., bm all equal−1. Let
Y 1 , Y 2 ,..., Ym each be chosen independently and uniformly at random from{ 0 , 1 }.
Show that there exists a positive constant c such that, for sufficiently large m ,

Pr
(∣∣
∣∣
∣
∑ m
i = 1
biYi
∣∣
∣∣
∣> c
√
m
)
>
1
2
.
( Hint: Condition on the number of Yi that are equal to 1.)
(c) Let b 1 , b 2 ,..., bm each be equal to either 1 or−1. Let Y 1 , Y 2 ,..., Ym each be cho-
sen independently and uniformly at random from{ 0 , 1 }. Show that there exists a
positive constant c such that, for sufficiently large m ,
Pr
(∣∣
∣
∣∣
∑ m
i = 1
biYi
∣∣
∣
∣∣> c
√
m
)
>
1
2
.
(d) Prove that there exists a matrix A for which‖ A b ̄‖∞is m

(√
n
)
for any choice of b ̄.
Exercise 6.17: Use the Lovász Local Lemma to show that, if

4
(
k
2
)(
n
k − 2
)
21 −
( k
2
)
≤ 1 ,
then it is possible to color the edges of Kn with two colors so that it has no monochro-
matic Kk subgraph.

Exercise 6.18: Use the general form of the Lovász Local Lemma to prove that the
symmetric version of Theorem 6.11 can be improved by replacing the condition 4 dp ≤
1 by the weaker condition e p ( d +1)≤1.

Exercise 6.19: Let G =( V , E ) be an undirected graph and suppose eachv∈ V is
associated with a set S (v)of8 r colors, where r ≥1. Suppose, in addition, that for each
v∈ V and c ∈ S (v) there are at most r neighbors u of v such that c lies in S ( u ). Prove
that there is a proper coloring of G assigning to each vertex v a color from its class
S (v) such that, for any edge ( u ,v)∈ E , the colors assigned to u and v are different.

6.11exercises
You may want to let Au ,v, c be the event that u and v are both colored with color c and
then consider the family of such events.

Exercise 6.20: A k -uniform hypergraph is an ordered pair G =( V , E ), but edges con-
sist of sets of k (distinct) vertices, instead of just 2. (So a 2-uniform hypergraph is just
what we normally call a graph.) A hypergraph is k -regular if all vertices have degree
k ; that is, they are in k hypergraph edges.
Show that for sufficiently large k , the vertices of a k -uniform, k -regular hypergraph
can be 2-colored so that no edge is monochromatic. What’s the smallest value of k you
can achieve?

Exercise 6.21: In our description of the k -Satisfiability Algorithm using the algo-
rithmic Lovász local lemma, we usedlog 2 m bits in the history to represent each
clause called in the main routine. Instead, however, we could simply record in the his-
tory which clauses are initially unsatisfied with an array of m bits. Explain any other
changes you need to make in the algorithm in order to properly record a history that
you can “reverse” to obtain the initial assignment, and explain how this allows one to
modify the proof of Theorem6.18so that only O ( m ) rounds are needed in expectation.

Exercise 6.22: Implement the algorithmic Lovász Local Lemma for the following
scenario. Consider a 9-SAT formula where each variable appears in 8 clauses. Set up
a formula with 112,500 variables and 100,000 clauses in the following manner: set up
8 copies of each of the 112,500 variables (900,000 total variables), permute them, and
use the ordering to assign the variables to the 100,000 clauses. (If any clauses share a
variable, which is likely to happen, try to locally correct for this by swapping one copy
to another clause.) Then assign a random “sign” to each variable – with probability
1 /2, use ̄ x instead of x. This gives a formula that satisfies the conditions of Theorem
6.18.
Your implementation of the algorithmic Lovász Local Lemma does not need to keep
track of the history. However, you should track how many times the local correction
procedure is required before termination. Repeat this experiment with 100 different
formulas derived from the process above, and report on the distribution of the number
of local corrections required. Note that you may want to take some care to make the
local correction step efficient in order to have your program run effectively.

chapter seven

Markov Chains and

Random Walks

Markov chains provide a simple but powerful framework for modeling random pro-
cesses. We start this chapter with the basic definitions related to Markov chains and
then show how Markov chains can be used to analyze simple randomized algorithms
for the 2-SAT and 3-SAT problems. Next we study the long-term behavior of Markov
chains, explaining the classifications of states and conditions for convergence to a sta-
tionary distribution. We apply these techniques to analyzing simple gambling schemes
and a discrete version of a Markovian queue. Of special interest is the limiting behav-
ior of random walks on graphs. We prove bounds on the covering time of a graph and
use this bound to develop a simple randomized algorithm for the s–t connectivity prob-
lem. Finally, we apply Markov chain techniques to resolve a subtle probability problem
known as Parrondo’s paradox.

7.1 Markov Chains: Definitions and Representations
A stochastic process X ={ X ( t ): t ∈ T }is a collection of random variables. The index
t often represents time, and in that case the process X models the value of a random
variable X that changes over time.
We call X ( t ) the state of the process at time t. In what follows, we use Xt inter-
changeably with X ( t ). If, for all t , Xt assumes values from a countably infinite set, then
we say that X is a discrete space process. If Xt assumes values from a finite set then the
process is finite. If T is a countably infinite set we say that X is a discrete time process.
In this chapter we focus on a special type of discrete time, discrete space stochastic
process X 0 , X 1 , X 2 ,...in which the value of Xt depends on the value of Xt − 1 but not on
the sequence of states that led the system to that value.

Definition 7.1: A discrete time stochastic process X 0 , X 1 , X 2 ,... is a Markov chain if^1

Pr( Xt = at | Xt − 1 = at − 1 , Xt − 2 = at − 2 ,..., X 0 = a 0 )=Pr( Xt = at | Xt − 1 = at − 1 )
= Pat − 1 , at.
(^1) Strictly speaking, this is a time-homogeneous Markov chain; this will be the only type we study in this book.

7.1markov chains: definitions and representations
This definition expresses that the state Xt depends on the previous state Xt − 1 but is
independent of the particular history of how the process arrived at state Xt − 1. This is
called the Markov property or memoryless property, and it is what we mean when we
say that a chain is Markovian. It is important to note that the Markov property does not
imply that Xt is independent of the random variables X 0 , X 1 ,..., Xt − 2 ; it just implies
that any dependency of Xt on the past is captured in the value of Xt − 1.
Without loss of generality, we can assume that the discrete state space of the Mar-
kov chain is{ 0 , 1 , 2 ,..., n }(or{ 0 , 1 , 2 ,...}if it is countably infinite). The transition
probability

Pi , j =Pr( Xt = j | Xt − 1 = i )
is the probability that the process moves from i to j in one step. The Markov property
implies that the Markov chain is uniquely defined by the one-step transition matrix :

P =
⎛
⎜⎜
⎜⎜
⎜⎜
⎜
⎝
P 0 , 0 P 0 , 1 ··· P 0 , j ···
P 1 , 0 P 1 , 1 ··· P 1 , j ···
..
.
..
.
... ..
.
...
Pi , 0 Pi , 1 ··· Pi , j ···
..
.
..
. ...
..
. ...
⎞
⎟⎟
⎟⎟
⎟⎟
⎟
⎠
.
That is, the entry in the i th row and j th column is the transition probability Pi , j .It
follows that, for all i ,

∑
j ≥ 0 Pi , j =1.
This transition matrix representation of a Markov chain is convenient for computing
the distribution of future states of the process. Let pi ( t ) denote the probability that the
process is at state i at time t. Let ̄ p ( t )=( p 0 ( t ), p 1 ( t ), p 2 ( t ),...) be the vector giving
the distribution of the state of the chain at time t. Summing over all possible states at
time t −1, we have

pi ( t )=
∑
j ≥ 0
pj ( t −1) Pj , i
or^2

p ̄( t )= p ̄( t −1) P.
We represent the probability distribution as a row vector and multiply ̄ p P instead
of P p ̄to conform with the interpretation that starting with a distribution ̄ p ( t −1) and
applying the operand P , we arrive at the distribution ̄ p ( t ).
For any m ≥0, we define the m -step transition probability
Pim , j =Pr( Xt + m = j | Xt = i )

as the probability that the chain moves from state i to state j in exactly m steps.
Conditioning on the first transition from i ,wehave

Pim , j =
∑
k ≥ 0
Pi , kPkm ,− j^1. (7.1)
(^2) Operations on vectors are generalized to a countable number of elements in the natural way.

markov chains and random walks
Figure 7.1: A Markov chain ( left ) and the corresponding transition matrix ( right ).
Let P ( m )be the matrix whose entries are the m -step transition probabilities, so that the
entry in the i th row and j th column is Pim , j. Then, applying Eqn. (7.1) yields

P ( m )= P · P ( m −1);
by induction on m ,

P ( m )= Pm.
Thus, for any t ≥0 and m ≥1,

p ̄( t + m )= p ̄( t ) Pm.
Another useful representation of a Markov chain is by a directed, weighted graph
D =( V , E ,w). The set of vertices of the graph is the set of states of the chain. There
is a directed edge ( i , j )∈ E if and only if Pi , j >0, in which case the weightw( i , j )of
the edge ( i , j ) is given byw( i , j )= Pi , j. Self-loops, where an edge starts and ends at
the same vertex, are allowed. Again, for each i we require that

∑
j :( i , j )∈ E w( i , j )=1. A
sequence of states visited by the process is represented by a directed path on the graph.
The probability that the process follows this path is the product of the weights of the
path’s edges.
Figure7.1gives an example of a Markov chain and the correspondence between the
two representations. Let us consider how we might calculate with each representation
the probability of going from state 0 to state 3 in exactly three steps. With the graph,
we consider all the paths that go from state 0 to state 3 in exactly three steps. There are
only four such paths: 0–1–0–3, 0–1–3–3, 0–3–1–3, and 0–3–3–3. The probabilities that
the process follows each of these paths are 3/32, 1/96, 1/16, and 3/64, respectively.
Summing these probabilities, we find that the total probability is 41/192. Alternatively,
we can simply compute

P^3 =
⎡
⎢
⎢⎢
⎣
3 /16 7/48 29/64 41/ 192
5 /48 5/24 79/144 5/ 36
00 1 0
1 /16 13/96 107/192 47/ 192
⎤
⎥
⎥⎥
⎦
.
7.1markov chains: definitions and representations
2-SAT Algorithm:
1. Start with an arbitrary truth assignment.
2. Repeat up to 2 mn^2 times, terminating if all clauses are satisfied:
(a) Choose an arbitrary clause that is not satisfied.
(b) Choose uniformly at random one of the literals in the clause and switch the
value of its variable.
3. If a valid truth assignment has been found, return it.
4. Otherwise, return that the formula is unsatisfiable.

Algorithm 7.1: 2-SAT algorithm.
The entry P 03 , 3 = 41 /192 gives the correct answer. The matrix is also helpful if we want
to know the probability of ending in state 3 after three steps when we begin in a state
chosen uniformly at random from the four states. This can be computed by calculating

(1/ 4 , 1 / 4 , 1 / 4 , 1 /4) P^3 =(17/ 192 , 47 / 384 , 737 / 1152 , 43 /288);
here the last entry, 43/288, is the required answer.

7.1.1 Application: A Randomized Algorithm for 2-Satisfiability
Recall from Section6.2.2that an input to the general satisfiability (SAT) problem is a
Boolean formula given as the conjunction (AND) of a set of clauses, where each clause
is the disjunction (OR) of literals and where a literal is a Boolean variable or the nega-
tion of a Boolean variable. A solution to an instance of a SAT formula is an assignment
of the variables to the values True (T) and False (F) such that all the clauses are satisfied.
The general SAT problem is NP-hard. We analyze here a simple randomized algorithm
for 2-SAT, a restricted case of the problem that is solvable in polynomial time.
For the k -satisfiability ( k -SAT) problem, the satisfiability formula is restricted so that
each clause has exactly k literals. Hence an input for 2-SAT has exactly two literals per
clause. The following expression is an instance of 2-SAT:

( x 1 ∨ x 2 )∧( x 1 ∨ x 3 )∧( x 1 ∨ x 2 )∧( x 4 ∨ x 3 )∧( x 4 ∨ x 1 ). (7.2)
One natural approach to finding a solution for a 2-SAT formula is to start with an
assignment, look for a clause that is not satisfied, and change the assignment so that
the clause becomes satisfied. If there are two literals in the clause, then there are two
possible changes to the assignment that will satisfy the clause. Our 2-SAT algorithm
(Algorithm7.1) decides which of these changes to try randomly. In the algorithm, n
denotes the number of variables in the formula and m is an integer parameter that deter-
mines the probability that the algorithm terminates with a correct answer.
In the instance given in (7.2), if we begin with all variables set to False then the
clause ( x 1 ∨ x 2 ) is not satisfied. The algorithm might therefore choose this clause and
then select x 1 to be set to True. In this case the clause ( x 4 ∨ x 1 ) would be unsatisfied
and the algorithm might switch the value of a variable in that clause, and so on.

markov chains and random walks
If the algorithm terminates with a truth assignment, it clearly returns a correct
answer. The case where the algorithm does not find a truth assignment requires some
care, and we will return to this point later. Assume for now that the formula is satisfi-
able and that the algorithm will actually run as long as necessary to find a satisfying
truth assignment.
We are mainly interested in the number of iterations of the while-loop executed by
the algorithm. We refer to each time the algorithm changes a truth assignment as a
step. Since a 2-SAT formula has O ( n^2 ) distinct clauses, each step can be executed in
O ( n^2 ) time. Faster implementations are possible but we do not consider them here. Let
S represent a satisfying assignment for the n variables and let Ai represent the variable
assignment after the i th step of the algorithm. Let Xi denote the number of variables
in the current assignment Ai that have the same value as in the satisfying assignment
S. When Xi = n , the algorithm terminates with a satisfying assignment. In fact, the
algorithm could terminate before Xi reaches n if it finds another satisfying assignment,
but for our analysis the worst case is that the algorithm only stops when Xi = n. Starting
with Xi < n , we consider how Xi evolves over time, and in particular how long it takes
before Xi reaches n.
First, if Xi =0 then, for any change in variable value on the next step, we have
Xi + 1 =1. Hence

Pr( Xi + 1 = 1 | Xi =0)= 1.
Suppose now that 1≤ Xi ≤ n −1. At each step, we choose a clause that is unsatis-
fied. Since S satisfies the clause, that means that Ai and S disagree on the value of at least
one of the variables in this clause. Because the clause has no more than two variables,
the probability that we increase the number of matches is at least 1/2; the probability
that we increase the number of matches could be 1 if we are in the case where Ai and
S disagree on the value of both variables in this clause. It follows that the probability
that we decrease the number of matches is at most 1/2. Hence, for 1≤ j ≤ n −1,

Pr( Xi + 1 = j + 1 | Xi = j )≥ 1 / 2 ;
Pr( Xi + 1 = j − 1 | Xi = j )≤ 1 / 2.
The stochastic process X 0 , X 1 , X 2 ,...is not necessarily a Markov chain, since the
probability that Xi increases could depend on whether Ai and S disagree on one or
two variables in the unsatisfied clause the algorithm chooses at that step. This, in turn,
might depend on the clauses that have been considered in the past. However, consider
the following Markov chain Y 0 , Y 1 , Y 2 ,...:

Y 0 = X 0 ;
Pr( Yi + 1 = 1 | Yi =0)= 1 ;
Pr( Yi + 1 = j + 1 | Yi = j )= 1 / 2 ;
Pr( Yi + 1 = j − 1 | Yi = j )= 1 / 2.
The Markov chain Y 0 , Y 1 , Y 2 ,...is a pessimistic version of the stochastic process
X 0 , X 1 , X 2 ,...in that, whereas Xi increases at the next step with probability at least

7.1markov chains: definitions and representations
1 /2, Yi increases with probability exactly 1/2. It is therefore clear that the expected
time to reach n starting from any point is larger for the Markov chain Y than for the
process X , and we use this fact hereafter. (A stronger formal framework for such ideas
is developed in Chapter 12 .)
This Markov chain models a random walk on an undirected graph G. (We elaborate
further on random walks in Section7.4.) The vertices of G are the integers 0,..., n
and, for 1≤ i ≤ n −1, node i is connected to node i −1 and node i +1. Let hj be the
expected number of steps to reach n when starting from j. For the 2-SAT algorithm, hj
is an upper bound on the expected number of steps to fully match S when starting from
a truth assignment that matches S in j locations.
Clearly, hn =0 and h 0 = h 1 +1, since from h 0 we always move to h 1 in one step.
We use linearity of expectations to find an expression for other values of hj. Let Zj be a
random variable representing the number of steps to reach n from state j. Now consider
starting from state j , where 1≤ j ≤ n −1. With probability 1/2, the next state is j −1,
and in this case Zj = 1 + Zj − 1. With probability 1/2, the next step is j +1, and in this
case Zj = 1 + Zj + 1. Hence

E [ Zj ]= E
[
1
2
(1+ Zj − 1 )+
1
2
(1+ Zj + 1 )
]
.
But E [ Zj ]= hj and so, by applying the linearity of expectations, we obtain

hj =
hj − 1 + 1
2
+
hj + 1 + 1
2
=
hj − 1
2
+
hj + 1
2
+ 1.
We therefore have the following system of equations:

hn = 0 ;
hj =
hj − 1
2
+
hj + 1
2
+ 1 , 1 ≤ j ≤ n − 1 ;
h 0 = h 1 + 1.
We can show inductively that, for 0≤ j ≤ n −1,
hj = hj + 1 + 2 j + 1.
It is true when j =0, since h 1 = h 0 −1. For other values of j , we use the equation

hj =
hj − 1
2
+
hj + 1
2
+ 1
to obtain

hj + 1 = 2 hj − hj − 1 − 2
= 2 hj −( hj +2( j −1)+1)− 2
= hj − 2 j − 1 ,
using the induction hypothesis in the second line. We can conclude that

h 0 = h 1 + 1 = h 2 + 1 + 3 = ··· =
n ∑− 1
i = 0
(2 i +1)= n^2.
markov chains and random walks
An alternative approach for solving the system of equations for the hj is to guess and
verify the solution hj = n^2 − j^2. The system has n +1 linearly independent equations
and n +1 unknowns, and hence there is a unique solution for each value of n. Therefore,
if this solution satisfies the foregoing equations then it must be correct. We have hn =0.
For 1≤ j ≤ n −1, we check

hj =
n^2 −( j −1)^2
2
+
n^2 −( j +1)^2
2
+ 1
= n^2 − j^2
and

h 0 =( n^2 −1)+ 1
= n^2.
Thus we have proven the following fact.
Lemma 7.1: Assume that a 2 -SAT formula with n variables has a satisfying assign-
ment and that the 2-SAT algorithm is allowed to run until it finds a satisfying assign-
ment. Then the expected number of steps until the algorithm finds an assignment is at
most n^2_._

We now return to the issue of dealing with unsatisfiable formulas by forcing the algo-
rithm to stop after a fixed number of steps.

Theorem 7.2: The 2-SAT algorithm always returns a correct answer if the formula
is unsatisfiable. If the formula is satisfiable, then with probability at least 1 − 2 − mthe
algorithm returns a satisfying assignment. Otherwise, it incorrectly returns that the
formula is unsatisfiable.

Proof: It is clear that if there is no satisfying assignment then the algorithm correctly
returns that the formula is unsatisfiable. Suppose the formula is satisfiable. Divide the
execution of the algorithm into segments of 2 n^2 steps each. Given that no satisfying
assignment was found in the first i −1 segments, what is the conditional probability that
the algorithm did not find a satisfying assignment in the i th segment? By Lemma7.1,
the expected time to find a satisfying assignment, regardless of its starting position,
is bounded by n^2. Let Z be the number of steps from the start of segment i until the
algorithm finds a satisfying assignment. Applying Markov’s inequality,

Pr( Z > 2 n^2 )≤
n^2
2 n^2
=
1
2
Thus the probability that the algorithm fails to find a satisfying assignment after m
segments is bounded above by (1/2) m.  /theta

7.1.2 Application: A Randomized Algorithm for 3-Satisfiability
We now generalize the technique used to develop an algorithm for 2-SAT to obtain a
randomized algorithm for 3-SAT. This problem is NP-complete, so it would be rather

7.1markov chains: definitions and representations
3-SAT Algorithm:
1. Start with an arbitrary truth assignment.
2. Repeat up to m times, terminating if all clauses are satisfied:
(a) Choose an arbitrary clause that is not satisfied.
(b) Choose one of the literals uniformly at random, and change the value of the
variable in the current truth assignment.
3. If a valid truth assignment has been found, return it.
4. Otherwise, return that the formula is unsatisfiable.

Algorithm 7.2: 3-SAT algorithm.
surprising if a randomized algorithm could solve the problem in expected time poly-
nomial in n.^3 We present a randomized 3-SAT algorithm that solves 3-SAT in expected
time that is exponential in n , but it is much more efficient than the naïve approach of
trying all possible truth assignments for the variables.
Let us first consider the performance of a variant of the randomized 2-SAT algorithm
when applied to a 3-SAT problem. The basic approach is the same as in the previous
section; see Algorithm7.2. In the algorithm, m is a parameter that controls the prob-
ability of success of the algorithm. We focus on bounding the expected time to reach
a satisfying assignment (assuming one exists), as the argument of Theorem7.2can be
extended once such a bound is found.
As in the analysis of the 2-SAT algorithm, assume that the formula is satisfiable
and let S be a satisfying assignment. Let the assignment after i steps of the process be
Ai , and let Xi be the number of variables in the current assignment Ai that match S .It
follows from the same reasoning as for the 2-SAT algorithm that, for 1≤ j ≤ n −1,
Pr( Xi + 1 = j + 1 | Xi = j )≥ 1 / 3 ;
Pr( Xi + 1 = j − 1 | Xi = j )≤ 2 / 3.
These equations follow because at each step we choose an unsatisfied clause, so Ai and
S must disagree on at least one variable in this clause. With probability at least 1/3, we
increase the number of matches between the current truth assignment and S. Again we
can obtain an upper bound on the expected number of steps until Xi = n by analyzing
a Markov chain Y 0 , Y 1 ,...such that Y 0 = X 0 and
Pr( Yi + 1 = 1 | Yi =0)= 1 ,
Pr( Yi + 1 = j + 1 | Yi = j )= 1 / 3 ,
Pr( Yi + 1 = j − 1 | Yi = j )= 2 / 3.
In this case, the chain is more likely to go down than up. If we let hj be the expected
number of steps to reach n when starting from j , then the following equations hold
(^3) Technically, this would not settle the P=NP question, since we would be using a randomized algorithm and not
a deterministic algorithm to solve an NP-hard problem. It would, however, have similar far-reaching implications
about the ability to solve all NP-complete problems.

markov chains and random walks
for hj :

hn = 0 ;
hj =
2 hj − 1
3
+
hj + 1
3
+ 1 , 1 ≤ j ≤ n − 1 ;
h 0 = h 1 + 1.
Again, these equations have a unique solution, which is given by

hj = 2 n +^2 − 2 j +^2 −3( n − j ).
Alternatively, the solution can be found by using induction to prove the relationship

hj = hj + 1 + 2 j +^2 − 3.
We leave it as an exercise to verify that this solution indeed satisfies the foregoing
equations.
The algorithm just described takes /eta(2 n ) steps on average to find a satisfying assign-
ment. This result is not very compelling, since there are only 2 n truth assignments to
try! With some insight, however, we can significantly improve the process. There are
two key observations.

1. If we choose an initial truth assignment uniformly at random, then the number of
variables that match S has a binomial distribution with expectation n /2. With an
exponentially small but nonnegligible probability, the process starts with an initial
assignment that matches S in significantly more than n /2 variables.
2. Once the algorithm starts, it is more likely to move toward 0 than toward n. The
longer we run the process, the more likely it has moved toward 0. Therefore, we are
better off restarting the process with many randomly chosen initial assignments and
running the process each time for a small number of steps, rather than running the
process for many steps on the same initial assignment.

Based on these ideas, we consider the modified procedure of Algorithm7.3. The mod-
ified algorithm has up to 3 n steps to reach a satisfying assignment starting from a ran-
dom assignment. If it fails to find a satisfying assignment in 3 n steps, it restarts the
search with a new randomly chosen assignment. We now determine how many times
the process needs to restart before it reaches a satisfying assignment.
Let q represent the probability that the modified process reaches S (or some other
satisfying assignment) in 3 n steps starting with a truth assignment chosen uniformly
at random. Let qj be a lower bound on the probability that our modified algorithm
reaches S (or some other satisfying assignment) when it starts with a truth assignment
that includes exactly j variables that do not agree with S. Consider a particle moving
on the integer line, with probability 1/3 of moving up by one and probability 2/3of
moving down by one. Notice that
(
j + 2 k
k

)(
2
3
) k (
1
3
) j + k
is the probability of exactly k moves down and k + j moves up in a sequence of j + 2 k
moves. It is therefore a lower bound on the probability that the algorithm reaches a

7.1markov chains: definitions and representations
Modified 3-SAT Algorithm:
1. Repeat up to m times, terminating if all clauses are satisfied:
(a) Start with a truth assignment chosen uniformly at random.
(b) Repeat the following up to 3 n times, terminating if a satisfying assignment
is found:
i. Choose an arbitrary clause that is not satisfied.
ii. Choose one of the literals uniformly at random, and change the value of
the variable in the current truth assignment.
2. If a valid truth assignment has been found, return it.
3. Otherwise, return that the formula is unsatisfiable.

Algorithm 7.3: Modified 3-SAT algorithm.
satisfying assignment within j + 2 k ≤ 3 n steps, starting with an assignment that has
exactly j variables that did not agree with S. That is,

qj ≥ max
k = 0 ,..., j
(
j + 2 k
k
)(
2
3
) k (
1
3
) j + k
.
In particular, consider the case where k = j. In that case we have
qj ≥
(
3 j
j
)(
2
3
) j (
1
3
) 2 j
.
In order to approximate

( 3 j
j
)
we use Stirling’s formula, which is similar to the bound of
Eqn. (5.5) we have previously proven for factorials. Stirling’s formula is tighter, which
proves useful for this application. We use the following loose form.

Lemma 7.3 [ Stirling’s Formula ] : For m > 0 ,

m !=
√
2 π m
(
m
e
) m
(1± o (1)).
In particular, for m > 0 ,

√
2 π m
(
m
e
) m
≤ m !≤ 2
√
2 π m
(
m
e
) m
.
Hence, when j >0,
(
3 j
j

)
=
(3 j )!
j !(2 j )!
≥
√
2 π(3 j )
4
√
2 π j
√
2 π(2 j )
(
3 j
e
) 3 j (
e
2 j
) 2 j (
e
j
) j
=
√
3
8
√
π j
(
27
4
) j
=
c
√
j
(
27
4
) j
markov chains and random walks
for a constant c =

√
3 / 8
√
π. Thus, when j >0,
qj ≥
(
3 j
j
)(
2
3
) j (
1
3
) 2 j
≥
c
√
j
(
27
4
) j (
2
3
) j (
1
3
) 2 j
≥
c
√
j
1
2 j
Also, q 0 =1.
Having established a lower bound for qj , we can now derive a lower bound for q , the
probability that the process reaches a satisfying assignment in 3 n steps when starting
with a random assignment:

q ≥
∑ n
j = 0
Pr(a random assignment has j mismatches with S )· qj
≥
1
2 n
+
∑ n
j = 1
(
n
j
)(
1
2
) n
c
√
j
1
2 j
≥
c
√
n
(
1
2
) n ∑ n
j = 0
(
n
j
)(
1
2
) j
(1) n − j (7.3)
=
c
√
n
(
1
2
) n (
3
2
) n
=
c
√
n
(
3
4
) n
,
where in (7.3) we used

∑ n
j = 0
( n
j
)( 1
2
) j
(1) n − j =
(
1 +^12
) n
Assuming that a satisfying assignment exists, the number of random assignments the
process tries before finding a satisfying assignment is a geometric random variable with
parameter q. The expected number of assignments tried is 1/ q , and for each assignment
the algorithm uses at most 3 n steps. Thus, the expected number of steps until a solution
is found is bounded by O ( n^3 /^2 (4/3) n ). As in the case of 2-SAT (Theorem7.2), the
modified 3-SAT algorithm (Algorithm7.3) yields a Monte Carlo algorithm for the 3-
SAT problem. If the expected number of steps until a satisfying solution is found is
bounded above by a and if m is set to 2 ab , then the probability that no assignment is
found when the formula is satisfiable is bounded above by 2− b.

7.2 Classification of States
A first step in analyzing the long-term behavior of a Markov chain is to classify its
states. In the case of a finite Markov chain, this is equivalent to analyzing the connec-
tivity structure of the directed graph representing the Markov chain.

7.2 classification of states
Definition 7.2: State j is accessible from state i if, for some integer n ≥ 0 ,Pin , j > 0 .If
two states i and j are accessible from each other, we say that they communicate and we
write i ↔ j.

In the graph representation of a chain, i ↔ j if and only if there are directed paths
connecting i to j and j to i.
The communicating relation defines an equivalence relation. That is, the communi-
cating relation is

1. reflexive – for any state i , i ↔ i ;
2. symmetric –if i ↔ j then j ↔ i ; and
3. transitive –if i ↔ j and j ↔ k , then i ↔ k.

Proving this is left as Exercise7.4. Thus, the communication relation partitions the
states into disjoint equivalence classes, which we refer to as communicating classes. It
might be possible to move from one class to another, but in that case it is impossible to
return to the first class.

Definition 7.3: A Markov chain is irreducible if all states belong to one communicat-
ing class.

In other words, a Markov chain is irreducible if, for every pair of states, there is a
nonzero probability that the first state can reach the second. We thus have the following
lemma.

Lemma 7.4: A finite Markov chain is irreducible if and only if its graph representation
is a strongly connected graph.

Next we distinguish between transient and recurrent states. Let rit , j denote the proba-
bility that, starting at state i , the first transition to state j occurs at time t ; that is,

rit , j =Pr( Xt = j and, for 1≤ s ≤ t − 1 , Xs = j | X 0 = i ).
Definition 7.4: A state is recurrent if

∑
t ≥ 1 rit , i =^1 , and it is transient if
∑
t ≥ 1 rit , i <^1_.
A Markov chain is recurrent if every state in the chain is recurrent._

If state i is recurrent then, once the chain visits that state, it will (with probability 1)
eventually return to that state. Hence the chain will visit state i over and over again,
infinitely often. On the other hand, if state i is transient then, starting at i , the chain
will return to i with some fixed probability p =

∑
t ≥ 1 r
t
i , i. In this case, the number of
times the chain visits i when starting at i is given by a geometric random variable. If
one state in a communicating class is transient (respectively, recurrent) then all states
in that class are transient (respectively, recurrent); proving this is left as Exercise7.5.
We denote the expected time to return to state i when starting at state i by
hi , i =

∑
t ≥ 1 t · r
t
∑ i , i. Similarly, for any pair of states i and j , we denote by hi , j =
t ≥ 1 t · r

t
i , j the expected time to first reach j from state i. It may seem that if a chain is
recurrent, so that we visit a state i infinitely often, then hi , i should be finite. This is not
the case, which leads us to the following definition.

markov chains and random walks
Definition 7.5: A recurrent state i is positive recurrent if hi , i <∞. Otherwise, it is
null recurrent_._

To give an example of a Markov chain that has null recurrent states, consider a chain
whose states are the positive integers. From state i , the probability of going to state
i +1is i /( i +1). With probability 1/( i +1), the chain returns to state 1. Starting at
state 1, the probability of not having returned to state 1 within the first t steps is thus

∏ t
j = 1
j
j + 1
=
1
t + 1
.
Hence the probability of never returning to state 1 from state 1 is 0, and state 1 is
recurrent. It follows that

r 1 t , 1 =
1
t ( t +1)
.
However, the expected number of steps until the first return to state 1 from state 1 is

h 1 , 1 =
∑∞
t = 1
t · r 1 t , 1 =
∑∞
t = 1
1
t + 1
,
which is unbounded.
In the foregoing example the Markov chain had an infinite number of states. This is
necessary for null recurrent states to exist. The proof of the following important lemma
is left as Exercise7.16.

Lemma 7.5: In a finite Markov chain:

1. at least one state is recurrent; and
2. all recurrent states are positive recurrent.

Finally, for our later study of limiting distributions of Markov chains we will need to
define what it means for a state to be aperiodic. As an example of periodicity, consider
a random walk whose states are the positive integers. When at state i , with probability
1 /2 the chain moves to i +1 and with probability 1/2 the chain moves to i −1. If
the chain starts at state 0, then it can be at an even-numbered state only after an even
number of moves, and it can be at an odd-numbered state only after an odd number of
moves. This is an example of periodic behavior.

Definition 7.6: A state j in a discrete time Markov chain is periodic if there exists an
integer > 1 such that Pr( Xt + s = j | Xt = j )= 0 unless s is divisible by . A discrete
time Markov chain is periodic if any state in the chain is periodic. A state or chain that
is not periodic is aperiodic_._

In our example, every state in the Markov chain is periodic because, for every state j ,
Pr( Xt + s = j | Xt = j )=0 unless s is divisible by 2.
We end this section with an important corollary about the behavior of finite Markov
chains.

7.2 classification of states
Definition 7.7: An aperiodic, positive recurrent state is an ergodic state. A Markov
chain is ergodic if all its states are ergodic.

Corollary 7.6: Any finite, irreducible, and aperiodic Markov chain is an ergodic
chain.

Proof: A finite chain has at least one recurrent state by Lemma7.5, and if the chain
is irreducible then all of its states are recurrent. In a finite chain, all recurrent states
are positive recurrent by Lemma7.5and thus all the states of the chain are positive
recurrent and aperiodic. The chain is therefore ergodic.  /theta

7.2.1 Example: The Gambler’s Ruin
When a Markov chain has more than one class of recurrent states, we are often inter-
ested in the probability that the process will enter and thus be absorbed by a given
communicating class.
For example, consider a sequence of independent, fair gambling games between
two players. In each round a player wins a dollar with probability 1/2 or loses a dollar
with probability 1/2. The state of the system at time t is the number of dollars won by
player 1. If player 1 has lost money, this number is negative. The initial state is 0.
It is reasonable to assume that there are numbers n 1 and n 2 such that player i cannot
lose more than n i dollars, and thus the game ends when it reaches one of the two states
− n 1 or n 2. At this point, one of the gamblers is ruined; that is, he has lost all his money.
To conform with the formalization of a Markov chain, we assume that for each of these
two end states there is only one transition out and that it goes back to the same state.
This gives us a Markov chain with two absorbing, recurrent states.
What is the probability that player 1 wins n 2 dollars before losing n 1 dollars? If
 n 2 = n 1 , then by symmetry this probability must be 1/2. We provide a simple argument
for the general case using the classification of the states.
Clearly− n 1 and n 2 are recurrent states. All other states are transient, since there is a
nonzero probability of moving from each of these states to either state− n 1 or state n 2.
Let Pit be the probability that, after t steps, the chain is at state i .For− n 1 < i < n 2 ,
state i is transient and so lim t →∞ Pit =0.
Let q be the probability that the game ends with player 1 winning n 2 dollars, so
that the chain was absorbed into state n 2. Then 1− q is the probability the chain was
absorbed into state− n 1. By definition,

t lim→∞ P n t^2 = q.
Since each round of the gambling game is fair, the expected gain of player 1 in each
step is 0. Let Wt be the gain of player 1 after t steps. Then E [ Wt ]=0 for any t by
induction. Thus,

E [ Wt ]=
∑ n^2
i =− n 1
iPit = 0
markov chains and random walks
and

lim
t →∞
E [ Wt ]= n 2 q − n 1 (1− q )
= 0.
Thus,

q =
 n 1
 n 1 + n 2
That is, the probability of winning (or losing) is proportional to the amount of money
a player is willing to lose (or win).
Another approach that yields the same answer is to let qj represent the probabil-
ity that player 1 wins n 2 dollars before losing n 1 dollars when having won j dollars
for− n 1 ≤ j ≤ n 2. Clearly, q − n 1 =0 and q n 2 =1. For− n 1 < j < n 2 , we compute by
considering the outcome of the first game:

qj =
qj − 1
2
+
qj + 1
2
We have n 2 + n 1 −2 linearly independent equations and n 2 + n 1 −2 unknowns, so
there is a unique solution to this set of equations. It is easy to verify that qj =( n 1 + j )/
( n 1 + n 2 ) satisfies the given equations.
In Exercise7.20, we consider the question of what happens if, as is generally the
case in real life, one player is at a disadvantage and so is slightly more likely to lose
than to win any single game.

7.3 Stationary Distributions
Recall that if P is the one-step transition probability matrix of a Markov chain and if
p ̄( t ) is the probability distribution of the state of the chain at time t , then
p ̄( t +1)= p ̄( t ) P.

Of particular interest are state probability distributions that do not change after a tran-
sition.

Definition 7.8: A stationary distribution (also called an equilibrium distribution )ofa
Markov chain is a probability distribution π ̄ such that

π ̄=π ̄ P.
If a chain ever reaches a stationary distribution then it maintains that distribution for all
future time, and thus a stationary distribution represents a steady state or an equilibrium
in the chain’s behavior. Stationary distributions play a key role in analyzing Markov
chains. The fundamental theorem of Markov chains characterizes chains that converge
to stationary distributions.
We discuss first the case of finite chains and then extend the results to any discrete
space chain. Without loss of generality, assume that the finite set of states of the Markov
chain is{ 0 , 1 ,..., n }.

7.3stationary distributions
Theorem 7.7: Any finite, irreducible, and ergodic Markov chain has the following
properties:

1. the chain has a unique stationary distribution π ̄=(π 0 ,π 1 ,...,π n ) ;
2. for all j and i, the limit lim t →∞ Ptj , iexists and it is independent of j;
3. π i =lim t →∞ Ptj , i = 1 / hi , i.

Under the conditions of this theorem, the stationary distribution ̄πhas two interpre-
tations. First,π i is the limiting probability that the Markov chain will be in state i
infinitely far out in the future, and this probability is independent of the initial state. In
other words, if we run the chain long enough, the initial state of the chain is almost for-
gotten and the probability of being in state i converges toπ i. Second,π i is the inverse of
hi , i =

∑∞
t = 1 t · r
t
i , i , the expected number of steps for a chain starting in state i to return
to i. This stands to reason; if the average time to return to state i from i is hi , i , then
we expect to be in state i for 1/ hi , i of the time and thus, in the limit, we must have
π i = 1 / hi , i.

Proof of Theorem7.7: We prove the theorem using the following result, which we
state without proof.  /theta

Lemma 7.8: For any irreducible, ergodic Markov chain and for any state i, the limit
lim t →∞ Pit , iexists and

t lim→∞ Pit , i =
1
hi , i
.
This lemma is a corollary of a basic result in renewal theory. We give an informal
justification for Lemma7.8: the expected time between visits to i is hi , i , and therefore
state i is visited 1/ hi , i of the time. Thus lim t →∞ Pit , i , which represents the probability a
state chosen far in the future is at state i when the chain starts at state i , must be 1/ hi , i.
Using the fact that lim t →∞ Pit , i exists, we now show that, for any j and i ,

t lim→∞ Ptj , i = t lim→∞ Pit , i =
1
hi , i
;
that is, these limits exist and are independent of the starting state j.
Recall that rtj , i is the probability that starting at j , the chain first visits i at time t.
Since the chain is irreducible we have that

∑∞
t = 1 rjt , i =1, and for anyε>0 there exists
(a finite) t 1 = t 1 (ε) such that

∑ t 1
t = 1 rtj , i ≥^1 −ε.
For j = i ,wehave
Ptj , i =
∑ t
k = 1
rkj , iPit ,− ik.
For t ≥ t 1 ,

∑ t^1
k = 1
rkj , iPit ,− ik ≤
∑ t
k = 1
rkj , iPti ,− ik = Ptj , i.
markov chains and random walks
Using the facts that lim t →∞ Pit , i exists and t 1 is finite, we have

lim
t →∞
Ptj , i ≥lim
t →∞
∑ t^1
k = 1
rkj , iPit ,− ik
=
∑ t^1
k = 1
rkj , i lim
t →∞
Pit , i
= t lim→∞ Pit , i
∑ t^1
k = 1
rkj , i
≥(1−ε) lim t →∞ Pit , i.
Similarly,

Ptj , i =
∑ t
k = 1
rkj , iPit ,− ik
≤
∑ t^1
k = 1
rkj , iPit ,− ik +ε,
from which we can deduce that

t lim→∞ Ptj , i ≤ t lim→∞
( t 1
∑
k = 1
rkj , iPti ,− ik +ε
)
=
∑ t^1
k = 1
rkj , it lim→∞ Pit ,− ik +ε
≤ t lim→∞ Pit , i +ε.
Lettingεapproach 0, we have proven that, for any pair i and j ,

t lim→∞ Ptj , i = t lim→∞ Pit , i =
1
hi , i
.
Now let
π i = t lim→∞ Ptj , i =
1
hi , i
.
We show that ̄π=(π 0 ,π 1 ,...) forms a stationary distribution.
For every t ≥0, we have Pit , i ≥0 and thusπ i ≥0. For any t ≥0,

∑ n
i = 0 P
t
j , i =^1
and thus

t lim→∞
∑ n
i = 0
Ptj , i =
∑ n
i = 0
t lim→∞ Ptj , i =
∑ n
i = 0
π i = 1 ,
and ̄πis a proper distribution. Now,

Ptj +, i^1 =
∑ n
k = 0
Ptj , kPk , i.
7.3stationary distributions
Letting t →∞,wehave

π i =
∑ n
k = 0
π kPk , i ,
proving that ̄πis a stationary distribution.
Suppose there were another stationary distributionφ ̄. Then by the same argument
we would have

φ i =
∑ n
k = 0
φ kPkt , i ,
and taking the limit as t →∞yields

φ i =
∑ n
k = 0
φ k π i =π i
∑ n
k = 0
φ k.
Since

∑ n
k = 0 φ k =1 it follows thatφ i =π i for all i ,orφ ̄=π ̄.  /theta
It is worth making a few remarks about Theorem7.7. First, the requirement that the
Markov chain be aperiodic is not necessary for the existence of a stationary distribution.
In fact, any finite Markov chain has a stationary distribution; but in the case of a periodic
state i , the stationary probabilityπ i is not the limiting probability of being in i but
instead just the long-term frequency of visiting state i. Second, any finite chain has at
least one component that is recurrent. Once the chain reaches a recurrent component,
it cannot leave that component. Thus, the subchain that corresponds to that component
is irreducible and recurrent, and the limit theorem applies to any aperiodic recurrent
component of the chain.
One way to compute the stationary distribution of a finite Markov chain is to solve
the system of linear equations

π ̄ P =π. ̄
This is particularly useful if one is given a specific chain. For example, given the tran-
sition matrix

P =
⎡
⎢⎢
⎢
⎣
01 / 403 / 4
1 / 201 / 31 / 6
1 / 41 / 41 / 20
01 / 21 / 41 / 4
⎤
⎥⎥
⎥
⎦
,
we have five equations for the four unknowns∑ π 0 ,π 1 ,π 2 , andπ 3 given by ̄π P =π ̄and
3
i = 0 π i =1. The equations have a unique solution.
Another useful technique is to study the cut-sets of the Markov chain. For any state
i of the chain,

∑ n
j = 0
π jPj , i =π i =π i
∑ n
j = 0
Pi , j
markov chains and random walks
Figure 7.2: A simple Markov chain used to represent bursty behavior.
or
∑

j = i
π jPj , i =
∑
j = i
π iPi , j.
That is, in the stationary distribution the probability that a chain leaves a state equals
the probability that it enters the state. This observation can be generalized to sets of
states as follows.

Theorem 7.9: Let S be a set of states of a finite, irreducible, aperiodic Markov chain.
In the stationary distribution, the probability that the chain leaves the set S equals the
probability that it enters S.

In other words, if C is a cut-set in the graph representation of the chain, then in the
stationary distribution the probability of crossing the cut-set in one direction is equal
to the probability of crossing the cut-set in the other direction.
A basic but useful Markov chain that serves as an example of cut-sets is given in
Figure7.2. The chain has only two states. From state 0, you move to state 1 with prob-
ability p and stay at state 0 with probability 1− p. Similarly, from state 1 you move
to state 0 with probability q and remain in state 1 with probability 1− q. This Markov
chain is often used to represent bursty behavior. For example, when bits are corrupted in
transmissions they are often corrupted in large blocks, since the errors are often caused
by an external phenomenon of some duration. In this setting, being in state 0 after t
steps represents that the t th bit was sent successfully, while being in state 1 represents
that the bit was corrupted. Blocks of successfully sent bits and corrupted bits both have
lengths that follow a geometric distribution. When p and q are small, state changes are
rare, and the bursty behavior is modeled.
The transition matrix is

P =
[
1 − pp
q 1 − q
]
.
Solving ̄π P =π ̄corresponds to solving the following system of three equations:

π 0 (1− p )+π 1 q =π 0 ;
π 0 p +π 1 (1− q )=π 1 ;
π 0 +π 1 = 1.
7.3stationary distributions
The second equation is redundant, and the solution isπ 0 = q /( p + q ) andπ 1 =
p /( p + q ). For example, with the natural parameters p = 0 .005 and q = 0 .1, in the
stationary distribution more than 95% of the bits are received uncorrupted.
Using the cut-set formulation, we have that in the stationary distribution the proba-
bility of leaving state 0 must equal the probability of entering state 0, or

π 0 p =π 1 q.
Again, now usingπ 0 +π 1 =1 yieldsπ 0 = q /( p + q ) andπ 1 = p /( p + q ).
Finally, for some Markov chains the stationary distribution is easy to compute by
means of the following theorem.

Theorem 7.10: Consider a finite, irreducible, and ergodic Markov chain with
transition matrix ∑ P. If there are nonnegative numbers π ̄=(π 0 ,...,π n ) such that
n
i = 0 π i =^1 and if, for any pair of states i , j,
π iPi , j =π jPj , i ,

then π ̄ is the stationary distribution corresponding to P.

Proof: Consider the j th entry of ̄π P. Using the assumption of the theorem, we find
that it equals

∑ n
i = 0
π iPi , j =
∑ n
i = 0
π jPj , i =π j.
Thus ̄πsatisfies ̄π=π ̄ P. Since

∑ n
i = 0 π i =1, it follows from Theorem7.7that ̄πmust
be the unique stationary distribution of the Markov chain.  /theta

Chains that satisfy the condition

π iPi , j =π jPj , i
are called time reversible ; Exercise7.13helps explain why. You may check that the
chain of Figure7.2is time reversible.
We turn now to the convergence of Markov chains with countably infinite state
spaces. Using essentially the same technique as in the proof of Theorem7.7, one can
prove the next result.

Theorem 7.11: Any irreducible aperiodic Markov chain belongs to one of the follow-
ing two categories:

1. the chain is ergodic – for any pair of states i and j, the limit lim t →∞ Ptj , iexists
and is independent of j, and the chain has a unique stationary distribution π i =
lim t →∞ Ptj , i > 0 ;or
2. no state is positive recurrent – for all i and j, lim t →∞ Ptj , i = 0 , and the chain has
no stationary distribution.

Cut-sets and the property of time reversibility can also be used to find the stationary
distribution for Markov chains with countably infinite state spaces.

markov chains and random walks
7.3.1 Example: A Simple Queue
A queue is a line where customers wait for service. We examine a model for a bounded
queue where time is divided into steps of equal length. At each time step, exactly one
of the following occurs.

 /thetaIf the queue has fewer than n customers, then with probabilityλa new customer joins
the queue.
 /thetaIf the queue is not empty, then with probabilityμthe head of the line is served and
leaves the queue.
 /thetaWith the remaining probability, the queue is unchanged.
If Xt is the number of customers in the queue at time t , then under the foregoing rules
the Xt yield a finite-state Markov chain. Its transition matrix has the following nonzero
entries:

Pi , i + 1 =λ if i < n ;
Pi , i − 1 =μ if i > 0 ;
Pi , i =
⎧
⎪⎨
⎪⎩
1 −λ if i = 0 ,
1 −λ−μ if 1≤ i ≤ n − 1 ,
1 −μ if i = n.
The Markov chain is irreducible, finite, and aperiodic, so it has a unique stationary
distribution ̄π. We use ̄π=π ̄ P to write

π 0 =(1−λ)π 0 +μπ 1 ,
π i =λπ i − 1 +(1−λ−μ)π i +μπ i + 1 , 1 ≤ i ≤ n − 1 ,
π n =λπ n − 1 +(1−μ)π n.
It is easy to verify that

π i =π 0
(
λ
μ
) i
is a solution to the preceding system of equations. Adding the requirement

∑ n
i = 0 π i =
1, we have

∑ n
i = 0
π i =
∑ n
i = 0
π 0
(
λ
μ
) i
= 1
or

π 0 =
1
∑ n
i = 0 (λ/μ) i
.
For all 0≤ i ≤ n ,

π i =
(λ/μ) i
∑ n
i = 0 (λ/μ) i
. (7.4)
7.4 Random Walks on Undirected Graphs
Another way to compute the stationary probability in this case is to use cut-sets. For
any i , the transitions i → i +1 and i + 1 → i constitute a cut-set of the graph represent-
ing the Markov chain. Thus, in the stationary distribution, the probability of moving
from state i to state i +1 must be equal to the probability of moving from state i + 1
to i ,or

λπ i =μπ i + 1.
A simple induction now yields

π i =π 0
(
λ
μ
) i
.
In the case where there is no upper limit n on the number of customers in a queue,
the Markov chain is no longer finite. The Markov chain has a countably infinite state
space. Applying Theorem7.11, the Markov chain has a stationary distribution if and
only if the following set of linear equations has a solution with allπ i >0:

π 0 =(1−λ)π 0 +μπ 1 ;
(7.5)
π i =λπ i − 1 +(1−λ−μ)π i +μπ i + 1 , i ≥ 1.
It is easy to verify that

π i =
(λ/μ) i
∑∞
i = 0 (λ/μ) i
=
(
λ
μ
) i (
1 −
λ
μ
)
is a solution of the system of equations (7.5). This naturally generalizes the solution
to the case where there is an upper bound n on the number of the customers in the
system given in Eqn. (7.4).Alloftheπ i are greater than 0 if and only ifλ<μ, which
corresponds to the situation when the rate at which customers arrive is lower than the
rate at which they are served. Ifλ>μ, then the rate at which customers arrive is higher
than the rate at which they depart. Hence there is no stationary distribution, and the
queue length will become arbitrarily long. In this case, each state in the Markov chain
is transient. The case ofλ=μis more subtle. Again, there is no stationary distribution
and the queue length will become arbitrarily long, but now the states are null recurrent.
(See the related Exercise7.17.)

7.4. Random Walks on Undirected Graphs
A random walk on an undirected graph is a special type of Markov chain that is often
used in analyzing algorithms. Let G =( V , E ) be a finite, undirected, and connected
graph.

Definition 7.9: A random walk on G is a Markov chain defined by the sequence of
moves of a particle between vertices of G. In this process, the place of the particle at
a given time step is the state of the system. If the particle is at vertex i and if i has d ( i )
outgoing edges, then the probability that the particle follows the edge ( i , j ) and moves
to a neighbor j is 1 / d ( i ).

markov chains and random walks
We have already seen an example of such a walk when we analyzed the randomized
2-SAT algorithm.
For a random walk on an undirected graph, we have a simple criterion for aperiod-
icity as follows.

Lemma 7.12: A random walk on an undirected graph G is aperiodic if and only if G
is not bipartite.

Proof: A graph is bipartite if and only if it does not have cycles with an odd number
of edges. In an undirected graph, there is always a path of length 2 from a vertex to
itself. If the graph is bipartite then the random walk is periodic with period d =2.
If the graph is not bipartite then it has an odd cycle, and by traversing that cycle we
have an odd-length path from any vertex to itself. It follows that the Markov chain is
aperiodic.  /theta

For the remainder of this section we assume that G is not bipartite. A random walk
on a finite, undirected, connected, and non-bipartite graph G satisfies the conditions of
Theorem7.7, and hence the random walk converges to a stationary distribution. We
show that this distribution depends only on the degree sequence of the graph.

Theorem 7.13: A random walk on G converges to a stationary distribution π ̄ , where

πv=
d (v)
2 | E |
.
Proof: Since

∑
v∈ Vd (v)=^2 | E |, it follows that
∑
v∈ V
πv=
∑
v∈ V
d (v)
2 | E |
= 1 ,
and ̄πis a proper distribution overv∈ V.
Let P be the transition probability matrix of the Markov chain. Let N (v) represent
the neighbors ofv. The relation ̄π=π ̄ P is equivalent to

πv=
∑
u ∈ N (v)
d ( u )
2 | E |
1
d ( u )
=
d (v)
2 | E |
,
and the theorem follows.  /theta

Recall that we have used hu ,vto denote the expected time to reach statevwhen
starting at state u. The value hu ,vis often referred to as the hitting time from u tov,
or just the hitting time where the meaning is clear. Another value related to the hitting
time is the commute time between u andv, given by hu ,v+ h v, u. Unlike the hitting time,
the commute time is symmetric; it represents the time to go from u tovand back to u ,
and this is the same as the time to go fromvto u and back tov. Finally, for random
walks on graphs, we are also interested in a quantity called the cover time.

Definition 7.10: The cover time of a graph G =( V , E ) is the maximum over all ver-
tices v∈ V of the expected time to visit all of the nodes in the graph by a random walk
starting from v_._

7.4random walks on undirected graphs
We consider here some basic bounds on the commute time and the cover time for
standard random walks on a finite, undirected, connected graph G =( V , E ).

Lemma 7.14: If ( u ,v)∈ E, the commute time hu ,v+ h v, uis at most 2 | E |.

Proof: Let D be a set of directed edges such that for every edge ( u ,v)∈ E we have
the two directed edges u →vandv→ u in D. We can view the random walk on G as
a Markov chain with state space D , where the state of the Markov chain at time t is the
directed edge taken by the random walk in its t th transition. The Markov chain has 2| E |
states and it is easy to verify that it has a uniform stationary distribution. (This is left
as Exercise7.29.) Since the stationary probability of being in state u →vis 1/ 2 | E |,
once the original random walk traverses the directed edge u →vthe expected time
to traverse that directed edge again is 2| E |. Because the random walk is memoryless,
once it reaches vertexvwe can “forget” that it reached it through the edge u →v,and
therefore the expected time starting atvto reach u and then traverse the edge u →v
back tovis bounded above by 2| E |. As this is only one of the possible ways to go from
vto u and back tov, we have shown that h v, u + hu ,v≤ 2 | E |.  /theta

Lemma 7.15: The cover time of G =( V , E ) is bounded above by 2 | E |(| V |−1).

Proof: Choose a spanning tree T of G ; that is, choose any subset of the edges that gives
an acyclic graph connecting all the vertices of G. Starting from any vertexv, there exists
a cyclic (Eulerian) tour on the spanning tree in which every edge is traversed once in
each direction; for example, such a tour can be found by considering the sequence of
vertices passed through by a depth first search. The maximum expected time to go
through the vertices in the tour, where the maximum is over the choice of starting
vertex, is an upper bound on the cover time. Letv 0 ,v 1 ,...,v 2 | V |− 2 be the sequence of
vertices in the tour starting fromv 0 =v. Then the expected time to go through all the
vertices in sequence order is

2 |∑ V |− 3
i = 0
h v i ,v i + 1 =
∑
( x , y )∈ T
( hx , y + hy , x )≤ 2 | E |(| V |−1).
In words, the commute time for every pair of adjacent vertices in the tree is bounded
above by 2| E |, and there are| V |−1 pairs of adjacent vertices.  /theta

The following result is known as Matthews’ theorem, which relates the cover time
of a graph to the hitting time. Recall that we use∑ H ( n ) to denote the harmonic number
n
i = 11 / i ≈ln n.

Lemma 7.16: The cover time CGof G =( V , E ) with n vertices is bounded by

CG ≤ H ( n −1) max
u ,v∈ V : u =v
hu ,v.
Proof: For convenience let B =max u ,v∈ V : u =v hu ,v. Consider a random walk starting
from a vertex u. We choose an ordering of the vertices according to a uniform permu-
tation; let Z 1 , Z 2 ,..., Zn be the ordering. Let Tj be the first time when all of the first
j vertices in the order, Z 1 , Z 2 ,..., Zj , have been visited, and let Aj be the last vertex
from the set{ Z 1 ,..., Zj }that was visited. Following the spirit of the coupon collector’s

markov chains and random walks
analysis, we consider the successive time intervals Tj − Tj − 1. If the chain’s history is
given by X 1 , X 2 ,..., then in particular for j ≥2 we consider

Yj = E [ Tj − Tj − 1 | Z 1 ,..., Zj ; X 1 ,..., XTj − 1 ].
The expected time to cover the graph starting from u is
∑ n
j = 2
Yj + E [ T 1 ].
If Z 1 is chosen to be u , which happens with probability 1/ n , then T 1 is 0. Otherwise,
E [ T 1 | Z 1 ]= hu , Z 1 ≤ B. Hence E [ T 1 ]≤(1− 1 / n ) B.
For the Yj , there are two cases to consider. If Zj is not the last vertex seen from the set
{ Z 1 , Z 2 ,..., Zj }, then Yj is 0, since Tj = Tj − 1 in that case. If Zj is the last vertex seen
from this set, then, regardless of the rest of the history of the chain, Yj ≤ B , since Yj is
the hitting time hZk , Zj for the Zk that was visited last out of{ Z 1 , Z 2 ,..., Zj − 1 }. As the
Zj were chosen according to a random permutation, independent of the random walk,
we have Zj is last out of the set{ Z 1 , Z 2 ,..., Zj }with probability 1/ j. It follows that

∑ n
j = 2
Yj + E [ T 1 ]≤
∑ n
j = 2
1
j
B +
(
1 −
1
n
)
B
=
⎛
⎝ 1 +
∑ n
j = 2
1
j
⎞
⎠ B −^1
n
B
= H ( n −1) B.
Since this holds for every starting vertex u , the lemma is proven.  /theta
One can similarly obtain lower bounds using the same technique. A natural lower
bound is

CG ≥ H ( n −1) min
u ,v∈ V : u =v
hu ,v.
However, the minimum hitting time can be very small for some graphs, making this
bound less useful. In some cases, the lower bound can be made stronger by considering
a subset of vertices V ′⊂ V. In this case, the proof can be modified to give

CG ≥ H (| V ′|−1) min
u ,v∈ V ′: u =v
hu ,v.
The term from the harmonic series is smaller, but the minimum hitting time used in the
bound may correspondingly be larger.

7.4.1 Application: An s – t Connectivity Algorithm
Suppose we are given an undirected graph G =( V , E ) and two vertices s and t in G. Let
n =| V |and m =| E |. We want to determine if there is a path connecting s and t. This
is easily done in linear time using a standard breadth-first search or depth-first search.
Such algorithms, however, require m( n ) space.

7.5 Parrondo’s Paradox
s – t Connectivity Algorithm:
1. Start a random walk from s.
2. If the walk reaches t within 2 n^3 steps, return that there is a path. Otherwise,
return that there is no path.

Algorithm 7.4: s – t Connectivity algorithm.
Here we develop a randomized algorithm that works with only O (log n ) bits of mem-
ory. This could be even less than the number of bits required to write the path between
s and t. The algorithm is simple: perform a random walk on G for enough steps so that
a path from s to t is likely to be found. We use the cover time result (Lemma7.16)
to bound the number of steps that the random walk has to run. For convenience,
assume that the graph G has no bipartite connected components, so that the results of
Theorem7.13apply to any connected component of G. (The results can be made to
apply to bipartite graphs with some additional technical work.)

Theorem 7.17: The s–t connectivity algorithm (Algorithm7.4) returns the correct
answer with probability 1 / 2 , and it only errs by returning that there is no path from s
to t when there is such a path.

Proof: If there is no path then the algorithm returns the correct answer. If there is a path,
the algorithm errs if it does not find the path within 2 n^3 steps of the walk. The expected
time to reach t from s (if there is a path) is bounded from above by the cover time
of their shared component, which by Lemma7.15is at most 2 nm < n^3. By Markov’s
inequality, the probability that a walk takes more than 2 n^3 steps to reach t from s is at
most 1/2.  /theta

The algorithm must keep track of its current position, which takes O (log n ) bits, as well
as the number of steps taken in the random walk, which also takes only O (log n ) bits
(since we count up only to 2 n^3 ). As long as there is some mechanism for choosing a
random neighbor from each vertex, this is all the memory required.

7.5. Parrondo’s Paradox
Parrondo’s paradox provides an interesting example of the analysis of Markov chains
while also demonstrating a subtlety in dealing with probabilities. The paradox appears
to contradict the old saying that two wrongs don’t make a right, showing that two losing
games can be combined to make a winning game. Because Parrondo’s paradox can be
analyzed in many different ways, we will go over several approaches to the problem.
First, consider game A , in which we repeatedly flip a biased coin (call it coin a ) that
comes up heads with probability pa < 1 /2 and tails with probability 1− pa .Youwin
a dollar if the coin comes up heads and lose a dollar if it comes up tails. Clearly, this is
a losing game for you. For example, if pa = 0 .49, then your expected loss is two cents
per game.

markov chains and random walks
In game B , we also repeatedly flip coins, but the coin that is flipped depends on how
you have been doing so far in the game. Letwbe the number of your wins so far and
 nthe number of your losses. Each round we bet one dollar, sow− nrepresents your
winnings; if it is negative, you have lost money. Game B uses two biased coins, coin b
and coin c. If your winnings in dollars are a multiple of 3, then you flip coin b , which
comes up heads with probability pb and tails with probability 1− pb. Otherwise, you
flip coin c , which comes up heads with probability pc and tails with probability 1− pc.
Again, you win a dollar if the coin comes up heads and lose a dollar if it comes up tails.
This game is more complicated, so let us consider a specific example. Suppose coin
b comes up heads with probability pb = 0 .09 and tails with probability 0.91 and that
coin c comes up heads with probability pc = 0 .74 and tails with probability 0.26. At
first glance, it might seem that game B is in your favor. If we use coin b for the 1/3of
the time that your winnings are a multiple of 3 and use coin c the other 2/3 of the time,
then your probabilitywof winning is

w=
1
3
9
100
+
2
3
74
100
=
157
300
>
1
2
.
The problem with this line of reasoning is that coin b is not necessarily used 1/3of
the time! To see this intuitively, consider what happens when you first start the game,
when your winnings are 0. You use coin b and most likely lose, after which you use
coin c and most likely win. You may spend a great deal of time going back and forth
between having lost one dollar and breaking even before either winning one dollar or
losing two dollars, so you may use coin b more than 1/3 of the time.
In fact, the specific example for game B is a losing game for you. One way to show
this is to suppose that we start playing game B when your winnings are 0, continuing
until you either lose three dollars or win three dollars. If you are more likely to lose
than win in this case, by symmetry you are more likely to lose three dollars than win
three dollars whenever your winnings are a multiple of 3. On average, then, you would
obviously lose money on the game.
One way to determine if you are more likely to lose than win is to analyze the absorb-
ing states. Consider the Markov chain on the state space consisting of the integers
{− 3 ,..., 3 }, where the states represent your winnings. We want to know, when you
start at 0, whether or not you are more likely to reach−3 before reaching 3. We can
determine this by setting up a system of equations. Let zi represent the probability you
will end up having lost three dollars before having won three dollars when your current
winnings are i dollars. We calculate all the probabilities z − 3 , z − 2 , z − 1 , z 0 , z 1 , z 2 , and z 3 ,
although what we are really interested in is z 0 .If z 0 > 1 /2, then we are more likely to
lose three dollars than win three dollars starting from 0. Here z − 3 =1 and z 3 =0; these
are boundary conditions. We also have the following equations:

z − 2 =(1− pc ) z − 3 + pcz − 1 ,
z − 1 =(1− pc ) z − 2 + pcz 0 ,
z 0 =(1− pb ) z − 1 + pbz 1 ,
z 1 =(1− pc ) z 0 + pcz 2 ,
z 2 =(1− pc ) z 1 + pcz 3.
7.5 parrondo’s paradox
This is a system of five equations with five unknowns, and hence it can be solved easily.
The general solution for z 0 is

z 0 =
(1− pb )(1− pc )^2
(1− pb )(1− pc )^2 + pbp^2 c
.
For the specific example here, the solution yields z 0 = 15 , 379 / 27 , 700 ≈ 0 .555, show-
ing that one is much more likely to lose than win playing this game over the long run.
Instead of solving these equations directly, there is a simpler way of determining
the relative probability of reaching−3 or 3 first. Consider any sequence of moves that
starts at 0 and ends at 3 before reaching−3. For example, a possible sequence is

s = 0 , 1 , 2 , 1 , 2 , 1 , 0 ,− 1 ,− 2 ,− 1 , 0 , 1 , 2 , 1 , 2 , 3.
We create a one-to-one and onto mapping of such sequences with the sequences that
start at 0 and end at−3 before reaching 3 by negating every number starting from the
last 0 in the sequence. In this example, s maps to f ( s ), where

f ( s )= 0 , 1 , 2 , 1 , 2 , 1 , 0 ,− 1 ,− 2 ,− 1 , 0 ,− 1 ,− 2 ,− 1 ,− 2 ,− 3.
It is simple to check that this is a one-to-one mapping of the relevant sequences.
The following lemma provides a useful relationship between s and f ( s ).

Lemma 7.18: For any sequence s of moves that starts at 0 and ends at 3 before reach-
ing − 3 , we have

Pr( s occurs)
Pr( f ( s ) occurs)
=
pbp^2 c
(1− pb )(1− pc )^2
.
Proof: For any given sequence s satisfying the properties of the lemma, let t 1 be the
number of transitions from 0 to 1; t 2 , the number of transitions from 0 to−1; t 3 , the
sum of the number of transitions from−2to−1,−1 to 0, 1 to 2, and 2 to 3; and t 4 ,
the sum of the number of transitions from 2 to 1, 1 to 0,−1to−2, and−2to−3. Then
the probability that the sequence s occurs is ptb^1 (1− pb ) t^2 ptc^3 (1− pc ) t^4.
Now consider what happens when we transform s to f ( s ). We change one transition
from 0 to 1 into a transition from 0 to−1. After this point, in s the total number of
transitions that move up 1 is two more than the number of transitions that move down
1, since the sequence ends at 3. In f ( s ), then, the total number of transitions that move
down 1 is two more than the number of transitions that move up 1. It follows that
the probability that the sequence f ( s ) occurs is ptb^1 −^1 (1− pb ) t^2 +^1 ptc^3 −^2 (1− pc ) t^4 +^2. The
lemma follows.  /theta

By letting S be the set of all sequences of moves that start at 0 and end at 3 before
reaching−3, it immediately follows that

Pr(3 is reached before−3)
Pr(−3 is reached before 3)
=
∑
∑ s ∈ S Pr( s occurs)
s ∈ S Pr( f ( s ) occurs)
=
pbp^2 c
(1− pb )(1− pc )^2
.
If this ratio is less than 1, then you are more likely to lose than win. In our specific
example, this ratio is 12, 321 / 15 , 379 <1.

markov chains and random walks
Finally, yet another way to analyze the problem is to use the stationary distribution.
Consider the Markov chain on the states{ 0 , 1 , 2 }, where here the states represent the
remainder when our winnings are divided by 3. (That is, the state keeps track ofw− n
mod 3.) Letπ i be the stationary probability of this chain. The probability that we win
a dollar in the stationary distribution, which is the limiting probability that we win a
dollar if we play long enough, is then

pb π 0 + pc π 1 + pc π 2 = pb π 0 + pc (1−π 0 )
= pc −( pc − pb )π 0.
Again, we want to know if this is greater than or less than 1/2.
The equations for the stationary distribution are easy to write:
π 0 +π 1 +π 2 = 1 ,
pb π 0 +(1− pc )π 2 =π 1 ,
pc π 1 +(1− pb )π 0 =π 2 ,
pc π 2 +(1− pc )π 1 =π 0.

Indeed, since there are four equations and only three unknowns, one of these equations
is actually redundant. The system is easily solved to find

π 0 =
1 − pc + p^2 c
3 − 2 pc − pb + 2 pbpc + p^2 c
,
π 1 =
pbpc − pc + 1
3 − 2 pc − pb + 2 pbpc + p^2 c
,
π 2 =
pbpc − pb + 1
3 − 2 pc − pb + 2 pbpc + p^2 c
.
Recall that you lose if the probability of winning in the stationary distribution is less
than 1/2 or, equivalently, if pc −( pc − pb )π 0 < 1 /2. In our specific example,π 0 =
673 / 1759 ≈ 0. 3826 ...,and

pc −( pc − pb )π 0 =
86 , 421
175 , 900
<
1
2
.
Again, we find that game B is a losing game in the long run.
We have now completely analyzed game A and game B. Next let us consider what
happens when we try to combine these two games. In game C , we repeatedly perform
the following bet. We start by flipping a fair coin, call it coin d. If coin d is heads, we
proceed as in game A : we flip coin a , and if the coin is heads, you win. If coin d is tails,
we then proceed to game B : if your current winnings are a multiple of 3, we flip coin b ;
otherwise, we flip coin c , and if the coin is heads then you win. It would seem that this
must be a losing game for you. After all, game A and game B are both losing games,
and this game just flips a coin to decide which of the two games to play.
In fact, game C is exactly like game B , except the probabilities are slightly different.
If your winnings are a multiple of 3, then the probability that you win is p ∗ b =^12 pa +
1
2 pb. Otherwise, the probability that you win is p

∗
c =
1
2 pa +
1
2 pc. Using p
∗
b and p
∗
c in
place of pb and pc , we can repeat any of the foregoing analyses we used for game B.

7.5 parrondo’s paradox
For example: if the ratio
p ∗ b ( p ∗ c )^2
(1− p ∗ b )(1− p ∗ c )^2
< 1 ,
then the game is a losing game for you; if the ratio is larger than 1, it is a winning game.
In our specific example the ratio is 438, 741 / 420 , 959 >1, so game C appears to be a
winning game.
This seems somewhat odd, so let us recheck by using our other approach of consid-
ering the stationary distribution. The game is a losing game if p ∗ c −( p ∗ c − p ∗ b )π 0 <
1 /2 and a winning game if p ∗ c −( p ∗ c − p ∗ b )π 0 > 1 /2, whereπ 0 is now the station-
ary distribution for the chain corresponding to game C. In our specific example,
π 0 = 30 , 529 / 88 ,597, and

p ∗ c −( p ∗ c − p ∗ b )π 0 =
4 , 456 , 523
8 , 859 , 700
>
1
2
,
so game C again appears to be a winning game.
How can randomly combining two losing games yield a winning game? The key
is that game B was a losing game because it had a very specific structure. You were
likely to lose the next round in game B if your winnings were divisible by 3, but if
you managed to get over that initial barrier you were likely to win the next few games
as well. The strength of that barrier made game B a losing game. By combining the
games that barrier was weakened, because now when your winnings are divisible by 3
you sometimes get to play game A , which is close to a fair game. Although game A
is biased against you, the bias is small, so it becomes easier to overcome that initial
barrier. The combined game no longer has the specific structure required to make it a
losing game.
You may be concerned that this seems to violate the law of linearity of expectations.
If the winnings from a round of game A , B , and C are XA , XB , and XC (respectively),
then it seems that

E [ XC ]= E
[
1
2
XA +
1
2
XB
]
=
1
2
E [ XA ]+
1
2
E [ XB ],
so if E [ XA ] and E [ XB ] are negative then E [ XC ] should also be negative. The problem
is that this equation does not make sense, because we cannot talk about the expected
winnings of a round of games B and C without reference to the current winnings. We
have described a Markov chain on the states{ 0 , 1 , 2 }for games B and C. Let s represent
the current state. We have

E [ XC | s ]= E
[
1
2
( XA + XB )| s
]
=
1
2
E [ XA | s ]+
1
2
E [ XB | s ].
Linearity of expectations holds for any given step, but we must condition on the current
state. By combining the games we have changed how often the chain spends in each
state, allowing the two losing games to become a winning game.

markov chains and random walks
7.6 Exercises
Exercise 7.1: Consider a Markov chain with state space{ 0 , 1 , 2 , 3 }and a transition
matrix

P =
⎡
⎢
⎢⎢
⎣
03 /10 1/10 3/ 5
1 /10 1/10 7/10 1/ 10
1 /10 7/10 1/10 1/ 10
9 /10 1/10 0 0
⎤
⎥
⎥⎥
⎦
,
so P 0 , 3 = 3 /5 is the probability of moving from state 0 to state 3.

(a) Find the stationary distribution of the Markov chain.
(b) Find the probability of being in state 3 after 32 steps if the chain begins at state 0.
(c) Find the probability of being in state 3 after 128 steps if the chain begins at a state
chosen uniformly at random from the four states.
(d) Suppose that the chain begins in state 0. What is the smallest value of t for which
max s | Pt 0 , s −π s |≤ 0 .01? Here ̄πis the stationary distribution. What is the smallest
value of t for which max s | P 0 t , s −π s |≤ 0 .001?

Exercise 7.2: Consider the two-state Markov chain with the following transition
matrix

P =
[
p 1 − p
1 − pp
]
.
Find a simple expression for P 0 t , 0.

Exercise 7.3: Consider a process X 0 , X 1 , X 2 ,...with two states, 0 and 1. The process
is governed by two matrices, P and Q .If k is even, the values Pi , j give the probability
of going from state i to state j on the step from Xk to Xk + 1. Likewise, if k is odd then
the values Qi , j give the probability of going from state i to state j on the step from Xk to
Xk + 1. Explain why this process does not satisfy Definition7.1of a (time-homogeneous)
Markov chain. Then give a process with a larger state space that is equivalent to this
process and satisfies Definition7.1.

Exercise 7.4: Prove that the communicating relation defines an equivalence relation.

Exercise 7.5: Prove that if one state in a communicating class is transient (respectively,
recurrent) then all states in that class are transient (respectively, recurrent).

Exercise 7.6: In studying the 2-SAT algorithm, we considered a 1-dimensional ran-
dom walk with a completely reflecting boundary at 0. That is, whenever position 0 is
reached, with probability 1 the walk moves to position 1 at the next step. Consider
now a random walk with a partially reflecting boundary at 0. Whenever position 0 is
reached, with probability 1/2 the walk moves to position 1 and with probability 1/2 the

7.6 exercises
walk stays at 0. Everywhere else the random walk moves either up or down 1, each with
probability 1/2. Find the expected number of moves to reach n , starting from position
i and using a random walk with a partially reflecting boundary.

Exercise 7.7: Suppose that the 2-SAT Algorithm7.1starts with an assignment chosen
uniformly at random. How does this affect the expected time until a satisfying assign-
ment is found?

Exercise 7.8: Generalize the randomized algorithm for 3-SAT to k -SAT. What is the
expected time of the algorithm as a function of k?

Exercise 7.9: In the analysis of the randomized algorithm for 3-SAT, we made the pes-
simistic assumption that the current assignment Ai and the truth assignment S differ on
just one variable in the clause chosen at each step. Suppose instead that, independently
at each step, the two assignments disagree on one variable in the clause with probabil-
ity p and at least two variables with probability 1− p. What is the largest value of p
for which you can prove that the expected number of steps before Algorithm7.2termi-
nates is polynomial in the number of variables n? Give a proof for this value of p and
give an upper bound on the expected number of steps in this case.

Exercise 7.10: A coloring of a graph is an assignment of a color to each of its vertices.
A graph is k -colorable if there is a coloring of the graph with k colors such that no two
adjacent vertices have the same color. Let G be a 3-colorable graph.

(a) Show that there exists a coloring of the graph with two colors such that no triangle
is monochromatic. (A triangle of a graph G is a subgraph of G with three vertices,
which are all adjacent to each other.)
(b) Consider the following algorithm for coloring the vertices of G with two colors
so that no triangle is monochromatic. The algorithm begins with an arbitrary 2-
coloring of G. While there are any monochromatic triangles in G , the algorithm
chooses one such triangle and changes the color of a randomly chosen vertex of
that triangle. Derive an upper bound on the expected number of such recoloring
steps before the algorithm finds a 2-coloring with the desired property.

Exercise 7.11: An n × n matrix P with entries Pi , j is called stochastic if all entries are
nonnegative and if the sum of the entries in each row is 1. It is called doubly stochastic
if, additionally, the sum of the entries in each column is 1. Show that the uniform
distribution is a stationary distribution for any Markov chain represented by a doubly
stochastic matrix.

Exercise 7.12: Let Xn be the sum of n independent rolls of a fair die. Show that, for
any k ≥2,

lim
n →∞
Pr( Xn is divisible by k )=
1
k
.
markov chains and random walks
Exercise 7.13: Consider a finite Markov chain on n states with stationary distribution
π ̄and transition probabilities Pi , j. Imagine starting the chain at time 0 and running it for
m steps, obtaining the sequence of states X 0 , X 1 ,..., Xm. Consider the states in reverse
order, Xm , Xm − 1 ,..., X 0.

(a) Argue that given Xk + 1 , the state Xk is independent of Xk + 2 , Xk + 3 ,..., Xm. Thus the
reverse sequence is Markovian.
(b) Argue that for the reverse sequence, the transition probabilities Qi , j are given by

Qi , j =
π jPj , i
π i
.
(c) Prove that if the original Markov chain is time reversible, so thatπ iPi , j =π jPj , i ,
then Qi , j = Pi , j. That is, the states follow the same transition probabilities whether
viewed in forward order or reverse order.
Exercise 7.14: Prove that the Markov chain corresponding to a random walk on an
undirected, non-bipartite graph that consists of one component is time reversible.

Exercise 7.15: Let Pit , i be the probability that a Markov chain returns to state i when
started in state i after t steps. Prove that

∑∞
t = 1
Pit , i
is unbounded if and only if state i is recurrent.

Exercise 7.16: Prove Lemma7.5.

Exercise 7.17: Consider the following Markov chain, which is similar to the 1-dimen-
sional random walk with a completely reflecting boundary at 0. Whenever position 0
is reached, with probability 1 the walk moves to position 1 at the next step. Otherwise,
the walk moves from i to i +1 with probability p and from i to i −1 with probability
1 − p. Prove that:

(a) if p < 1 /2, each state is positive recurrent;
(b) if p = 1 /2, each state is null recurrent;
(c) if p > 1 /2, each state is transient.

Exercise 7.18: (a) Consider a random walk on the 2-dimensional integer lattice, where
each point has four neighbors (up, down, left, and right). Is each state transient, null
recurrent, or positive recurrent? Give an argument.
(b) Answer the problem in (a) for the 3-dimensional integer lattice.

Exercise 7.19: Consider the gambler’s ruin problem, where a player plays until she
lose n 1 dollars or win n 2 dollars. Prove that the expected number of games played is
 n 1 n 2.

7.6 exercises
Exercise 7.20: We have considered the gambler’s ruin problem in the case where the
game is fair. Consider the case where the game is not fair; instead, the probability of
losing a dollar each game is 2/3 and the probability of winning a dollar each game is
1 /3. Suppose that you start with i dollars and finish either when you reach n or lose it
all. Let Wt be the amount you have gained after t rounds of play.

(a) Show that E [2 Wt +^1 ]= E [2 Wt ].
(b) Use part (a) to determine the probability of finishing with 0 dollars and the proba-
bility of finishing with n dollars when starting at position i.
(c) Generalize the preceding argument to the case where the probability of losing is
p > 1 /2. ( Hint: Try considering E [ cWt ] for some constant c .)

Exercise 7.21: Consider a Markov chain on the states{ 0 , 1 ,..., n }, where for i < n
we have Pi , i + 1 = 1 /2 and Pi , 0 = 1 /2. Also, Pn , n = 1 /2 and Pn , 0 = 1 /2. This process
can be viewed as a random walk on a directed graph with vertices{ 0 , 1 ,..., n }, where
each vertex has two directed edges: one that returns to 0 and one that moves to the
vertex with the next higher number (with a self-loop at vertex n ). Find the stationary
distribution of this chain. (This example shows that random walks on directed graphs
are very different than random walks on undirected graphs.)

Exercise 7.22: A cat and a mouse each independently take a random walk on a con-
nected, undirected, non-bipartite graph G. They start at the same time on different
nodes, and each makes one transition at each time step. The cat eats the mouse if they
are ever at the same node at some time step. Let n and m denote, respectively, the
number of vertices and edges of G. Show an upper bound of O ( m^2 n ) on the expected
time before the cat eats the mouse. ( Hint: Consider a Markov chain whose states are
the ordered pairs ( a , b ), where a is the position of the cat and b is the position of the
mouse.)

Exercise 7.23: One way of spreading information on a network uses a rumor-
spreading paradigm. Suppose that there are n hosts currently on the network. Initially,
one host begins with a message. Each round, every host that has the message con-
tacts another host chosen independently and uniformly at random from the other n − 1
hosts, and sends that host the message. We would like to know how many rounds are
necessary before all hosts have received the message with probability 0.9999.

(a) Explain how this problem can be viewed in terms of Markov chains.
(b) Determine a method for computing the probability that j hosts have received the
message after round k given that i hosts have received the message after round
k −1. ( Hint: There are various ways of doing this. One approach is to let P ( i , j , c )
be the probability that j hosts have the message after the first c of the i hosts have
made their choices in a round; then find a recurrence for P .)
(c) As a computational exercise, write a program to determine the number of rounds
required for a message starting at one host to reach all other hosts with probability
0.9999 when n =128.

markov chains and random walks
Figure 7.3: Lollipop graph.
Exercise 7.24: The lollipop graph on n vertices is a clique on n /2 vertices connected
to a path on n /2 vertices, as shown in Figure7.3. The node u is a part of both the clique
and the path. Letvdenote the other end of the path.

(a) Show that the expected covering time of a random walk starting atvis /eta( n^2 ).
(b) Show that the expected covering time for a random walk starting at u is /eta( n^3 ).

Exercise 7.25: The following is a variation of a simple children’s board game. A
player starts at position 0. On a player’s turn, she rolls a standard six-sided die. If her
old position was the positive integer x and her roll is y , then her new position is x + y ,
except in two cases:

 /thetaif x + y is divisible by 6 and less than 36, her new position is x + y −6;
 /thetaif x + y is greater than 36, the player remains at x.
The game ends when a player reaches the goal position, 36.

(a) Let Xi be a random variable representing the number of rolls needed to get to 36
from position i for 0≤ i ≤35. Give a set of equations that characterize E [ Xi ].
(b) Using a program that can solve systems of linear equations, find E [ Xi ]for0≤ i ≤

Exercise 7.26: Let n equidistant points be marked on a circle. Without loss of gen-
erality, we think of the points as being labeled clockwise from 0 to n −1. Initially, a
wolf begins at 0 and there is one sheep at each of the remaining n −1 points. The wolf
takes a random walk on the circle. For each step, it moves with probability 1/2toone
neighboring point and with probability 1/2 to the other neighboring point. At the first
visit to a point, the wolf eats a sheep if there is still one there. Which sheep is most
likely to be the last eaten?

Exercise 7.27: Suppose that we are given n records, R 1 , R 2 ,..., Rn. The records are
kept in some order. The cost of accessing the j th record in the order is j. Thus, if we
had four records ordered as R 2 , R 4 , R 3 , R 1 , then the cost of accessing R 4 would be 2
and the cost of accessing R 1 would be 4.
Suppose further that, at each step, record Rj is accessed with probability pj , with
each step being independent of other steps. If we knew the values of the pj in advance,

7.6 exercises
we would keep the Rj in decreasing order with respect to pj. But if we don’t know the
pj in advance, we might use the “move to front” heuristic: at each step, put the record
that was accessed at the front of the list. We assume that moving the record can be done
with no cost and that all other records remain in the same order. For example, if the
order was R 2 , R 4 , R 3 , R 1 before R 3 was accessed, then the order at the next step would
be R 3 , R 2 , R 4 , R 1.
In this setting, the order of the records can be thought of as the state of a Markov
chain. Give the stationary distribution of this chain. Also, let Xk be the cost for accessing
the k th requested record. Determine an expression for lim k →∞ E [ Xk ]. Your expression
should be easily computable in time that is polynomial in n , given the pj.

Exercise 7.28: Consider the following variation of the discrete time queue. Time is
divided into fixed-length steps. At the beginning of each time step, a customer arrives
with probabilityλ. At the end of each time step, if the queue is nonempty then the
customer at the front of the line completes service with probabilityμ.

(a) Explain how the number of customers in the queue at the beginning of each time
step forms a Markov chain, and determine the corresponding transition probabili-
ties.
(b) Explain under what conditions you would expect a stationary distribution ̄πto
exist.
(c) If a stationary distribution exists, then what should be the value ofπ 0 , the proba-
bility that no customers are in the queue at the beginning of the time step? ( Hint:
Consider that, in the long run, the rate at which customers enter the queue and the
rate at which customers leave the queue must be equal.)
(d) Determine the stationary distribution and explain how it corresponds to your con-
ditions from part (b).
(e) Now consider the variation where we change the order of incoming arrivals and
service. That is: at the beginning of each time step, if the queue is nonempty then
a customer is served with probabilityμ; and at the end of a time step a customer
arrives with probabilityλ. How does this change your answers to parts (a)–(d)?

Exercise 7.29: Prove that the Markov chain from Lemma7.14where the states are
the 2| E |directed edges of the graph has a uniform stationary distribution.

Exercise 7.30: We consider the covering time for the standard random walk on a
hypercube with N = 2 n nodes. (See Definition4.3if needed to recall the definition of
a hypercube.) Let ( u ,v) be an edge in the hypercube.

(a) Prove that the expected time between traversals of the edge ( u ,v) from u tovis
Nn.
(b) We consider the time between transitions from u tovin a different way. After
moving from u tov, the walk must first return to u. When it returns to u , the walk
might next move tov, or it might move to another neighbor of u , in which case it
must return to u again before moving tovfor there to be a transition from u tov.

markov chains and random walks
Use symmetry and the above description to prove the following recurrence:
Nn =
∑∞
i = 1
1
n
(
n − 1
n
) i − 1
( i ( hu ,v+1))= n ( hu ,v+1).
(c) Conclude from the above that hu ,v= N −1.
(d) Using the result on the hitting time of adjacent vertices and Matthews’ theorem,
show that the cover time is O ( N log^2 N ).
(e) As a much more challenging problem, you can try to prove that the maximum
hitting time between any two vertices for the random walk on the hypercube is
O ( N ), and that the cover time is correspondingly O ( N log N ).

chapter eight

Continuous Distributions and

the Poisson Process

This chapter introduces the general concept of continuous random variables, focusing
on two examples of continuous distributions: the uniform distribution and the expo-
nential distribution. We then proceed to study the Poisson process, a continuous time
counting process that is related to both the uniform and exponential distributions. We
conclude this chapter with basic applications of the Poisson process in queueing theory.

8.1 Continuous Random Variables
8.1.1 Probability Distributions inR
The continuous roulette wheel in Figure8.1has circumference 1. We spin the wheel,
and when it stops, the outcome is the clockwise distance X (computed with infinite
precision) from the “0” mark to the arrow.
The sample space mof this experiment consists of all real numbers in the range
[0,1). Assume that any point on the circumference of the disk is equally likely to face
the arrow when the disk stops. What is the probability p of a given outcome x?
To answer this question, we recall that in Chapter 1 we defined a probability function
to be any function that satisfies the following three requirements:

1. Pr( m)= 1 ;
2. for any event E ,

0 ≤Pr( E )≤ 1 ;
3. for any (finite or enumerable) collectionBof disjoint events,

Pr
(
⋃
E ∈B
E
)
=
∑
E ∈B
Pr( E ).
Let S ( k ) be a set of k distinct points in the range [0,1), and let p be the probability that
any given point in [0,1) is the outcome of the roulette experiment. Since the probability

continuous distributions and the poisson process
(^0) x
Figure 8.1: A continuous roulette wheel.
of any event is bounded by 1,
Pr( x ∈ S ( k ))= kp ≤ 1.
We can choose any number k of distinct points in the range [0,1), so we must have kp ≤
1 for any integer k , which implies that p =0. Thus, we observe that in an infinite sample
space there may be possible events that have probability 0. Taking the complement of
such an event, we observe that in an infinite sample space there can be events with
probability 1 that do not correspond to all possible experimental outcomes, and thus
there can be events with probability 1 that are, in some sense, not certain!
If the probability of each possible outcome of our experiment is 0, how do we define
the probability of larger events with nonzero probability? For probability distributions
overR, probabilities are assigned to intervals rather than to individual values.^1
The probability distribution of a random variable X is given by its distribution func-
tion F ( x ), where for any x ∈Rwe define
F ( x )=Pr( X ≤ x ).
We say that a random variable X is continuous if its distribution function F ( x )is
a continuous function of x. We will assume that our random variables are continuous
throughout this chapter. In this case, we must have that Pr( X = x )=0 for any specific
value x. This further implies that Pr( X ≤ x )=Pr( X < x ), a fact we make use of freely
throughout this chapter.
If there is a function f ( x ) such that, for all−∞< a <∞,
F ( a )=
∫ a
−∞
f ( t ) dt ,
then f ( x ) is called the density function of F ( x ), and
f ( x )= F ′( x )
where the derivative is well-defined.
(^1) A formal treatment of nondenumerably infinite probability spaces relies on measure theory and is beyond the
scope of this book. We just note here that the probability function needs to be measurable on the set of events.
This cannot hold in general for the family of all subsets of the sample space, but it does always hold for the
Borel set of intervals.

8.1 continuous random variables
Because
Pr( x < X ≤ x + dx )= F ( x + dx )− F ( x )≈ f ( x ) dx ,
we can informally think of f ( x ) dx as the “probability” of the infinitesimal interval
( x , x + dx ]. Carrying this analogy forward, in discrete spaces the probability of an event
E is the sum of the probabilities of the simple events included in E. The parallel concept
in the case of events inRis the integral of the probability density function over the basic
events in E.
For example, the probability of the interval [ a , b ) is given by the integral

Pr( a ≤ x < b )=
∫ b
a
f ( x ) dx ,
and the expectation and higher moments of a random variable X with density function
f ( x ) are defined by the integrals

E [ Xi ]=
∫∞
−∞
xif ( x ) dx.
More generally, for any function g ,

E [ g ( X )]=
∫∞
−∞
g ( x ) f ( x ) dx ,
when this integral exists. The variance of X is given by

Var [ X ]= E [( X − E [ X ])^2 ]=
∫∞
−∞
( x − E [ X ])^2 f ( x ) dx = E [ X^2 ]−( E [ X ])^2.
The following lemma gives the continuous analog to Lemma2.9.
Lemma 8.1: Let X be a continuous random variable that takes on only nonnegative
values. Then

E [ X ]=
∫∞
0
Pr( X ≥ x ) dx.
Proof: Let f ( x ) be the density function of X. Then
∫∞

x = 0
Pr( X ≥ x ) dx =
∫∞
x = 0
∫∞
y = x
f ( y ) dydx
=
∫∞
y = 0
∫ y
x = 0
f ( y ) dxdy
=
∫∞
y = 0
yf ( y ) dy
= E [ X ].
The interchange of the order of the integrals is justified because the expression being
integrated is nonnegative.  /theta

continuous distributions and the poisson process
8.1.2 Joint Distributions and Conditional Probability
The notion of a distribution function for a real-valued random variable easily general-
izes to multiple random variables.

Definition 8.1: The joint distribution function of X and Y is

F ( x , y )=Pr( X ≤ x , Y ≤ y ).
The variables X and Y have joint density function f if, for all x , y,

F ( x , y )=
∫ y
−∞
∫ x
−∞
f ( u ,v) dud v.
Again, we denote

f ( x , y )=
∂^2
∂ x ∂ y
F ( x , y )
when the derivative exists. These definitions are generalized to joint distribution func-
tions over more than two variables in the obvious way.
Given a joint distribution function F ( x , y ) over X and Y , one may consider the
marginal distribution functions

FX ( x )=Pr( X ≤ x ), FY ( y )=Pr( Y ≤ y ),
and the corresponding marginal density functions fX ( x ) and fY ( y ).

Definition 8.2: The random variables X and Y are independent if, for all x and y,

Pr(( X ≤ x )∩( Y ≤ y ))=Pr( X ≤ x )Pr( Y ≤ y ).
From the definition, two random variables are independent if and only if their joint
distribution function is the product of their marginal distribution functions:

F ( x , y )= FX ( x ) FY ( y ).
It follows from taking the derivatives with respect to x and y that, if X and Y are inde-
pendent, then

f ( x , y )= fX ( x ) fY ( y ),
and this condition is sufficient as well.
As an example, let a and b be positive constants, and consider the joint distribution
function for two random variables X and Y given by

F ( x , y )= 1 −e− ax −e− by +e−( ax + by )
over the range x , y ≥0. We can compute that

FX ( x )= F ( x ,∞)= 1 −e− ax ,
and similarly FY ( y )= 1 −e− by. Alternatively, we could compute

f ( x , y )= ab e−( ax + by ),
8.1 continuous random variables
from which it follows that

FX ( z )=
∫ z
x = 0
∫∞
y = 0
ab e−( ax + by ) dydx =
∫ z
x = 0
− a e− ax = 1 −e− az.
We obtain

F ( x , y )= 1 −e− ax −e− by +e−( ax + by )=(1−e− ax )(1−e− by )= FX ( x ) FY ( y ),
so X and Y are independent. Alternatively, working with the density functions we verify
their independence by

fX ( x )= a e− ax , fY ( y )= b e− by , f ( x , y )= fX ( x ) fY ( y ).
Conditional probability for continuous random variables introduces a nontrivial sub-
tlety. The natural definition,

Pr( E | F )=
Pr( E ∩ F )
Pr( F )
,
is suitable when Pr( F )=0. For example,

Pr( X ≤ 3 | Y ≤6)=
Pr(( X ≤3)∩( Y ≤6))
Pr( Y ≤6)
when Pr( Y ≤6) is not zero.
In the discrete case, if Pr( F )=0 then Pr( E | F ) was simply not well-defined. In
the continuous case, there are well-defined expressions that condition on events that
occur with probability 0. For example, for the joint distribution function F ( x , y )=
1 −e− ax −e− by +e−( ax + by )examined previously, it seems reasonable to consider

Pr( X ≤ 3 | Y =4),
but since Pr( Y =4) is an event with probability 0, the definition is not applicable.
If we did apply the definition, it would yield

Pr( X ≤ 3 | Y =4)=
Pr(( X ≤3)∩( Y =4))
Pr( Y =4)
.
Both the numerator and denominator are zero, suggesting that we should be taking a
limit as they both approach zero. The natural choice is

Pr( X ≤ 3 | Y =4)=lim
δ→ 0
Pr( X ≤ 3 | 4 ≤ Y ≤ 4 +δ).
This choice leads us to the following definition:

Pr( X ≤ x | Y = y )=
∫ x
u =−∞
f ( u , y )
fY ( y )
du.
continuous distributions and the poisson process
To see informally why this is a reasonable choice, consider

lim
δ→ 0
Pr( X ≤ x | y ≤ Y ≤ y +δ)
=δlim→ 0
Pr(( X ≤ x )∩( y ≤ Y ≤ y +δ))
Pr( y ≤ Y ≤ y +δ)
=δlim→ 0
F ( x , y +δ)− F ( x , y )
FY ( y +δ)− FY ( y )
=lim
δ→ 0
∫ x
u =−∞
∂ F ( u , y +δ)/∂ x −∂ F ( u , y )/∂ x
FY ( y +δ)− FY ( y )
du
=
∫ x
u =−∞
lim
δ→ 0
(∂ F ( u , y +δ)/∂ x −∂ F ( u , y )/∂ x )/δ
( FY ( y +δ)− FY ( y ))/δ
du
=
∫ x
u =−∞
f ( u , y )
fY ( y )
du.
Here we have assumed that we can interchange the limit with the integration and that
fY ( y )=0.
The value

fX | Y ( x , y )=
f ( x , y )
fY ( y )
is also called a conditional density function. We may similarly use

fY | X ( x , y )=
f ( x , y )
fX ( x )
Our definition yields the natural interpretation that, in order to compute Pr( X ≤ x |
Y = y ), we integrate the corresponding conditional density function over the appropri-
ate range. You can check that this definition yields the standard definition for Pr( X ≤ x |
Y ≤ y ) through appropriate integration. Similarly, we may compute the conditional
expectation

E [ X | Y = y ]=
∫∞
x =−∞
xfX | Y ( x , y ) dx
using the conditional density function.
For our example, when F ( x , y )= 1 −e− ax −e− by +e−( ax + by ), it follows that

Pr( X ≤ 3 | Y =4)=
∫ 3
u = 0
ab e− ax +^4 b
b e−^4 b
du = 1 −e−^3 a ,
a result we could also have achieved directly using independence.

8.2 The Uniform Distribution
When a random variable X assumes values in the interval [ a , b ] such that all subintervals
of equal length have equal probability, we say that X has the uniform distribution over
the interval [ a , b ] or alternatively that it is uniform over the interval [ a , b ]. We denote
such a random variable by U [ a , b ]. We may also talk about uniform distributions over

8.2 the uniform distribution
Figure 8.2: The uniform distribution.
the interval [ a , b ), ( a , b ], or ( a , b ). Indeed, since the probability of taking on any specific
value is 0 when b > a , the distributions are essentially the same.
The probability distribution function of such an X is

F ( x )=
⎧
⎪⎨
⎪⎩
0if x ≤ a ,
x − a
b − a if a ≤ x ≤ b ,
1if x ≥ b ,
and its density function is

f ( x )=
⎧
⎪⎨
⎪⎩
0if x < a ,
1
b − a if a ≤ x ≤ b ,
0if x > b.
These are shown in Figure8.2.
The expectation of X is

E [ X ]=
∫ b
a
x
b − a
dx =
b^2 − a^2
2( b − a )
=
b + a
2
,
and the second moment is

E [ X^2 ]=
∫ b
a
x^2
b − a
dx =
b^3 − a^3
3( b − a )
=
b^2 + ab + a^2
3
The variance is computed by

Var [ X ]= E [ X^2 ]−( E [ X ])^2 =
b^2 + ab + a^2
3
−
( b + a )^2
4
=
( b − a )^2
12
In our continuous roulette example, the outcome X of the experiment has a uniform
distribution over [0,1). Thus, the expectation of X is 1/2 and the variance of X is 1/12.

8.2.1 Additional Properties of the Uniform Distribution
Suppose you have a random variable X chosen from a uniform distribution, say over
[0,1], and it is revealed that X is less than or equal to 1/2. With this information, the
conditional distribution of X remains uniform over the smaller interval [0, 1 /2].

continuous distributions and the poisson process
Lemma 8.2: Let X be a uniform random variable on [ a , b ]. Then, for c ≤ d,

Pr( X ≤ c | X ≤ d )=
c − a
d − a
.
That is, conditioned on the fact that X ≤ d, X is uniform on [ a , d ].

Proof:

Pr( X ≤ c | X ≤ d )=
Pr(( X ≤ c )∩( X ≤ d ))
Pr( X ≤ d )
=
Pr( X ≤ c )
Pr( X ≤ d )
=
c − a
d − a
.
It follows that X , conditioned on being less than or equal to d , has a distribution function
that is exactly that of a uniform random variable on [ a , d ].  /theta

Of course, a similar statement holds if we consider Pr( X ≤ c | X ≥ d );conditioned on
X ≥ d , the resulting distribution is uniform over [ d , b ].
Another fact about the uniform distribution stems from the intuition that, if n points
are uniformly distributed over an interval, we expect them to be roughly equally spaced.
We can codify this idea as follows.

Lemma 8.3: Let X 1 , X 2 ,..., Xnbe independent uniform random variables over [0,1].
LetY 1 , Y 2 ,..., Ynbe the same values as X 1 , X 2 ,..., Xnin increasing sorted order. Then
E [ Yk ]= k /( n +1).

Proof: Let us first prove the result for Y 1 with an explicit calculation. By definition,
Y 1 =min( X 1 , X 2 ,..., Xn ). Now

Pr( Y 1 ≥ y )=Pr(min( X 1 , X 2 ,..., Xn )≥ y )
=Pr(( X 1 ≥ y )∩( X 2 ≥ y )∩···∩( Xn ≥ y ))
=
∏ n
i = 1
Pr( Xi ≥ y )
=(1− y ) n.
It follows from Lemma8.1that

E [ Y 1 ]=
∫ 1
y = 0
(1− y ) ndy =
1
n + 1
.
Alternatively, one could use F ( y )= 1 −(1− y ) n so that the density function of Y 1 is
f ( y )= n (1− y ) n −^1 , and hence using integration by parts yields

E [ Y 1 ]=
∫ 1
y = 0
ny (1− y ) n −^1 dy =− y (1− y ) n | yy ==^10 +
∫ 1
y = 0
(1− y ) ndy =
1
n + 1
.
This analysis can be extended to find E [ Yk ] with some computation, which we leave
as Exercise8.5. A simpler approach, however, makes use of symmetry. Consider the

8.3 The Exponential Distribution
Figure 8.3: A correspondence between random points on a circle and random points on a line.
circle of circumference 1, and place n +1 points P 0 , P 1 ,..., Pn independently and uni-
formly at random on the circle. This is equivalent to choosing each point by a spin of
the continuous roulette wheel of Section8.1.1. Label the point P 0 as 0, and let Xi be
the distance traveling clockwise from P 0 to Pi. The Xi are then independent, uniform
random variables from [0,1]. The value Yk is just the distance to the k th point reached
traveling clockwise from P 0. See Figure8.3.
The distance between Yk and Yk + 1 is the length of the arc between the two correspond-
ing adjacent points. By symmetry, however, all of the arcs between adjacent points must
have the same expected length. The expected length of each arc is therefore 1/( n +1),
since there are n +1 arcs created by the n points and since their total length is 1. By
the linearity of expectations, E [ Yk ] is the sum of the expected lengths of the first k arcs,
and hence E [ Yk ]= k /( n +1).  /theta

This proof makes use of an interesting one-to-one correspondence between choos-
ing n points independently and uniformly at random from [0,1] and choosing n + 1
points independently and uniformly at random from the boundary of the circle with cir-
cumference 1. Such relationships, when they are available, can often greatly simplify
an otherwise lengthy analysis. We develop other similar relationships throughout this
chapter.

8.3. The Exponential Distribution
Another important continuous distribution is the exponential distribution.

Definition 8.3: An exponential distribution with parameter θ is given by the following
probability distribution function:

F ( x )=
{
1 −e−θ x for x ≥ 0 ,
0 otherwise.
continuous distributions and the poisson process
(a)f(x)=θe−θx,x≥ 0.
1
(b)F(x)= 1 −e−θx,x≥ 0.
Figure 8.4: The exponential distribution.
The density function of the exponential distribution is
f ( x )=θe−θ x for x ≥ 0.
See Figure8.4.
Its first and second moments are

E [ X ]=
∫∞
0
t θe−θ tdt =
1
θ
,
E [ X^2 ]=
∫∞
0
t^2 θe−θ tdt =
2
θ^2
Hence,

Var [ X ]= E [ X^2 ]−( E [ X ])^2 =
1
θ^2
8.3.1 Additional Properties of the Exponential Distribution
Perhaps the most important property of the exponential distribution is that, like the
discrete geometric distribution, it is memoryless.

Lemma 8.4: For an exponential random variable with parameter θ ,

Pr( X > s + t | X > t )=Pr( X > s ).
Proof:

Pr( X > s + t | X > t )=
Pr( X > s + t )
Pr( X > t )
=
1 −Pr( X ≤ s + t )
1 −Pr( X ≤ t )
=
e−θ( s + t )
e−θ t
=e−θ s
=Pr( X > s ).  /theta
9 The Normal Distribution
The exponential distribution is the only continuous memoryless distribution. It can be
viewed as the continuous version of the discrete geometric distribution, which is the
only discrete memoryless distribution. The geometric distribution models the time until
first success in a sequence of independent identical Bernoulli trials, whereas the expo-
nential distribution models the time until the first event in a memoryless continuous
time stochastic process.
The minimum of several exponential random variables also exhibits some interesting
properties.
Lemma 8.5: If X 1 , X 2 ,..., Xnare independent exponentially distributed random vari-
ables with parameters θ 1 ,θ 2 ,...,θ n, respectively, then min( X 1 , X 2 ,..., Xn ) is expo-
nentially distributed with parameter
∑ n
i = 1 θ iand
Pr(min( X 1 , X 2 ,..., Xn )= Xi )=
θ i
∑ n
i = 1 θ i
.
Proof: It suffices to prove the statement for two exponential random variables; the gen-
eral case then follows by induction. Let X 1 and X 2 be independent exponential random
variables with parametersθ 1 andθ 2. Then
Pr(min( X 1 , X 2 )> x )=Pr(( X 1 > x )∩( X 2 > x ))
=Pr( X 1 > x )Pr( X 2 > x )
=e−θ^1 x e−θ^2 x
=e−(θ^1 +θ^2 ) x.
Hence the minimum has an exponential distribution with parameterθ 1 +θ 2.
Moreover, let f ( x 1 , x 2 ) be the joint distribution of ( X 1 , X 2 ). Since the variables are
independent, we have f ( x 1 , x 2 )=θ 1 e−θ^1 x^1 θ 2 e−θ^2 x^2. Hence
Pr( X 1 < X 2 )=
∫∞
x 2 = 0
∫ x 2
x 1 = 0
f ( x 1 , x 2 ) dx 1 dx 2
=
∫∞
x 2 = 0
θ 2 e−θ^2 x^2
(∫ x 2
x 1 = 0
θ 1 e−θ^1 x^1 dx 1
)
dx 2
=
∫∞
x 2 = 0
θ 2 e−θ^2 x^2 (1−e−θ^1 x^2 ) dx 2
=
∫∞
x 2 = 0
(θ 2 e−θ^2 x^2 −θ 2 e−(θ^1 +θ^2 ) x^2 ) dx 2
= 1 −
θ 2
θ 1 +θ 2
=
θ 1
θ 1 +θ 2
.
 /theta
For example, suppose that an airline ticket counter has n service agents, where the time
that agent i takes per customer has an exponential distribution with parameterθ i .You
stand at the head of the line at time T 0 , and all of the n agents are busy. What is the
average time you wait for an agent?
continuous distributions and the poisson process
Because service times are exponentially distributed, it does not matter for how long
each agent has been helping another customer before time T 0 ;the remaining time for
each customer is still exponentially distributed. This is a feature of the memoryless
property of the exponential distribution. Lemma8.5therefore applies. The time until
the first agent becomes free is exponentially distributed with parameter

∑ n
i = 1 θ i , so the
expected waiting time is 1/

∑ n
i = 1 θ i. Indeed, you can even determine the probability that
each agent is the first to become free; the j th agent is first with probabilityθ j /

∑ n
i = 1 θ i.
8.3.2 ∗ Example: Balls and Bins with Feedback
As an application of the exponential distribution, we consider an interesting variation
of our standard balls-and-bins model. In this problem we have only two bins, and balls
arrive one by one. Initially both bins have at least one ball. Suppose that, if bin 1 has
x balls and bin 2 has y balls, then the probability that bin 1 obtains the next ball is
x /( x + y ) while the probability that bin 2 obtains the next ball is y /( x + y ). This system
has feedback: the more balls a bin has, the more balls it is likely to obtain in the future.
An equivalent problem is given in Exercise1.6. You may wish to check (by induction)
that, if both bins start with one ball and there are n total balls, then the number of balls
in bin 1 is uniformly distributed in the range [1, n −1].
Suppose instead that we strengthen the feedback in the following way. If bin 1 has
x balls and bin 2 has y balls, then the probability that bin 1 obtains the next ball is
xp /( xp + yp ) and the probability that bin 2 obtains the next ball is yp /( xp + yp )for
some p >1. For example, when p =2, if bin 1 has three balls and bin 2 has four balls,
then the probability that the next ball goes into bin 1 is only 9/ 25 < 3 /7. Setting p > 1
strengthens the advantage of the bin with more balls.
This model has been suggested to describe economic situations that result in
monopoly. For example, suppose there are two operating systems, Lindows and Winux.
Users will tend to purchase machines with the same operating system that other users
have in order to maintain compatibility. This effect might be nonlinear in the number
of users of each system; this is modeled by the parameter p.
We now show a remarkable result: as long as p >1, there is some point at which
one bin obtains all the rest of the balls thrown. In the economic setting, this is a very
strong form of monopoly; the other competitor simply stops obtaining new customers.

Theorem 8.6: Under any starting conditions, if p > 1 then with probability 1 there
exists a number c such that one of the two bins gets no more than c balls.

Note the careful wording of the theorem. We are not saying that there is some fixed c
(perhaps dependent on the initial conditions) such that one bin gets no more than c balls.
(If we meant this, we would say that there exists a number c such that, with probabil-
ity 1, one bin gets no more than c balls.) Instead, we are saying that, with probability 1,
at some point (which we do not know ahead of time) one bin stops receiving balls.

Proof: For convenience, assume that both bins start with one ball; this does not affect
the result.

8.3 the exponential distribution
Figure 8.5: In the setup where the time between ball arrivals is exponentially distributed, each bin
can be considered separately; an outcome of the original process is obtained by simply combining
the timelines of the two bins.

We start by considering a very closely related process. Consider two bins that start
with one ball at time 0. Balls arrive at each of the bins. If bin 1 obtains its z th ball
at time t then it obtains its next ball at a time t + Tz , where Tz is a random variable
exponentially distributed with parameter zp. Similarly, if bin 2 obtains its z th ball at
time t then it obtains its next ball at a time t + Uz , where Uz is also a random variable
exponentially distributed with parameter zp. All values of Tz and Uz are independent.
Each bin can be considered independently in this setup; what happens at one bin does
not affect the other.
Although this process may not seem related to the original problem, we now claim
that it mimics it exactly. Consider the point at which a ball arrives, leaving x balls in
bin 1 and y balls in bin 2. By the memoryless nature of the exponential distribution,
it does not matter which bin the most recently arrived ball has landed in; the time for
the next ball to land in bin 1 is exponentially distributed with mean x − p and the time
for the next ball to land in bin 2 is exponentially distributed with mean y − p. Moreover,
by Lemma8.5, the next ball lands in bin 1 with probability xp /( xp + yp ) and in bin 2
with probability yp /( xp + yp ). Therefore, this setup mimics exactly what happens in
the original problem. See Figure8.5.
Let us define the saturation time F 1 forbin1by F 1 =

∑∞
∑∞ j =^1 Tj , and similarly F^2 =
j = 1 Uj. The saturation time represents the first time in which the total number of balls
received by a bin is unbounded. It is not clear that saturation times are well-defined
random variables: What if the sum does not converge, and thus its value is infinity? It
is here that we make use of the fact that p >1. We have

E [ F 1 ]= E
⎡
⎣
∑∞
j = 1
Tj
⎤
⎦=
∑∞
j = 1
E [ Tj ]=
∑∞
j = 1
1
jp
.
Here we used linearity of expectations for a countably infinite summation of random
variables, which holds if

∑∞
j = 1 E [| Tj |] converges. (Chapter^2 discusses the applicabil-
ity of the linearity of expectations to countably infinite summations; see in particular
Exercise2.29.) It suffices to show that

∑∞
j = 11 / j
p converges to a finite number when-
ever p >1. This follows from bounding the summation by the appropriate integral:

∑∞
j = 1
1
jp
≤ 1 +
∫∞
u = 1
1
up
du = 1 +
1
p − 1
.
continuous distributions and the poisson process
Indeed, all of the integral moments converge to a finite number. It follows that both F 1
and F 2 are, with probability 1, finite and hence well-defined.
Furthermore, F 1 and F 2 are distinct with probability 1. To see this, suppose that the
values for all of the random variables Tz and Uz are given except for T 1. Then, for F 1 to
equal F 2 , it must be the case that

T 1 =
∑∞
j = 1
Uj −
∑∞
j = 2
Tj.
But the probability that T 1 takes on any specific value is 0, just as the probability that
our roulette wheel takes on any specific value is 0. Hence, F 1 = F 2 with probability 1.
Suppose that F 1 < F 2. Then we must have for some n that
∑ n

j = 1
Uj < F 1 <
∑ n +^1
j = 1
Uj.
This implies that, for any sufficiently large number m ,

∑ n
j = 1
Uj <
∑ m
i = 1
Ti <
∑ n +^1
j = 1
Uj ,
which means that bin 1 has obtained m balls before bin 2 has obtained its ( n +1)th
ball. Since our new process corresponds exactly to the original balls-and-bins process,
this is also what happens in the original process. But this means that, once bin 2 has
n balls, it does not receive any others; they all go to bin 1. The argument is the same
if F 2 < F 1. Hence, with probability 1, there exists some n such that one bin obtains no
more than n balls.  /theta

When p is close to 1 or when the bins start with a large and nearly equal number
of balls, it can take a long time before one bin dominates enough to obtain such a
monopoly. On the other hand, monopoly happens quickly when p is much greater than
1 (such as p =2) and the bins start with just one ball each. You are asked to simulate
this process in Exercise8.25.

8.4 The Poisson Process
The Poisson process is an important counting process that is related to both the uniform
and the exponential distribution. Consider a sequence of random events, such as arrivals
of customers to a queue or emissions of alpha particles from a radioactive material. Let
N ( t ) denote the number of events in the interval [0, t ]. The process{ N ( t ), t ≥ 0 }is a
stochastic counting process.

Definition 8.4: A Poisson process with parameter (or rate) λ is a stochastic counting
process { N ( t ) ,t ≥ 0 } such that the following statements hold.

1. N (0)= 0_._
2. The process has independent and stationary increments. That is, for anyt , s > 0 , the
distribution of N ( t + s )− N ( s ) is identical to the distribution of N ( t ) , and for any

8.4 the poisson process
two disjoint intervals [ t 1 , t 2 ] and [ t 3 , t 4 ] , the distribution of N ( t 2 )− N ( t 1 ) is inde-
pendent of the distribution of N ( t 4 )− N ( t 3 ).
3. lim t → 0 Pr( N ( t )=1)/ t =λ .Thatis, theprobability ofasingleeventinashortinter-
val t tends to λ t.
4. lim t → 0 Pr( N ( t )≥2)/ t = 0 .Thatis,theprobabilityofmorethanoneeventinashort
interval t tends to zero.

The surprising fact is that this set of broad, relatively natural conditions defines a unique
process. In particular, the number of events in a given time interval follows the Poisson
distribution defined in Section5.3.

Theorem 8.7: Let { N ( t )| t ≥ 0 } beaPoisson processwithparameter λ .Foranyt , s ≥
0 and any integer n ≥ 0 ,

Pn ( t )=Pr( N ( t + s )− N ( s )= n )=e−λ t
(λ t ) n
n!
.
Proof: We first observe that Pn ( t ) is well-defined since, by the second property of Def-
inition8.4, the distribution of N ( t + s )− N ( s ) depends only on t and is independent
of s.
To compute P 0 ( t ), we note that the number of events in the intervals [0, t ] and
( t , t + h ] are independent random variables and therefore

P 0 ( t + h )= P 0 ( t ) P 0 ( h ).
We now write
P 0 ( t + h )− P 0 ( t )
h

= P 0 ( t )
P 0 ( h )− 1
h
= P 0 ( t )
1 −Pr( N ( h )=1)−Pr( N ( h )≥2)− 1
h
= P 0 ( t )
−Pr( N ( h )=1)−Pr( N ( h )≥2)
h
.
Taking the limit as h →0 and applying properties 2–4 of Definition8.4, we obtain

P 0 ′( t )= h lim→ 0
P 0 ( t + h )− P 0 ( t )
h
=lim
h → 0
P 0 ( t )
−Pr( N ( h )=1)−Pr( N ( h )≥2)
h
=−λ P 0 ( t ).
To solve

P 0 ′( t )=−λ P 0 ( t ),
we rewrite it as

P 0 ′( t )
P 0 ( t )
=−λ.
Integrating with respect to t gives

ln P 0 ( t )=−λ t + C ,
continuous distributions and the poisson process
or

P 0 ( t )=e−λ t + C.
Since P 0 (0)=1, we conclude that

P 0 ( t )=e−λ t. (8.1)
For n ≥1, we write

Pn ( t + h )=
∑ n
k = 0
Pn − k ( t ) Pk ( h )
= Pn ( t ) P 0 ( h )+ Pn − 1 ( t ) P 1 ( h )+
∑ n
k = 2
Pn − k ( t )Pr( N ( h )= k ).
Computing the first derivative of Pn ( t ) yields

Pn ′( t )=lim h → 0
Pn ( t + h )− Pn ( t )
h
=lim
h → 0
1
h
(
Pn ( t )( P 0 ( h )−1)+ Pn − 1 ( t ) P 1 ( h )+
∑ n
k = 2
Pn − k ( t )Pr( N ( h )= k )
)
=−λ Pn ( t )+λ Pn − 1 ( t ),
where we use the facts that

lim
h → 0
P 1 ( h )
h
=λ
(by properties 2 and 3) and

0 ≤lim
h → 0
1
h
∑ n
k = 2
Pn − k ( t )Pr( N ( h )= k )≤lim
h → 0
Pr( N ( h )≥2)
h
= 0
(by property 4), so

lim h → 0
1
h
∑ n
k = 2
Pn − k ( t )Pr( N ( h )= k )= 0.
To solve

Pn ′( t )=−λ Pn ( t )+λ Pn − 1 ( t )
we write

eλ t ( Pn ′( t )+λ Pn ( t ))=eλ t λ Pn − 1 ( t ),
which gives

d
dt
(eλ tPn ( t ))=λeλ tPn − 1 ( t ). (8.2)
Using Eqn. (8.1) then yields

d
dt
(eλ tP 1 ( t ))=λeλ tP 0 ( t )=λ,
8.4 the poisson process
implying

P 1 ( t )=(λ t + c )e−λ t.
Since P 1 (0)=0, we conclude that

P 1 ( t )=λ t e−λ t. (8.3)
We continue by induction on n to prove that, for all n ≥0,
Pn ( t )=e−λ t
(λ t ) n
n!
Using Eqn. (8.2) and the induction hypothesis, we have

d
dt
(eλ tPn ( t ))=λeλ tPn − 1 ( t )=
λ ntn −^1
( n −1)!
Integrating and using the fact that Pn (0)=0 gives the result.  /theta

The parameterλis also called the rate of the Poisson process, since (as we have proved)
the number of events during any time period of length t is a Poisson random variable
with expectationλ t.
The reverse is also true. That is, we could equivalently have defined the Poisson
process as a process with Poisson arrivals, as follows.

Theorem 8.8: Let { N ( t )| t ≥ 0 } be a stochastic process such that:

1. N (0)= 0 ;
2. the process has independent increments (i.e., the number of events in disjoint time
intervals are independent events); and
3. the number of events in an interval of length t has a Poisson distribution with mean
λ t.

Then { N ( t )| t ≥ 0 } is a Poisson process with rate λ.

Proof: The process clearly satisfies conditions 1 and 2 of Definition8.4. To prove
condition 3, we have

lim
t → 0
Pr( N ( t )=1)
t
=lim
t → 0
e−λ t λ t
t
=λ.
Condition 4 follows from

lim
t → 0
Pr( N ( t )≥2)
t
=
∑
k ≥ 2
e−λ t (λ t ) k
k! t
= 0.
 /theta
8.4.1 Interarrival Distribution
Let X 1 be the time of the first event of the Poisson process, and let Xn be the interval of
time between the ( n −1)th and the n th event. The Xn are generally referred to as inter-
arrival times, since they represent the time between arrivals of events. Here, we show
that all of the Xn have the same distribution and that this distribution is exponential.

continuous distributions and the poisson process
We begin by deriving the distribution of X 1.
Theorem 8.9: X 1 has an exponential distribution with parameter λ.

Proof:

Pr( X 1 > t )=Pr( N ( t )=0)=e−λ t.
Thus,

F ( X 1 )= 1 −Pr( X 1 > t )= 1 −e−λ t.  /theta
Using the fact that the Poisson process has independent and stationary increments, we
can prove the following stronger result.

Theorem 8.10: The random variables Xi,i = 1 , 2 ,... , are independent, identically
distributed, exponential random variables with parameter λ.

Proof: The distribution of Xi is given by

Pr( Xi > ti |( X 0 = t 0 )∩( X 1 = t 1 )∩···∩( Xi − 1 = ti − 1 ))
=Pr
(
N
( i
∑
k = 0
tk
)
− N
( i − 1
∑
k = 0
tk
)
= 0
)
=e−λ ti.
Thus, the distribution of Xi is exponential with parameterλ, and it is independent of
other interarrival values.  /theta

Theorem8.10states that, if we have a Poisson arrival process, then the interarrival times
are identically distributed exponential random variables. In fact, it is easy to check that
the reverse is also true (this is left as Exercise8.17).

Theorem 8.11: Let { N ( t )| t ≥ 0 } be a stochastic process such that :

1. N (0)= 0 ; and
2. the interarrival times are independent, identically distributed, exponential random
variables with parameter λ.

Then { N ( t )| t ≥ 0 } is a Poisson process with rate λ.
8.4.2. Combining and Splitting Poisson Processes
The correspondence between Poisson processes and exponentially distributed inter-
arrival times is quite useful in proving facts about the behavior of Poisson processes.
One immediate fact is that Poisson processes combine in a natural way. We say that
two Poisson processes N 1 ( t ) and N 2 ( t )are independent if and only if the values N 1 ( x )
and N 2 ( y ) are independent for any x and y. Let N 1 ( t )+ N 2 ( t ) denote the process that
counts the number of events corresponding to both of the processes N 1 ( t ) and N 2 ( t ).
We show that, if N 1 ( t ) and N 2 ( t ) are independent Poisson processes, then they combine
to form a Poisson process N 1 ( t )+ N 2 ( t ).

8.4 the poisson process
Theorem 8.12: Let N 1 ( t ) and N 2 ( t ) be independent Poisson processes with parame-
ters λ 1 and λ 2 , respectively. Then N 1 ( t )+ N 2 ( t ) is a Poisson process with parameter
λ 1 +λ 2 , and each event of the process N 1 ( t )+ N 2 ( t ) arises from the process N 1 ( t ) with
probability λ 1 /(λ 1 +λ 2 ).

Proof: Clearly N 1 (0)+ N 2 (0)=0, and since the two processes are independent and
each has independent increments, the sum of the two processes also has independent
increments. The number of arrivals N 1 ( t )+ N 2 ( t ) is a sum of two independent Poisson
random variables, which (as we saw in Lemma5.2) has a Poisson distribution with
parameterλ 1 +λ 2. Thus, by Theorem8.8, N 1 ( t )+ N 2 ( t ) is a Poisson process with rate
λ 1 +λ 2.
By Theorem8.9, the interarrival time for N 1 ( t )+ N 2 ( t ) is exponentially distributed
with parameterλ 1 +λ 2 , and by Lemma8.5an event in N 1 ( t )+ N 2 ( t ) comes from the
process N 1 ( t ) with probabilityλ 1 /(λ 1 +λ 2 ).  /theta

The theorem extends to more than two processes by induction.
It is interesting to note that Poisson processes can be split as well as combined. If
we split a Poisson process with rateλby labeling each event as being either type 1
with probability p or type 2 with probability (1− p ), then it seems that we should get
two Poisson processes with ratesλ p andλ(1− p ). In fact, we can say something even
stronger: the two processes will be independent.

Theorem 8.13: Suppose that we have a Poisson process N ( t ) with rate λ. Each event
is independently labeled as being type 1 with probability p or type 2 with probabil-
ity 1 − p. Then the type-1 events form a Poisson process N 1 ( t ) of rate λ p, the type-2
events form a Poisson process N 2 ( t ) of rate λ(1− p ) , and the two Poisson processes
are independent.

Proof: We first show that the type-1 events in fact form a Poisson process. Clearly
N 1 ( t )=0, and since the process N ( t ) has independent increments, so does the process
N 1 ( t ). Next we show that N 1 ( t ) has a Poisson distribution:

Pr( N 1 ( t )= k )=
∑∞
j = k
Pr( N 1 ( t )= k | N ( t )= j )Pr( N ( t )= j )
=
∑∞
j = k
(
j
k
)
pk (1− p ) j − k
e−λ t (λ t ) j
j!
=
e−λ pt (λ pt ) k
k!
∑∞
j = k
e−λ t (1− p )(λ t (1− p )) j − k
( j − k )!
=
e−λ pt (λ pt ) k
k!
.
Thus, by Theorem8.8, N 1 ( t ) is a Poisson process with rateλ p.
To show independence, we need to show that N 1 ( t ) and N 2 ( u ) are independent for
any t and u. In fact, it suffices to show that N 1 ( t ) and N 2 ( t ) are independent for any t ;

continuous distributions and the poisson process
we can then show that N 1 ( t ) and N 2 ( u ) are independent for any t and u by taking advan-
tage of the fact that Poisson processes have independent and stationary increments (see
Exercise8.18). We have:

Pr(( N 1 ( t )= m )∩( N 2 ( t )= n ))=Pr(( N ( t )= m + n )∩( N 2 ( t )= n ))
=
e−λ t (λ t ) m + n
( m + n )!
(
m + n
n
)
pm (1− p ) n
=
e−λ t (λ t ) m + n
m! n!
pm (1− p ) n
=
e−λ tp (λ tp ) m
m!
e−λ t (1− p )(λ t (1− p )) n
n!
=Pr( N 1 ( t )= m )Pr( N 2 ( t )= n ).  /theta
8.4.3 Conditional Arrival Time Distribution
We have used the fact that a Poisson process has independent increments to show
that the distribution of the interarrival times is exponential. Another application of this
assumption is the following: If we condition on exactly one event occurring in an inter-
val, then the actual time at which that event occurs is uniformly distributed over that
interval. To see this, consider a Poisson process where N ( t )=1, and consider the time
X 1 of the single event that falls in the interval (0, t ]:

Pr( X 1 < s | N ( t )=1)=
Pr(( X 1 < s )∩( N ( t )=1))
Pr( N ( t )=1)
=
Pr(( N ( s )=1)∩( N ( t )− N ( s )=0))
Pr( N ( t )=1)
=
(λ s e−λ s )e−λ( t − s )
λ t e−λ t
=
s
t
.
Here we have used the independence of N ( s ) and N ( t )− N ( s ).
To generalize this to the case of N ( t )= n , we use the concept of order statistics. Let
X 1 ,..., Xn be n independent observations of a random variable. The order statistics of
X 1 ,..., Xn consists of the n observations in (increasing) sorted order. For example, if
X 1 , X 2 , X 3 , X 4 are independent random variables generated by taking a number chosen
uniformly on [0,1] and rounding to two decimal places, we might have X 1 = 0 .47,
X 2 = 0 .33, X 3 = 0 .93, and X 4 = 0 .26. The corresponding order statistics, where Y ( i )
is used to refer to the i th smallest, would be Y (1)= 0 .26, Y (2)= 0 .33, Y (3)= 0 .47, and
Y (4)= 0 .93.

Theorem 8.14: Given that N ( t )= n, the n arrival times have the same distribution as
the order statistics of n independent random variables with uniform distribution over
[0, t ].

8.4 the poisson process
Proof: We first compute the distribution of the order statistics of n independent obser-
vations X 1 , X 2 ,..., Xn drawn from a uniform distribution in [0, t ]. Let Y (1),..., Y ( n )
denote the order statistics.
We want an expression for

Pr( Y (1)≤ s 1 , Y (2)≤ s 2 ,..., Y ( n )≤ sn ).
LetEbe the event that

Y (1)≤ s 1 , Y (2)≤ s 2 ,..., Y ( n )≤ sn.
For any permutation i 1 , i 2 ,..., in of the numbers from 1 to n , letE i 1 , i 2 ,..., in be the event
that

Xi 1 ≤ s 1 , Xi 1 ≤ Xi 2 ≤ s 2 ,..., Xin − 1 ≤ Xin ≤ sn.
The eventsE i 1 , i 2 ,..., in are disjoint, except for the cases where Xij = Xij + 1 for some j. Since
two uniform random variables are equal with probability 0, the total probability of
such events is 0 and can be ignored. By symmetry, all eventsE i 1 , i 2 ,..., in have the same
probability. Also,

E=
⋃
E i 1 , i 2 ,..., in ,
where the union is over all permutations. It follows that

Pr( Y (1)≤ s 1 , Y (2)≤ s 2 ,..., Y ( n )≤ sn )
=
∑
Pr( Xi 1 ≤ s 1 , Xi 1 ≤ Xi 2 ≤ s 2 ,..., Xin − 1 ≤ Xin ≤ sn )
= n !Pr( X 1 ≤ s 1 , X 1 ≤ X 2 ≤ s 2 ,..., Xn − 1 ≤ Xn ≤ sn ),
where the sum in the second line is over all n! permutations. If we now think of ui as
representing the value taken on by Xi , then

Pr( X 1 ≤ s 1 , X 1 ≤ X 2 ≤ s 2 ,..., Xn − 1 ≤ Xn ≤ sn )
=
∫ s 1
u 1 = 0
∫ s 2
u 2 = u 1
···
∫ sn
un = un − 1
(
1
t
) n
dun ··· du 1 ,
where we use the fact that the density function of a uniform random variable on [0, t ]
is f ( t )= 1 / t. This gives

Pr( Y (1)≤ s 1 , Y (2)≤ s 2 ,..., Y ( n )≤ sn )=
n!
tn
∫ s 1
u 1 = 0
∫ s 2
u 2 = u 1
···
∫ sn
un = un − 1
dun ··· du 1.
We now consider the distribution of the arrival times for a Poisson process, condi-
tioned on N ( t )= n. Let S 1 ,..., Sn + 1 be the first n +1 arrival times. Also, let T 1 = S 1
and Ti = Si − Si − 1 be the length of the interarrival intervals. By Theorem8.10,we
know that (a) without the condition N ( t )= n , the distributions of the random variables
T 1 ,..., Tn are independent, and (b) for each i , Ti has an exponential distribution with
parameterλ. Recalling that the density function of the exponential distribution isλe−λ t ,

continuous distributions and the poisson process
we have

Pr( S 1 ≤ s 1 , S 2 ≤ s 2 ,..., Sn ≤ sn , N ( t )= n )
=Pr
(
T 1 ≤ s 1 , T 2 ≤ s 2 − T 1 ,..., Tn ≤ sn −
∑ n −^1
i = 1
Ti , Tn + 1 > t −
∑ n
i = 1
Ti
)
=
∫ s 1
t 1 = 0
∫ s 2 − t 1
t 2 = 0
···
∫ sn −∑ ni =− 11 ti
tn = 0
∫∞
tn + 1 = t −∑ ni = 1 ti
λ n +^1 e−λ(
∑ n + 1
i = 1 ti ) dtn + 1 ··· dt 1.
Integrating with respect to tn + 1 then yields
∫∞

tn + 1 = t −∑ ni = 1 ti
λ n +^1 e−λ(
∑ n + 1
i = 1 ti ) dtn + 1 =−λ n [e−λ
∑ n + 1
i = 1 ti ]∞ t
n + 1 = t −∑ ni = 1 ti
=λ n e−λ t.
Thus,

Pr( S 1 ≤ s 1 , S 2 ≤ s 2 ,..., Sn ≤ sn , N ( t )= n )
=λ n e−λ t
∫ s 1
t 1 = 0
∫ s 2 − t 1
t 2 = 0
···
∫ sn −∑ ni =− 11 ti
tn = 0
dtn ··· dt 1
=λ n e−λ t
∫ s 1
u 1 = 0
∫ s 2
u 2 = u 1
···
∫ sn
un = un − 1
dun ··· du 1 ,
where the last equation is obtained by substituting ui =

∑ i
j = 1 ti.
Since
Pr( N ( t )= n )=e−λ t
(λ t ) n
n!
and because the number of events in an interval of length t has a Poisson distribution
with parameterλ t , the conditional probability computation gives

Pr( S 1 ≤ s 1 , S 2 ≤ s 2 ,..., Sn ≤ sn | N ( t )= n )
=
Pr( S 1 ≤ s 1 , S 2 ≤ s 2 ,..., Sn ≤ sn , N ( t )= n )
Pr( N ( t )= n )
=
n!
tn
∫ s 1
u 1 = 0
∫ s 2
u 2 = u 1
···
∫ sn
un = un − 1
dun ··· du 1.
This is exactly the distribution function of the order statistics, proving the theorem. /theta

8.5 Continuous Time Markov Processes
In Chapter 7 we studied discrete time and discrete space Markov chains. With the intro-
duction of continuous random variables, we can now study the continuous time ana-
logue of Markov chains, where the process spends a random interval of time in a state
before moving to the next one. To distinguish between the discrete and continuous
processes, when dealing with continuous time we speak of Markov processes.

8.5 continuous time markov processes
Definition 8.5: A continuous time random process { Xt | t ≥ 0 } is Markovian ( or is
called a Markov process) if, for all s , t ≥ 0 :
Pr( X ( s + t )= x | X ( u ), 0 ≤ u ≤ t )=Pr( X ( s + t )= x | X ( t )),
and this probability is independent of the time t.^2
The definition says that distribution of the state of the system at time X ( s + t ), condi-
tioned on the history up to time t , depends only on the state X ( t ) and is independent of
the particular history that led the process to state X ( t ).
Restricting our discussion to discrete space, continuous time Markov processes,
there is another equivalent way of formulating such processes that is more convenient
for analysis. Recall that a discrete time Markov chain is determined by a transition
matrix P =( Pi , j ), where Pi , j is the probability of a transition from state i to state j in
one step. A continuous time Markov process can be expressed as a combination of two
random processes as follows.
1. A transition matrix P =( pi , j ), where pi , j is the probability that the next state is
j given that the current state is i. (We use lowercase letters here for the transition
probabilities in order to distinguish them from the transition probabilities for cor-
responding discrete time processes.) The matrix P is the transition matrix for what
is called the embedded or skeleton Markov chain of the corresponding Markov pro-
cess.
2. A vector of parameters (θ 1 ,θ 2 ,...) such that the distribution of time that the process
spends in state i before moving to the next step is exponential with parameterθ i. The
distribution of time spent at a given state must be exponential in order to satisfy the
memoryless requirement of the Markov process.

A formal treatment of continuous time Markov processes is more involved than their
discrete counterparts, and a full discussion is beyond the scope of this book. We limit
our discussion to the question of computing the stationary distribution (also called
equilibrium distribution ) for discrete space, continuous time processes, assuming that
a stationary distribution exists. As for the discrete time case, the valueπ i in a stationary
distribution ̄πgives the limiting probability that the Markov process will be in state i
infinitely far out in the future, regardless of the initial state. That is, if we let Pj , i ( t )
be the probability of being in state i at time t when starting from state j at time 0,
then
t lim→∞ Pj , i ( t )=π i.
Similarly,π i gives the long-term proportion of the time the process is in state i. Further-
more, if the initial state j is chosen from the stationary distribution, then the probability
of being in state i at time t isπ i for all t.
(^2) Technically, as with the discrete time Markov chains, this is a time-homogeneous Markov process; this will be
the only type we study in this book.

continuous distributions and the poisson process
To determine the stationary distribution, consider the derivative P ′ j , i ( t ):
P ′ j , i ( t )=lim
h → 0
Pj , i ( t + h )− Pj , i ( t )
h
=lim
h → 0
∑
kPj , k ( t ) Pk , i ( h )− Pj , i ( t )
h
=lim
h → 0
⎛
⎝
∑
k = i
Pk , i ( h )
h
Pj , k ( t )−
1 − Pi , i ( h )
h
Pj , i ( t )
⎞
⎠.
Since the distribution of time spent at state k is exponential with parameterθ k ,we
can use the properties of the Poisson process to observe that, as h tends to zero, the
limiting probability of a transition out of state k in an interval of length h is h θ k , and
the limiting probability of more than one transition is 0. Thus,

lim
h → 0
Pk , i ( h )
h
=θ kpk , i.
Similarly, 1− Pi , i ( h ) is the probability that a transition occurs over the interval of time
h , and the transition is not from state i back to itself. Thus,

lim
h → 0
1 − Pi , i ( h )
h
=θ i (1− pi , i ).
We now assume that we can interchange the limit and the summation; we emphasize
that this interchange is not always justified for countably infinite spaces. Subject to this
assumption,

lim
h → 0
⎛
⎝
∑
k = i
Pk , i ( h )
h
Pj , k ( t )−
1 − Pi , i ( h )
h
Pj , i ( t )
⎞
⎠
=
∑
k = i
θ kpk , iPj , k ( t )− Pj , i ( t )(θ i −θ ipi , i )
=
∑
k
θ kpk , iPj , k ( t )−θ iPj , i ( t ).
Taking the limit as t →∞,wehave

t lim→∞ P ′ j , i ( t )= t lim→∞
∑
k
θ kpk , iPj , k ( t )−θ iPi , i ( t )=
∑
k
θ kpk , i π k −θ i π i.
If the process has a stationary distribution, it must be that
t lim→∞ P ′ j , i ( t )=^0.
Otherwise, Pj , i ( t ) would not converge to a stationary value. Hence, in the stationary
distribution ̄πwe have the following rate equations :

π i θ i =
∑
k
π k θ kpk , i. (8.4)
This set of equations has a nice interpretation. The expression on the left,π i θ i , is the
rate at which transitions occur out of state i. The expression on the right,

∑
k π k θ kpk , i ,
8.6 Example: Markovian Queues
is the rate at which transitions occur into state i. (A transition that goes from state i
back to state i is counted both as a transition into and as a transition out of state i .)
At the stationary distribution, these rates must be equal, so that the long-term rates
of transitions into and out of the state are equal. This equalization of rates into and
out of every state provides a simple, intuitive way to find stationary distributions for
continuous Markovian processes. This observation can be generalized to sets of states,
showing that a result similar to the cut-set equations of Theorem7.9for discrete time
Markov chains can be formulated for continuous time Markov processes.
If the exponential distributions governing the time spent in all of the states have the
same parameter, so that all theθ i are equal, then Eqn. (8.4) becomes

π i =
∑
k
π kpk , i.
This corresponds to

π ̄=π ̄ P ,
where P is the transition matrix of the embedded Markov chain. We can conclude that
the stationary distribution of the continuous time process is the same as the stationary
distribution of the embedded Markov chain in this case.

8.6. Example: Markovian Queues
Queues appear in many basic applications in computer science. In operating systems,
schedulers can hold tasks in a queue until the processor or other required resources
are available. In parallel or distributed programming, threads can queue for a critical
section that allows access to only one thread at a time. In networks, packets are queued
while waiting to be forwarded by a router. Even before computer systems were preva-
lent, queues were widely studied to understand the performance of telephone networks,
where similar scheduling issues arise. In this section we analyze some of the most basic
queueing models, which use Poisson processes to model the stochastic process of cus-
tomers arriving at a queue and exponentially distributed random variables to model the
time required for service.
In what follows, we refer to queue models using the standard notation Y / Z / n , where
Y represents the distribution of the incoming stream of customers, Z represents the ser-
vice time distribution, and n represents the number of servers. The standard notation
for a Markovian or memoryless distribution is M. Thus, M / M / n stands for a queue
model with customers arriving according to a Poisson process and served by n servers
having identical and independent exponentially distributed service times. Other queue-
ing models include the M / M /∞model, where there are an infinite number of servers,
and the M / G /1 model, where the G indicates that the service time can be any arbitrary
general distribution.
A queue must also have a rule for determining the order in which customers are
served. Unless otherwise specified, we assume that a queue follows the First In First
Out (FIFO) rule, where customers are served in order of their arrival.

continuous distributions and the poisson process
8.6.1 M / M /1 Queue in Equilibrium
Assume that customers arrive to a queue according to a Poisson process with parameter
λ, and assume they are served by one server. The service times for the customers are
independent and exponentially distributed with parameterμ.
Let M ( t ) be the number of customers in the queue at time t. Since both the arrival
process and the service time have memoryless distributions, the process{ M ( t )| t ≥ 0 }
defines a continuous time Markov process. We consider the stationary distribution for
this process.
Let

Pk ( t )=Pr( M ( t )= k )
denote the probability that the queue has k customers at time t. We use the fact that, in
the limit as h approaches 0, the probability of an arrival (respectively, a departure) over
a time interval isλ h (respectively,μ h ). Thus,

dP 0 ( t )
dt
=lim h → 0
P 0 ( t + h )− P 0 ( t )
h
=lim
h → 0
P 0 ( t )(1−λ h )+ P 1 ( t )μ h − P 0 ( t )
h
=−λ P 0 ( t )+μ P 1 ( t ), (8.5)
and for k ≥1,

dPk ( t )
dt
=lim
h → 0
Pk ( t + h )− Pk ( t )
h
= h lim→ 0
Pk ( t )(1−λ h −μ h )+ Pk − 1 ( t )λ h + Pk + 1 ( t )μ h − Pk ( t )
h
=−(λ+μ) Pk ( t )+λ Pk − 1 ( t )+μ Pk + 1 ( t ). (8.6)
In equilibrium,

dPk ( t )
dt
=0for k = 0 , 1 , 2 ,....
If the system converges to a stationary distribution^3 π ̄, then applying Eqn. (8.5)
yields

μπ 1 =λπ 0.
This equation has a simple interpretation in terms of rates. In equilibrium, the rate into
the state where there are no customers in the queue isμπ 1 ;the rate out isλπ 0. These
two rates must be equal. If we write this asπ 1 =π 0 (λ/μ), then Eqn. (8.6) and a simple
induction give

π k =π k − 1
(
λ
μ
)
=π 0
(
λ
μ
) k
.
(^3) Again, the proof that the system indeed converges relies on renewal theory and is beyond the scope of this book.

8.6example: markovian queues
Since

∑
k ≥ 0 π k =1, we must have
π 0
∑
k ≥ 0
(
λ
μ
) k
= 1. (8.7)
Assuming thatλ<μ, it follows that

π 0 = 1 −
λ
μ
and π k =
(
1 −
λ
μ
)(
λ
μ
) k
.
Ifλ>μ, then the summation in Eqn. (8.7) does not converge and, in fact, the system
does not reach a stationary distribution. This is intuitively clear; if the rate of arrival of
new customers is larger than the rate of service completions, then the system cannot
reach a stationary distribution. Ifλ=μ, the system also cannot reach an equilibrium
distribution, as discussed in Exercise8.23.
To compute the expected number of customers in the system in equilibrium, which
we denote by L , we write

L =
∑∞
k = 0
k π k
=
λ
μ
∑∞
k = 1
k
(
1 −
λ
μ
)(
λ
μ
) k − 1
=
λ
μ
1
1 −λ/μ
=
λ
μ−λ
,
where in the third equation we used the fact that the sum is the expectation of a geo-
metric random variable with parameter 1−λ/μ.
It is interesting that we have nowhere used the fact that the service rule was to serve
the customer that had been waiting the longest. Indeed, since all service times are expo-
nentially distributed and since the exponential distribution is memoryless, all customers
appear equivalent to the queue in terms of the distribution of the service time required
until they leave, regardless of how long they have already been served. Thus, our equa-
tions for the equilibrium distribution and the expected number of customers in the sys-
tem hold for any service rule that serves some customer whenever at least one customer
is in the queue.
Next we compute the expected time a customer spends in the system when the sys-
tem is in equilibrium, denoted by W , assuming a FIFO queue. Let L ( k ) denote the event
that a new customer finds k customers in the queue. We can write

W =
∑∞
k = 0
E [ W | L ( k )] Pr( L ( k )).
continuous distributions and the poisson process
Since the service times are independent, memoryless, and have expectation 1/μ,it
follows that

E [ W | L ( k )]=( k +1)
1
μ
.
To compute Pr( L ( k )), we observe that if the system is in equilibrium then the rate
of transitions out of state k isπ k θ k , whereθ 0 =λandθ k =λ+μfor k ≥1. Applying
Lemma8.5, the probability that the next transition from state k is caused by the arrival
of a new customer isλ/θ k. Therefore, the rate at which customers arrive and find k
customers already in the queue is

π k θ k
λ
θ k
=π k λ.
Since the total rate of new arrivals to the system isλ, we conclude that the probability
that a new arrival finds k customers in the system is

Pr( L ( k ))=
π k λ
λ
=π k.
This is an example of the PASTA principle, which states that Poisson Arrivals See
Time Averages. That is, if a Markov process with Poisson arrivals has a stationary
distribution and if the fraction of time the system is in state k isπ k , thenπ k is also
the proportion of arrivals that find the system in state k when they arrive. The PASTA
principle, which is due to the independence and memoryless properties of the Poisson
process, is a useful tool that often simplifies analysis. A proof of the PASTA principle
for more general situations is beyond the scope of this book.
We can now compute

W =
∑∞
k = 0
E [ W | L ( k )] Pr( L ( k ))
=
∑∞
k = 0
k + 1
μ
π k
=
1
μ
(
1 +
∑∞
k = 0
k π k
)
=
1
μ
(1+ L )
=
1
μ
(
1 +
λ
μ−λ
)
=
1
μ−λ
=
L
λ
.
The relationship L =λ W is known as Little’s result, and it holds not only for M / M / 1
queues but for any stable queueing system. The proof of this fundamental result is
beyond the scope of this book.

8.6example: markovian queues
Although the M / M /1 queue represents a very simple process, it can be useful for
studying more complicated processes. For example, suppose that we have several types
of customers entering a queue, with each type arriving according to a Poisson process,
and that all customers have exponentially distributed service times of meanμ. Since
Poisson processes combine, the arrival process to the queue is Poisson, and this can be
modeled as an M / M /1 queue. Similarly, suppose that we have a single Poisson arrival
process, and we establish a separate queue for each type of customer. If each arriving
customer is of type i with some fixed probability pi , then the Poisson process splits
into independent Poisson processes for each type of customer, and hence the queue for
each type is an M / M /1 queue. This type of splitting might occur, for example, if we
use separate processors for different types of jobs in a computer network.

8.6.2 M / M / 1 / K Queue in Equilibrium
An M / M / 1 / K queue is an M / M /1 queue with bounded queue size. If a customer arrives
while the queue already has K customers, then this customer leaves the system instead
of joining the queue. Models with bounded queue size are useful for applications such
as network routers, where packets that arrive once the packet buffer is full must be
dropped.
The system is entirely similar to the previous example. In equilibrium we have

π k =
{
π 0 (λ/μ) k for k ≤ K ,
0for k > K ,
and

π 0 =
1
∑ K
k = 0 (λ/μ) k
These equations define a proper probability distribution for anyλ, μ >0, and we no
longer require thatλ<μ.

8.6.3 The Number of Customers in an M / M /∞Queue
Suppose new users join a peer-to-peer network according to a Poisson process with rate
λ. The length of time a user stays connected to the network has exponential distribution
with parameterμ. Assume that, at time 0, no users were connected to the network. Let
M ( t ) be the number of connected users at time t. What is the distribution of M ( t )?
We can view this process as a Markovian queue with an unlimited number of servers.
A customer starts being served the moment she joins the system and leaves when she
is done. We demonstrate two ways of analyzing this process. We first use the rate equa-
tions (8.4) to compute the stationary distribution for the process. The second approach
is more complex, but it yields more information: we explicitly compute the distribution
of the number of customers in the system at time t and then consider the limit as t goes
to infinity.
To write the rate equations of the process, we observe that if (at a given time) there
are k ≥0 customers in the system, then the next event can be either termination of

continuous distributions and the poisson process
service of one of the k current customers or the arrival of a new customer. Thus, the
time to the first event is the minimum of k +1 independent exponentially distributed
random variables; k of these variables have parameterμ, and one has parameterλ.
Applying Lemma8.5shows that, when there are k customers in the system, the time
to the first event has an exponential distribution with parameter k μ+λ. Furthermore,
the lemma implies that, given that an event occurs, the probability that the event is an
arrival of a new customer is

pk , k + 1 =
λ
λ+ k μ
,
and when k ≥1 the probability that the event is the departure of a customer is

pk , k − 1 =
k μ
λ+ k μ
.
Plugging these values into (8.4), we have that the stationary distribution ̄πsatisfies
π 0 λ=π 1 μ
and, for k ≥1,

π k (λ+ k μ)=π k − 1 λ+π k + 1 ( k +1)μ. (8.8)
We rewrite (8.8)as

π k + 1 ( k +1)μ=π k (λ+ k μ)−π k − 1 λ
=π k λ+π kk μ−π k − 1 λ.
A simple induction yields that

π kk μ=π k − 1 λ,
and therefore

π k + 1 =
λ
μ( k +1)
π k.
Now, again a simple induction yields

π k =π 0
(
λ
μ
) k
1
k!
,
and therefore

1 =
∑∞
k = 0
π 0
(
λ
μ
) k
1
k!
=π 0 eλ/μ.
We conclude thatπ 0 =e−λ/μand, more generally,

π k =
e−λ/μ(λ/μ) k
k!
,
so that the equilibrium distribution is the discrete Poisson distribution with parameter
λ/μ.

8.6example: markovian queues
We now proceed with our second approach: computing the distribution of the num-
ber of customers in the system at time t , denoted by M ( t ), and then considering the limit
of M ( t )as t goes to infinity. Let N ( t ) be the total number of users that have joined the
network in the interval [0, t ]. Since N ( t ) has a Poisson distribution, we can condition
on this value and write

Pr( M ( t )= j )=
∑∞
n = 0
Pr( M ( t )= j | N ( t )= n )e−λ t
(λ t ) n
n!
. (8.9)
If a user joins the network at time x , then the probability that she is still connected
at time t is e−μ( t − x ). From Section8.4.3, we know that the arrival time of an arbitrary
user is uniform on [0, t ]. Thus, the probability that an arbitrary user is still connected
at time t is given by

p =
∫ t
0
e−μ( t − x )
dx
t
=
1
μ t
(1−e−μ t ).
Because the events for different users are independent, for j ≤ n we have
Pr( M ( t )= j | N ( t )= n )=
(
n
j
)
pj (1− p ) n − j.
Plugging this value into Eqn. (8.9), we find that

Pr( M ( t )= j )=
∑∞
n = j
(
n
j
)
pj (1− p ) n − j e−λ t
(λ t ) n
n!
=e−λ t
(λ tp ) j
j!
∑∞
n = j
(λ t (1− p )) n − j
( n − j )!
=e−λ t
(λ tp ) j
j!
∑∞
m = 0
(λ t (1− p )) m
m!
=e−λ t
(λ tp ) j
j!
eλ t (1− p )
=e−λ tp
(λ tp ) j
j!
.
Thus, the number of users at time t has a Poisson distribution with parameterλ tp.
Since

lim
t →∞
λ tp =lim
t →∞
λ t
1
μ t
(1−e−μ t )=
λ
μ
,
it follows that, in the limit, the number of customers has a Poisson distribution with
parameterλ/μ, matching our previous calculation.

continuous distributions and the poisson process
8.7 Exercises
Exercise 8.1: Let X and Y be independent, uniform random variables on [0,1]. Find
the density function and distribution function for X + Y.

Exercise 8.2: Let X and Y be independent, exponentially distributed random variables
with parameter 1. Find the density function and distribution function for X + Y.

Exercise 8.3: Let X be a uniform random variable on [0,1]. Determine Pr( X ≤ 1 / 2 |
1 / 4 ≤ X ≤ 3 /4) and Pr( X ≤ 1 / 4 |( X ≤ 1 /3)∪( X ≥ 2 /3)).

Exercise 8.4: We agree to try to meet between 12 and 1 for lunch at our favorite
sandwich shop. Because of our busy schedules, neither of us is sure when we’ll arrive;
we assume that, for each of us, our arrival time is uniformly distributed over the hour.
So that neither of us has to wait too long, we agree that we will each wait exactly 15
minutes for the other to arrive, and then leave. What is the probability we actually meet
each other for lunch?

Exercise 8.5: In Lemma8.3, we found the expectation of the smallest of n independent
uniform random variables over [0,1] by directly computing the probability that it was
larger than y for 0≤ y ≤1. Perform a similar calculation to find the probability that
the k th smallest of the n random variables is larger than y , and use this to show that its
expected value is k /( n +1).

Exercise 8.6: Let X 1 , X 2 ,..., Xn be independent exponential random variables with
parameter 1. Find the expected value of the k th largest of the n random variables.

Exercise 8.7: Consider a complete graph on n vertices. Each edge is assigned a
weight chosen independently and uniformly at random from the real interval [0,1].
Show that the expected weight of the minimum spanning tree of this graph is at least
1 − 1 /

(
1 +
( n
2
))
. Find a similar bound when each edge is independently assigned a
weight from an exponential distribution with parameter 1.

Exercise 8.8: Consider a complete graph on n vertices. Each edge is assigned a weight
chosen independently and uniformly at random from the real interval [0,1]. We propose
the following greedy method for finding a small-weight Hamiltonian cycle in the graph.
At each step, there is a head vertex. Initially the head is vertex 1. At each step, we find
the edge of least weight between the current head vertex and a new vertex that has
never been the head. We add this edge to the cycle and set the head vertex to the new
vertex. After n −1 steps, we have a Hamiltonian path, which we complete to make
a Hamiltonian cycle by adding the edge from the last head vertex back to vertex 1.
What is the expected weight of the Hamiltonian cycle found by this greedy approach?
Also, find the expectation when each edge is independently assigned a weight from an
exponential distribution with parameter 1.

Exercise 8.9: You would like to write a simulation that uses exponentially dis-
tributed random variables. Your system has a random number generator that produces

8.7 exercises
independent, uniformly distributed numbers from the real interval (0,1). Give a proce-
dure that transforms a uniform random number as given to an exponentially distributed
random variable with parameterλ.
Exercise 8.10: Let n points be placed uniformly at random on the boundary of a circle
of circumference 1. These n points divide the circle into n arcs. Let Zi for 1≤ Zi ≤ n
be the length of these arcs in some arbitrary order.
(a) Prove that all Zi are at most c ln n /( n −1) with probability at least 1− 1 / nc −^1.
(b) Prove that, for sufficiently large n , there exists a constant c ′such that at least one
Zi is at least c ′ln n with probability at least 1/2. ( Hint: Use the second moment
method.)
(c) Prove that all Zi are at least 1/ 2 n^2 with probability at least 1/2.
(d) Prove that, for sufficiently large n , there exists a constant c ′such that at least one
Zi is at most c ′/ n^2 with probability at least 1/2. ( Hint: Use the second moment
method.)
(e) Explain how these results relate to the following problem: X 1 , X 2 ,..., Xn − 1 are
values chosen independently and uniformly at random from the real interval [0,1].
We let Y 1 , Y 2 ,..., Yn − 1 represent these values in increasing sorted order, and we
also define Y 0 =0 and Yn =1. The points Yi break the unit interval into n segments.
What can we say about the shortest and longest of these segments?

Exercise 8.11: Bucket sort is a simple sorting algorithm discussed in Section5.2.2.
(a) Explain how to implement Bucket sort so that its expected running time is O ( n )
when the n elements to be sorted are independent, uniform random numbers that
are chosen from [0,1].
(b) We now consider how to implement Bucket sort when the elements to be sorted
are not necessarily uniform over an interval. Specifically, suppose the elements to
be sorted are numbers of the form X + Y , where (for each element) X and Y are
independent, uniform random numbers chosen from [0,1]. How can you modify
the buckets so that Bucket sort still has expected running time O ( n )? What if the
elements to be sorted were numbers of the form max( X , Y ) instead of X + Y?
Exercise 8.12: Let n points be placed uniformly at random on the boundary of a circle
of circumference 1. These n points divide the circle into n arcs. Let Zi for 1≤ Zi ≤ n
be the length of these arcs in some arbitrary order, and let X be the number of Zi that
are at least 1/ n. Find E [ X ] and Var [ X ].
Exercise 8.13: A digital camera needs two batteries. You buy a pack of n batteries,
labeled 1 to n. Initially, you install batteries 1 and 2. Whenever a battery is drained,
you immediately replace the drained battery with the lowest numbered unused battery.
Assume that each battery lasts for an amount of time that is exponentially distributed
with meanμbefore being drained, independent of all other batteries. Eventually, all
the batteries but one will be drained.
continuous distributions and the poisson process
(a) Find the probability that the battery numbered i is the one that is not eventually
drained.
(b) Find the expected time your camera will be able to run with this pack of batteries.

Exercise 8.14: Let X 1 , X 2 ,...be exponential random variables with parameter 1.

(a) Argue that X 1 + X 2 is not an exponential random variable.
(b) Let N be a geometric random variable with parameter p. Prove that

∑ N
i = 1 Xi is
exponentially distributed with parameter p.
Exercise 8.15: (a) Let X 1 , X 2 ,...be a sequence of independent exponential random
variables, each with mean 1. Given a positive real number k , let N be defined by

N =min
{
n :
∑ n
i = 1
Xi > k
}
.
That is, N is the smallest number for which the sum of the first N of the Xi is larger than
k. Determine E [ N ].
(b) Let X 1 , X 2 ,...be a sequence of independent uniform random variables on the
interval (0,1). Given a positive real number k with 0< k <1, let N be defined by

N =min
{
n :
∏ n
i = 1
Xi < k
}
.
That is, N is the smallest number for which the product of the first N of the Xi is smaller
than k. Determine E [ N ]. ( Hint: You may find Exercise8.9helpful.)

Exercise 8.16: There are n tasks that are given to n processors. Each task has two
phases, and the time for each phase is given by an exponentially distributed random
variable with parameter 1. The times for all phases and for all tasks are independent.
We say that a task is half-done if it has finished one of its two phases.

(a) Derive an expression for the probability that there are k tasks that are half-done at
the instant when exactly one task becomes completely done.
(b) Derive an expression for the expected time until exactly one task becomes com-
pletely done.
(c) Explain how this problem is related to the birthday paradox.

Exercise 8.17: Prove Theorem8.11.

Exercise 8.18: Complete the proof of Theorem8.13by showing formally that, if N 1 ( t )
and N 2 ( t ) are independent, then so are N 1 ( t ) and N 2 ( u ) for any t , u >0.

Exercise 8.19: You are waiting at a bus stop to catch a bus across town. There are
actually n different bus lines you can take, each following a different route. Which bus
you decide to take will depend on which bus gets to the bus stop first. As long as you are
waiting, the time you have to wait for a bus on the i th line is exponentially distributed
with meanμ i minutes. Once you get on a bus on the i th line, it will take you ti minutes
to get across town.

8.7 exercises
Figure 8.6: The Ehrenfest model.
Design an algorithm for deciding – when a bus arrives – whether or not you should
get on the bus, assuming your goal is to minimize the expected time to cross town.
( Hint: You want to determine the set of buses that you want to take as soon as they
arrive. There are 2 n possible sets, which is far too large for an efficient algorithm. Argue
that you need only consider a small number of these sets.)

Exercise 8.20: Given a discrete space, continuous time Markov process X ( t ), we can
derive a discrete time Markov chain Z ( t ) by considering the states the process visits.
That is, let Z (0)= X (0), let Z (1) be the state that process X ( t )firstmovestoafter
time t =0, let Z (2) be the next state process X ( t ) moves to, and so on. (If the Markov
process X ( t ) makes a transition from state i to state i , which can occur when pi , i = 0
in the associated transition matrix, then the Markov chain Z ( t ) should also make a
transition from state i to state i .)

(a) Suppose that, in the process X ( t ), the time spent in state i is exponentially dis-
tributed with parameterθ i =θ(which is the same for all i ). Further suppose that
the process X ( t ) has a stationary distribution. Show that the Markov chain Z ( t ) has
the same stationary distribution.
(b) Give an example showing that, if theθ i are not all equal, then the stationary distri-
butions for X ( t ) and Z ( t ) may differ.

Exercise 8.21: The Ehrenfest model is a basic model used in physics. There are n
particles moving randomly in a container. We consider the number of particles in the
left and right halves of the container. A particle in one half of the container moves to
the other half after an amount of time that is exponentially distributed with parameter 1,
independently of all other particles. See Figure8.6.

(a) Find the stationary distribution of this process.
(b) What state has the highest probability in the stationary distribution? Can you sug-
gest an explanation for this?

Exercise 8.22: The following type of geometric random graph arises in the analy-
sis of dynamic wireless and sensor networks. We have n points uniformly distributed
in a square S of area n. Each point is connected to its k closest points in the square.
Denote this random graph by Gn , k. We show that there is a constant c >0 such that if
k = c log n then with probability at least 1− 1 / n the graph Gn , k is connected. Consider
tessellating (tiling with smaller squares) the square S with n /( b log n ) squares of area
b log n each, for some constant b ; we assume that b log n divides n for convenience.

continuous distributions and the poisson process
(a) Choose constants b and c 1 such that with sufficiently high probability, every square
has at least 1 point, and at most c 1 log n points.
(b) Conclude that with c ≥ 25 c 1 , the graph is connected with probability at least 1−
1 / n.

Exercise 8.23: We can obtain a discrete time Markov chain from the M / M /1 queueing
process in the manner described in Exercise8.20. The discrete time chain tracks the
number of customers in the queue. It is useful to allow departure events to occur with
rateλat the queue even when it is empty; this does not affect the queue behavior, but
it gives transitions from state 0 to state 0 in the corresponding Markov chain.

(a) Describe the possible transitions of this discrete-time chain and give their proba-
bilities.
(b) Show that the stationary distribution of this chain whenλ<μis the same as for
the M / M /1 process.
(c) Show that, in the caseλ=μ, there is no valid stationary distribution for the Mar-
kov chain.

Exercise 8.24: In a tandem queue, customers arrive to an M / M /1 queue according to a
Poisson process of rateλwith service times independent and exponentially distributed
with parameterμ 1. After completing service at this first queue, the customers proceed
immediately to a second queue, also being served by a single server, where service
times are independent and exponentially distributed with parameterμ 2. Find the sta-
tionary distribution of this system. ( Hint: Try to generalize the form of the stationary
distribution for a single queue.)

Exercise 8.25: Write a program to simulate the model of balls and bins with feedback.

(a) Start your simulation with 51 balls in bin 1 and 49 balls in bin 2, using p =2. Run
your program 100 times, having it stop each time one bin has 60% of the balls. On
average, how many balls are in the bins when the program stops? How often does
bin 1 have the majority?
(b) Perform the same experiment as in part (a) but start with 52 balls in bin 1 and 48
balls in bin 2. How much does this change your answers?
(c) Perform the same experiment as in part (a) but start with 102 balls in bin 1 and 98
balls in bin 2. How much does this change your answers?
(d) Perform the same experiment as in part (a), but now use p = 1 .5. How much does
this change your answers?

Exercise 8.26: We consider here one approach for studying a FIFO queue with a
constant service time of duration 1 and Poisson arrivals with parameterλ<1. We
replace the constant service time by k exponentially distributed service stages, each
of mean duration 1/ k. A customer must pass through all k stages before leaving the
queue, and once one customer begins going through the k stages, no other customer
can receive service until that customer finishes.

8.7 exercises
(a) Derive Chernoff bounds for the probability that the total time taken in k exponen-
tially distributed stages, each of mean 1/ k , deviates significantly from 1.
(b) Derive a set of equations that define the stationary distribution for this situation.
( Hint: Try lettingπ j be the limiting probability of having j stages of service left to
be served the queue. Each waiting customer requires k stages; the one being served
requires between 1 and k stages.) You should not try to solve these equations to give
a closed form forπ j.
(c) Use these equations to numerically determine the average number of customers
in the queue in equilibrium, say forλ= 0 .8 and for k =10, 20, 30, 40, and 50.
Discuss whether your results seem to be converging as k increases, and compare
the expected number of customers to an M / M /1 queue with arrival rateλ<1 and
expected service timeμ=1.

Exercise 8.27: Write a simulation for a bank of nM / M /1 FIFO queues, each with
Poisson arrivals of rateλ<1 per second and each with service times exponentially
distributed with mean 1 second. Your simulation should run for t seconds and return
the average amount of time spent in the system per customer who completed service.
You should present results for your simulations for n =100 and for t = 10 ,000 seconds
withλ= 0. 5 , 0. 8 , 0 .9, and 0.99.
A natural way to write the simulation that we now describe is to keep a priority
queue of events. Such a queue stores the times of all pending events, such as the next
time a customer will arrive or the next time a customer will finish service at a queue.
A priority queue can answer queries of the form, “What is the next event?” Priority
queues are often implemented as heaps, for example.
When a customer bound for queue k arrives, the arrival time for the next customer
to queue k must then be calculated and entered in the priority queue. If queue k is
empty, the time that the arriving customer will complete service should be put in the
priority queue. If queue k is not empty, the customer is put at the tail of the queue. If
a queue is not empty after completing service for a customer, then the time that the
next customer (at the head of the queue) will complete service should be calculated
and put in the priority queue. You will have to track each customer’s arrival time and
completion time.
You may find ways to simplify this general scheme. For example, instead of con-
sidering a separate arrival process for each queue, you can combine them into a single
arrival process based on what we know from Section8.4.2. Explain whatever simplifi-
cations you use.
You may wish to use Exercise8.9to help construct exponentially distributed random
variables for your simulation.
Modify your simulation so that, instead of service times being exponentially dis-
tributed with mean 1 second, they are always exactly 1 second. Again present results
for your simulation for n =100 and for t = 10 ,000 seconds withλ= 0. 5 , 0. 8 , 0 .9,
and 0.99. Do customers complete more quickly with exponentially distributed service
times or constant service times?

chapter nine

The Normal Distribution

The normal (or Gaussian) distribution plays a central role in probability theory and
statistics. Empirically, many real-world observable quantities, such as height, weight,
grades, and measurement error, are often well approximated by the normal distribution.
Furthermore, the central limit theorem states that under very general conditions the dis-
tribution of the average of a large number of independent random variables converges
to the normal distribution.
In this chapter we introduce the basic properties of univariate and multivariate nor-
mal random variables, prove the central limit theorem, compute maximum likelihood
estimates for the parameters of the normal distribution, and demonstrate the applica-
tion of the Expectation Maximization (EM) algorithm to the analysis of a mixture of
Gaussian distributions.^1

9.1 The Normal Distribution
9.1.1 The Standard Normal Distribution
The standard normal distribution , denoted N (0,1), is the basic building block of all
univariate and multivariate normal distributions. We say a standard normal random
variable Z comes from N (0,1), or just say that Z is a standard normal random variable,
where the meaning is clear. The standard normal distribution is a continuous distribu-
tion over the real numbers, with density function

φ( z )=
1
√
2 π
e− z
(^2) / 2
,
and distribution function
( z )=

1
√
2 π
∫ z
−∞
e− x
(^2) / 2
dx.
(^1) Following the conventions of the different communities we use the term normal distribution in the context of
probability theory and the term Gaussian distribution in the context of machine learning.

9.1 the normal distribution
Because (see Exercise9.1)

∫∞
−∞
1
√
2 π
e− x
(^2) / 2
dx = 1 ,
the density function defines a proper probability distribution.
While the distribution function( z ) is well-defined for any z , it does not have a
closed form expression. The actual values of the standard normal distribution can be
computed numerically, as seen in Table9.1. The distribution is related to what is known
as the error function in statistics, which is commonly denoted by
erf( z )=

2
√
π
∫ z
0
e− x
2
dx ,
and it is straightforward to check that

( z )=
1
2
+
1
2
erf
(
z
√
2
)
Since the density of a standard normal random variable Z is symmetric with respect
to z =0, it follows that E [ Z ]=0. The variance of Z can be found by using integration
by parts (where the parts used below are u = x and d v= x e− x

(^2) / 2
dx ):
Var [ Z ]= E [ Z^2 ]=

1
√
2 π
∫∞
−∞
x^2 e− x^2 /^2 dx
=
1
√
2 π
∫∞
−∞
( x )
(
x e− x
(^2) / 2 )
dx

=−
1
√
2 π
x e− x
(^2) / 2
|∞−∞+

1
√
2 π
∫∞
−∞
e− x
(^2) / 2
dx = 1.
In the last equality, the first term is 0, and we have already observed that the second
term equals 1.

9.1.2 The General Univariate Normal Distribution
The univariate (or single-variable) normal distribution is characterized by two para-
metersμandσ, corresponding to the mean and the standard deviation, and is denoted
by N (μ, σ^2 ). Notice that we use the variance rather than the standard deviation in how
we denote the normal distribution, for reasons that will become clear later. As before,
we may say that X is a normal random variable (with parametersμandσ) to denote
that X is a random variable with a normal distribution.
The density function of a normal random variable X from N (μ, σ^2 )is

fX ( x )=
1
√
2 πσ
e−(( x −μ)/σ)
(^2) / 2
,

the normal distribution
Z ∼ N(0,1)
Pr(Z ≤ z) =∫
− 3 − 2 − 10123
zφ(t)dt
−∞
Table of Distribution Function
of the standard normal N distribution
0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09
0.00.50000.50400.50800.51200.51600.51990.52390.52790.53190.5359
0.10.53980.54380.54780.55170.55570.55960.56360.56750.57140.5753
0.20.57930.58320.58710.59100.59480.59870.60260.60640.61030.6141
0.30.61790.62170.62550.62930.63310.63680.64060.64430.64800.6517
0.40.65540.65910.66280.66640.67000.67360.67720.68080.68440.6879
0.50.69150.69500.69850.70190.70540.70880.71230.71570.71900.7224
0.60.72570.72910.73240.73570.73890.74220.74540.74860.75170.7549
0.70.75800.76110.76420.76730.77040.77340.77640.77940.78230.7852
0.80.78810.79100.79390.79670.79950.80230.80510.80780.81060.8133
0.90.81590.81860.82120.82380.82640.82890.83150.83400.83650.8389
1.00.84130.84380.84610.84850.85080.85310.85540.85770.85990.8621
1.10.86430.86650.86860.87080.87290.87490.87700.87900.88100.8830
1.20.88490.88690.88880.89070.89250.89440.89620.89800.89970.9015
1.30.90320.90490.90660.90820.90990.91150.91310.91470.91620.9177
1.40.91920.92070.92220.92360.92510.92650.92790.92920.93060.9319
1.50.93320.93450.93570.93700.93820.93940.94060.94180.94290.9441
1.60.94520.94630.94740.94840.94950.95050.95150.95250.95350.9545
1.70.95540.95640.95730.95820.95910.95990.96080.96160.96250.9633
1.80.96410.96490.96560.96640.96710.96780.96860.96930.96990.9706
1.90.97130.97190.97260.97320.97380.97440.97500.97560.97610.9767
2.00.97720.97780.97830.97880.97930.97980.98030.98080.98120.9817
2.10.98210.98260.98300.98340.98380.98420.98460.98500.98540.9857
2.20.98610.98640.98680.98710.98750.98780.98810.98840.98870.9890
2.30.98930.98960.98980.99010.99040.99060.99090.99110.99130.9916
2.40.99180.99200.99220.99250.99270.99290.99310.99320.99340.9936
2.50.99380.99400.99410.99430.99450.99460.99480.99490.99510.9952
2.60.99530.99550.99560.99570.99590.99600.99610.99620.99630.9964
2.70.99650.99660.99670.99680.99690.99700.99710.99720.99730.9974
2.80.99740.99750.99760.99770.99770.99780.99790.99790.99800.9981
2.90.99810.99820.99820.99830.99840.99840.99850.99850.99860.9986
3.00.99870.99870.99870.99880.99880.99890.99890.99890.99900.9990
3.10.99900.99910.99910.99910.99920.99920.99920.99920.99930.9993
z
Table 9.1: Standard normal distribution table. For z <0use( z )= 1 −(− z ).
and its distribution function is

FX ( x )=
1
√
2 πσ
∫ x
−∞
e−(( t −μ)/σ)
(^2) / 2
dt.
These expressions generalize the corresponding expressions for standard normal ran-
dom variables, whereμ=0 andσ=1.

9.1 the normal distribution
Indeed, let X be a normal random variable with parametersμandσ, and let Z =
( X −μ)/σ. Then

Pr( Z ≤ z )=Pr( X ≤σ z +μ)=
1
√
2 πσ
∫σ z +μ
−∞
e−(( t −μ)/σ)
(^2) / 2
dt.
Substituting x =( t −μ)/σand using dt =σ dx , we find the density of the standard
normal distribution,
Pr( Z ≤ z )=

1
√
2 π
∫ z
−∞
e− x
(^2) / 2
dx =( z ).
We see that the normal distribution X is a linear transformation of the standard normal
distribution. That is, if X is a normal random variable with parametersμandσ, then
Z =( X −μ)/σis a standard normal random variable (with mean 0 and variance 1),
and similarly, if Z is a standard normal random variable, then X =σ Z +μis a normal
random variable with parametersμandσ. We have shown the following.
Lemma 9.1: A random variable has a normal distribution if and only if it is a linear
transformation of a standard normal random variable.
Since a random variable X from the distribution N (μ, σ^2 ) has the same distribution
asσ Z +μ, we have that E [ X ]=μand Var [ X ]=σ^2 ,soμandσare indeed the mean
and standard deviation.
Example: Signal Detection
Assume that we have a transmitter that is sending a bit via the encoding S ∈{− 1 ,+ 1 },
and the communication channel adds noise Y to the signal, where Y is a normal ran-
dom variable with mean 0 and standard deviationσ. The receiver decodes by taking
the sign of the received signal, R =sgn( S + Y ). (Here sgn is the sign function, where
sgn( x )=1if x >0, sgn( x )=−1if x <0, and by convention sgn(0)=0.) We find the
probability that the decoded message is different from the original message, that is the
probability that R = S.
The probability of an error when S =1 is the probability that Y ≤−1:
Pr( Y ≤−1)=Pr

(
Y −μ
σ
≤
− 1 −μ
σ
)
=
(
−
1
σ
)
.
Similarly, the probability of an error when S =−1 is the probability that Y ≥1:

Pr( Y ≥1)= 1 −Pr
(
Y −μ
σ
≤
1 −μ
σ
)
= 1 −
(
1
σ
)
.
By the symmetry of the normal distribution around its mean,
(
−^1 σ
)
= 1 −
( 1
σ
)
,
and the total error probability is 2

(
1 −
( 1
σ
))
.
We can use Table9.1to determine the error probabilities (up to four decimal places).
Forσ= 0. 5 , 1 ,2, we find(2)= 0 .9772,(1)= 0 .8413, and(0.50)= 0 .6915.
The error probabilities are therefore 0. 0456 , 0 .3174, and 0.6170, respectively.

the normal distribution
9.1.3 The Moment Generating Function
We next compute the moment generating function of normal random variable X from
N (μ, σ^2 ). In what follows, let z =( x −μ)/σ, and noteσ dz = dx.

MX ( t )= E [e tX ]
=
1
√
2 πσ
∫∞
x =−∞
e tx e−( x −μ)
(^2) /(2σ (^2) )
dx

=
1
√
2 πσ
∫∞
z =−∞
e t σ z + t μe− z
(^2) / 2
σ dz

=
1
√
2 π
eμ t
∫∞
z =−∞
e−( z − t σ)
(^2) / 2 +( t σ) (^2) / 2
dz

=
(
e t
(^2) σ (^2) / 2 +μ t ) 1
√
2 π

∫∞
z =−∞
e−( z − t σ)
(^2) / 2
dz
=e t
(^2) σ (^2) / 2 +μ t
.
Here in the last equality we used the fact that
1
√
2 π

∫∞
−∞
e−( z − t σ)
(^2) / 2
dz = 1 ;
to see this, note that
1
√
2 π
∫ x
−∞
e−( z − t σ)
(^2) / 2
dz
is the distribution function of a normal random variable with mean t σand standard
deviation 1, and hence when x goes to infinity the integral is 1.
We can use the moment generating function to verify our computation of the expec-
tation and variance of the normal distribution from Section9.1.2.
MX ′( t )=(μ+ t σ^2 )e t
(^2) σ (^2) / 2 +μ t
and
M ′′ X ( t )=(μ+ t σ^2 )^2 e t
(^2) σ (^2) / 2 +μ t
+σ^2 e t
(^2) σ (^2) / 2 +μ t
.
Thus, E [ X ]= M ′(0)=μ, E [ X^2 ]= M ′′(0)=μ^2 +σ^2 , and Var [ X ]= E [ X^2 ]−( E [ X ])^2
=σ^2.
Another important property of the normal distribution is that a linear combination
of normal random variables has a normal distribution:
Theorem 9.2: Let X and Y be independent random variables with distributions
N (μ 1 ,σ 12 ) and N (μ 2 ,σ 22 ) , respectively. Then X + Y is distributed according to the nor-
mal distribution N (μ 1 +μ 2 ,σ 12 +σ 22 ).
Proof: The moment generating function of a sum of independent random variables is
the product of their moment generating functions (Theorem4.3). Thus,
MX + Y ( t )= MY ( t ) MY ( t )=e t
(^2) σ 12 / 2 +μ 1 t
e t
(^2) σ 22 / 2 +μ 2 t
=e t
(^2) (σ 12 +σ 22 )/ 2 +(μ 1 +μ 2 ) t
.

9.2 ∗ Limit of the Binomial Distribution
The rightmost expression is the moment generating function of a normal distribu-
tion. Theorem4.2implies that X + Y has a normal distribution with the corresponding
parameters.  /theta

Using the moment generating function we can also obtain a large deviation bound
for a normal random variable.

Theorem 9.3: Let X have distribution N (μ, σ^2 ) , then

Pr
(∣∣
∣∣ X −μ
σ
∣
∣∣
∣≥ a
)
≤2e− a
(^2) / 2
.
Proof: Let Z =( X −μ)/σ,so Z has distribution N (0,1). Using the general technique
presented in Section4.2, for any t >0,
Pr( Z ≥ a )=Pr(e tZ ≥e ta )
≤
E [e tZ ]
e ta
≤e t
(^2) / 2 − ta
≤e− a
(^2) / 2
,
where in the last inequality we set t = a. The case Z ≤ a similarly yields the same
bound, proving the claim.  /theta

9.2. ∗ Limit of the Binomial Distribution
A function similar to the density of the normal distribution appeared first (around 1738)
in the De Moivre–Laplace approximation of the binomial distribution. De Moivre used
it to approximate the number of heads in a sequence of coin tosses, Laplace extended the
result to a sequence of Bernoulli trials. We present this result here since it gives insight
into the density function of the normal distribution and to the central limit theorem
presented in the next section.

Theorem 9.4: For a constant p with 0 < p < 1 and q = 1 − p, for k = np ±
O (

√
npq ) ,
lim
n →∞
(
n
k
)
pk (1− p ) n − k =
1
√
2 π npq
e−( k − np )^2 /(2 npq ).
Proof: Applying Stirling’s formula (Lemma 7.3),

lim
n →∞
n !=
√
2 π n
( n
e
) n
(1± o (1)),
the normal distribution
we have
(
n
k

)
pk (1− p ) n − k =
n!
k !( n − k )!
pk (1− p ) n − k
=
√
2 π n
√
2 π k
√
2 π( n − k )
e k e n − knn
e nkk ( n − k ) n − k
pk (1− p ) n − k (1± o (1))
=
1
√
2 π nnkn − nk
(
k
np
)− k (
n − k
nq
)−( n − k )
(1± o (1)).
For k = np ± O (√ npq ),
lim
n →∞
1
√
2 π nknn − nk
=
1
√
2 π npq
.
Let t =( k − np )/
√
npq = O (1). Then k = np + t
√
npq , and n − k = nq − t
√
npq.
Thus
(
k
np

)− k (
n − k
nq
)−( n − k )
=
(
1 +
t
√
q
√
np
)− k (
1 −
t
√
p
√
nq
)−( n − k )
.
Using the Taylor series expansion
ln(1+ x )= x −
x^2
2
+ O ( x^3 ),
we have

ln
[(
k
np
)− k (
n − k
nq
)−( k − n )]
=ln
[(
1 +
t √ q
√ np
)− k (
1 −
t √ p
√ nq
)−( n − k )]
=−( np + t
√
npq )
(
t √ q
√ np −
t^2 q
2 np
)
−( nq − t
√
npq )
(
−
t √ p
√ nq −
t^2 p
2 nq
)
+ O
(
t^3
√
n
)
=
(
− t
√
npq +
t^2 q
2
− t^2 q
)
+
(
t
√
npq +
t^2 p
2
− t^2 p
)
+ O
(
t^3
√
n
)
=−
t^2
2
+ O
(
t^3
√
n
)
.
Thus,
n lim→∞
(
k
np
)− k (
n − k
nq
)−( k − n )
=e− t
(^2) / 2
.
Combining the two limits we obtain
n lim→∞

(
n
k
)
pk (1− p ) n − k =
1
√
2 π npq
e− t
(^2) / 2
1
√
2 π npq
e−( k − np )
(^2) /(2 npq )

.  /theta

9.3 The Central Limit Theorem
Theorem9.4estimates a discrete probability; it does not define a density function.
However, as n →∞, it implies the following estimate which is a simple version of the
central limit theorem for a sum of independent Bernoulli random variables:
lim
n →∞
Pr
(
a ≤
k − np
√
npq
≤ b
)
=
np +∑ b √ npq
k = np − a √ npq
1
√
2 π npq
e−( k − np )
(^2) /(2 npq )

≈
∫ np + b √ npq
k = np − a √ npq
1
√
2 π npq
e−( k − np )
(^2) /(2 npq )
dk

≈
1
√
2 π
∫ b
a
e− t
(^2) / 2
dt ,
where we have approximated the sum by an integral and again used the substitution
t =( k − np )/

√
npq.
9.3. The Central Limit Theorem
The central limit theorem is one of the most fundamental results in probability theory,
giving the theoretical foundation for many statistical analysis techniques. The theorem
states that, under various mild conditions, the distribution of the average of a large num-
ber of independent random variables converges to the normal distribution, regardless
of the distribution of each of the random variables. The convergence is in distribution.
Definition 9.1: A sequence of distributions F 1 , F 2 ,...converges in distribution to a
distribution F, denoted Fn
D
−→ F, if for any a ∈R at which F is continuous,
n lim→∞ Fn ( a )= F ( a ).
Convergence in distribution is a relatively weak notion of convergence. In particular,
it does not guarantee a uniform bound on the rate of convergence of Fn ( a ) for different
values of a.
We prove here a basic version of the central limit theorem for the average of inde-
pendent, identically distributed, random variables with finite mean and variance.
Theorem 9.5 [ The Central Limit Theorem ] : Let X 1 ,..., Xnbe n independent, iden-
ticallydistributedrandomvariableswithmean μ andvariance σ^2 .LetX ̄ n =^1 n
∑ n
i = 1 Xi.
Then for any a and b,
n lim→∞Pr
(
a ≤
X ̄ n −μ
σ/
√
n
≤ b
)
−→ D ( b )−( a ).
That is, the average X ̄ n converges in distribution to a normal distribution with the
appropriate mean and variance. Our proof of the central limit theorem uses the follow-
ing result, which we quote without proof.
Theorem 9.6 [ Lévy’s Continuuity Theorem ] : Let Y 1 , Y 2 ,... be a sequence of ran-
dom variables with Yihaving distribution Fiand moment generating functions Mi.Let
Y be a random variable with distribution F and moment generating function M. If

lim n →∞ Mn ( t )= M ( t ) for all t, then Fn
D
−→ F for all t for which F ( t ) is continuous.
the normal distribution
Proof of the Central Limit Theorem: Let Zi =( Xi −μ)/σ. Then Z 1 , Z 2 ,...are inde-
pendent, identically distributed, random variables with expectation E [ Zi ]=0, variance
Var [ Zi ]=1, and

X ̄ n −μ
σ/
√
n
=
√
n
n
∑ n
i = 1
Xi −μ
σ
=
∑ n
√ i =^1 Zi
n
.
To apply Theorem9.6we show that the moment generating functions for the random
variables Yn =

∑ n
i = 1 Zi /
√
n converge to the moment generating function of the standard
normal distribution. That is, we need to show that

lim
n →∞
E
[
e t
∑ n
i = 1 Zi /
√ n ]
=e t
(^2) / 2
for all t.
Let M ( t )= E [e tZi ] be the moment generation function of Zi , so the moment gener-
ating function of Zi /

√
n is
E
[
e tZi /
√ n ]
= M
(
t
√
n
)
.
Since the Zi are independent and identically distributed,

E
[
e t
∑ n
i = 1 Zi /
√ n ]
=
(
M
(
t
√
n
)) n
.
Let L ( t )=ln M ( t ). Since M (0)=1, we have L (0)=0, and L ′(0)= M
′(0)
M (0)=
E [ Zi ]=0. We also can compute the second derivative:

L ′′(0)=
M (0) M ′′(0)−( M ′(0))^2
( M (0))^2
= E
[
Z^2 i
]
= 1.
We need to show that ( M ( t /
√
n )) n →e t^2 /^2 , or equivalently nL ( t /
√
n )→ t^2 /2.
Applying L’Hôpital’s rule (twice), we have

lim
n →∞
L ( t /
√
n )
n −^1
= lim
n →∞
− L ′( t /
√
n ) n −^3 /^2 t
− 2 n −^2
= lim
n →∞
L ′( t /
√
n ) t
2 n −^1 /^2
= n lim→∞
− L ′′( t /
√
n ) n −^3 /^2 t^2
− 2 n −^3 /^2
= lim
n →∞
L ′′( t /
√
n )
t^2
2
=
t^2
2
.
 /theta
The central limit theorem can be proven under a variety of conditions. For example,
in the following version of the theorem that we do not provide a proof for, the random
variables are not required to be identically distributed.

9.3the central limit theorem
Theorem 9.7: Let X 1 ,..., Xnbe n independent random variables with E [ Xi ]=μ iand
Var [ Xi ]=σ i^2_. Assume that:_

1. For some value M, Pr(| Xi |< M )= 1 for all i; and
2. lim n →∞

∑ n
i = 1 σ
2
i =∞.
Then

Pr
⎛
⎝ a ≤
∑ n
√ i =^1 ( Xi −μ i )
∑ n
i = 1 σ
2
i
≤ b
⎞
⎠−→ D ( b )−( a ).
While the central limit theorem only gives convergence in distribution, under some-
what more stringent conditions one can prove a uniform convergence result.

Theorem 9.8 [ Berry–Esséen Theorem ] : LetX 1 ,..., Xnbenindependent,identically
distributedrandomvariableswithfinitemean μ andvariance σ^2 .Let ρ= E [| Xi −μ|^3 ]
andX ̄ n =^1 n

∑ n
i = 1 Xi. Then there is a constant C such that for any a
∣∣
∣∣Pr
( ̄
Xn −μ
σ/
√
n
≤ a
)
−( a )
∣∣
∣∣≤ C ρ
σ^3
√
n
.
Example: Public Opinion Poll
Suppose that we want to estimate the support for the Yellow party. We poll n people to
determine if they support the party or not. We assume the n people can be considered
independent, uniform samples. If we assume further that n is sufficiently large to justify
approximating the results using a normal distribution as suggested by the central limit
theorem, what is the minimum number of samples needed to guarantee an estimate of
the true fraction of supporters p within an error of at most 0.05 with a 0.95 confidence
interval? Recall from Definition 4.2 that a 1−γconfidence interval for a parameter p
is an interval [ ̃ p −δ, p ̃+δ] such that

Pr( p ∈[ ̃ p −δ, p ̃+δ])≥ 1 −γ.
Let Xi =1 if the i th polled person supports the party and Xi =0 otherwise. The
fraction of support observed in the sample is given by

X ̄ n =^1
n
∑ n
i = 1
Xi.
Letδ= 0 .05 be our target error.
Clearly E [ X ̄ n ]= p and Var [ X ̄ n ]=^1 np (1− p ). Under our assumption that

Zn =
X ̄ n − E [ X ̄ n ]
√
Var [ X ̄ n ]
=
√
n ( X ̄ n − p )
√
p (1− p )
the normal distribution
is distributed as a standard normal distribution, we need to compute the minimum n
such that

Pr(| X ̄ n − p |≥δ)=Pr
(√
n ( X ̄ n − p )
√
p (1− p )
>
√
n δ
√
p (1− p )
)
+Pr
(√
n ( X ̄ n − p )
√
p (1− p )
<−
√
n δ
√
p (1− p )
)
= 2
(
1 −
( √
n δ
√
p (1− p )
))
≤ 0. 05.
Thus, we are looking for the minimum n such that


( √
n δ
√
p (1− p )
)
≥ 0. 975 =(1.96).
Solving

δ
√
n
√
p (1− p )
≥ 1. 96 ,
we obtain

n ≥
(
1. 96
√
p (1− p )
δ
) 2
Using the fact that p (1− p )≤ 1 /4, we find n ≥385 suffices.
9.4 ∗ Multivariate Normal Distributions
Assume that we want to study the relationship between the heights of parents and their
children. More concretely, consider families with at least one daughter and one son,
and for each such family let ( x 1 , x 2 , x 3 , x 4 ) be the heights of the mother, father, first
daughter, and first son, respectively. We know that each component in this vector can
be approximated by a univariate normal distribution, but what is the joint distribution
of the four variables? It turns out that for many natural phenomena, such as this one,
the joint distribution is well approximated by a multivariate normal distribution.
We saw in Lemma9.1that a univariate normal variable is always a linear transfor-
mation of a standard normal random variable. Similarly, the multivariate normal distri-
bution is a linear transformation of a number of independent, standard normal random
variables.
Let X =( X 1 ,..., Xn ) T denote a vector of n independent, standard normal random
variables. Let ̄ x =( x 1 ,..., xn ) T be a vector of real values. We define

Pr( X ≤ x ̄)=Pr( X 1 ≤ x 1 , X 2 ≤ x 2 ,..., Xn ≤ xn ).
9.4 ∗ multivariate normal distributions
The joint cumulative distribution function for X is given as follows, where ̄w=
(w 1 ,...,w n ) T :

Pr( X ≤ x ̄)=
∏ n
i = 1
(
1
√
2 π
∫ xi
w i =−∞
e−w
(^2) i / 2
d w i

)
=
1
(2π) n /^2
∫ x 1
w 1 =−∞
...
∫ xn
w n =−∞
e−w ̄
T w/ ̄ 2
d w 1 d w 2 ··· d w n.
Consider now a random variable vector Y =( Y 1 ,..., Ym ) T obtained by a linear
transformation on the vector X :

Y 1 = a 11 X 1 + a 12 X 2 +···+ a 1 nXn +μ 1 ;
Y 2 = a 21 X 1 + a 22 X 2 +···+ a 2 nXn +μ 2 ;
...
Ym = am 1 X 1 + am 2 X 2 +···+ amnXn +μ m.
Let A denote the matrix of coefficients aij , and ̄μ=(μ 1 ,...,μ m ) T. Then we can write

Y = A X +μ. ̄
The Yi s each have a normal distribution, specifically Yi has distribution
N (μ i ,

∑ n
j = 1 a^2 ij ). However, the Yi sare not independent. In particular,
Cov ( Yi , Yj )=
∑ n
k = 1
ai , kaj , k.
(This is left as Exercise9.3.)
The covariance matrix for Y is given by

 /theta= AA T =
⎛
⎜
⎜⎜
⎜
⎝
Var [ Y 1 ] Cov ( Y 1 , Yi )... Cov ( Yi , Yn )
Cov ( Y 1 , Y 2 ) Var [ Y 2 ] ... Cov ( Y 2 , Yn )
... .... ... ...
... .... ... ...
Cov ( Ym , Y 1 ) Cov ( Ym , Y 2 )... Var [ Ym ]
⎞
⎟
⎟⎟
⎟
⎠
= E [( Y −μ ̄)( Y −μ ̄) T ].
If A has a full rank, then X = A −^1 ( Y −μ ̄), and we can derive a density function for
the joint distribution.

Pr( Y ≤ y ̄)=Pr( Y −μ ̄≤ y ̄−μ ̄)
=Pr( A X ≤ y ̄−μ ̄)
=Pr( X ≤ A −^1 ( ̄ y −μ ̄))
=
1
(2π) n /^2
∫
w ̄≤ A −^1 ( ̄ y −μ ̄)
e−w ̄
T w/ ̄ 2
d w 1 ··· d w n.
Changing the integration variables to ̄ z = A w ̄+μ ̄we have

Pr( Y ≤ y ̄)=
1
√
(2π) n | AA T |
∫ y 1
−∞
...
∫ yn
−∞
e−( ̄ z −μ ̄)
T ( A − (^1) ) T A − (^1) ( ̄ z −μ ̄)/ 2
dz 1 ··· dzn.

the normal distribution
Here| AA T |denotes the determinant of AA T , a term which arises under the multivariate
change of variables.
Applying ( A −^1 ) T A −^1 =( A T )−^1 A −^1 =( AA T )−^1 = /theta−^1 , we can write the distribu-
tion function of Y as

Pr( Y ≤ y ̄)=
1
√
(2π) n | /theta|
∫ y 1
−∞
...
∫ yn
−∞
e−( ̄ z −μ ̄)
T  /theta− (^1) ( ̄ z −μ ̄)/ 2
dz 1 ··· dzn (9.1)
where, again,
 /theta= AA T = E [( Y −μ ̄)( Y −μ ̄) T ].
In general we have the following definition.
Definition 9.2: A vector Y =( Y 1 ,..., Yn ) Thas a multivariate normal distribution ,
denoted Y ∼ N ( ̄μ, /theta) , if and only if there is an n × k matrix A , a vector X =
( X 1 ,..., Xk ) Tof k independent standard normal random variables, and a vector μ ̄=
(μ 1 ,...,μ n ) T, such that
Y = A X +μ. ̄
If  /theta= AA T = E [( Y −μ ̄)( Y −μ ̄) T ] has full rank, then the density of Y is
1
√
(2π) n | /theta|
e−
(^12) ( Y −μ ̄) T  /theta− (^1) ( Y −μ ̄)
.
If  /theta is not invertible then the joint distribution has no density function.^2
Note that sometimes instead of saying that random variables have a multivariate
normal distribution, one says that they are jointly normal.
The corollary below follows readily, keeping in mind that a multivariate random
variable is a vector of random variables.
Corollary 9.9: Any linear combination of mutually independent multivariate normal
random variables of equal dimension has a multivariate normal distribution.
The special case of the bivariate normal density, with just two random variables,
has a simpler expression, using the correlation coefficient between the two random
variables.
Definition 9.3: The correlation coefficient between random variables X and Y is
ρ XY =
Cov ( X , Y )
σ X σ Y

.
As noted in Exercise9.4, the correlation coefficient is always in [− 1 ,1].
If ( X , Y ) T has a bivariate normal distribution,
(
X
Y
)
∼ N
((
μ X
μ Y
)
,
(
σ X ρ XY σ X σ Y
ρ XY σ X σ Y σ Y
))
,
(^2) In some settings multivariate normal distributions are defined so thatis required to be a symmetric positive
definite matrix and therefore invertible, but there are also settings where it is sensible to havenot invertible,
and therefore a distribution with no density function.

9.4 ∗ multivariate normal distributions
1 2
-1^0 X
Y -1 -2-2
(^10)
0.25
0.05 0
0.1
0.15
0.2
0.3
2
f(X,Y)
1 2
-1^0 X
Y -1 -2-2
(^10)
0.080.06
0.02 0
0.04
0.16
0.1
0.140.12
2
f(X,Y)
1 2
-1^0 X
Y -1 -2-2
(^10)
0
0.25
0.2
0.3
0.15
0.1
0.05
2
f(X,Y)
-2-2 -1.5 -1 -0.5 0 0.5 1 1.5 2
-1.5
-1
-0.5
0
0.5
1
1.5
2
(a) ρ= − 0. 8
-2-2 -1.5 -1 -0.5 0 0.5 1 1.5 2
-1.5
-1
-0.5
0
0.5
1
1.5
2
(b) ρ= 0
-2-2 -1.5 -1 -0.5 0 0.5 1 1.5 2
-1.5
-1
-0.5
0
0.5
1
1.5
2
(c) ρ= 0. 8
Figure 9.1: Bivariate Normal density function f ( X , Y ) (top) and the contour of the function at
increasing values of f ( X , Y ) (bottom), withμ X =μ Y =0,σ X =σ Y =1, andρ=− 0. 8 , 0 , 0 .8.
then for any|ρ XY |<1 (andσ X ,σ Y >0) the joint density function of X and Y is given
by
1
2 πσ X σ Y

√
1 −ρ XY^2
e−(( X −μ X )
(^2) /(2σ X (^2) )+( Y −μ Y ) (^2) /(2σ Y (^2) )−ρ XY ( X −μ X )( Y −μ Y )/(σ X σ Y ))/(1−ρ XY )

(9.2)
Examples of bivariate normal distributions are shown in Figure9.1.
9.4.1 Properties of the Multivariate Normal Distribution
We outline several important properties of the multivariate normal distribution that
make it particularly amendable to analysis. The proofs are left as Exercise9.10.

Theorem 9.10: Assume that X is an n × 1 vector distributed as N ( ̄μ, /theta) .Let

X =
(
X 1
X 2
)
and μ ̄=
(
μ ̄ 1
μ ̄ 2
)
,
where X 1 and X 2 are k × 1 and ( n − k )× 1 vectors respectively, and μ ̄ 1 and μ ̄ 2 are the
expectation vectors of X 1 and X 2 respectively. Similarly, let

 /theta=
(
 /theta 11  /theta 12
 /theta 12  /theta 22
)
,
where  /theta 11 isthek × k correlationmatrixofX 1 ,  /theta 22 isthe ( n − k )×( n − k ) correlation
matrixofX 2 ,  /theta 12 isthecorresponding ( n − k )× k matrix,and  /theta 21 isthecorresponding
k ×( n − k ) matrix.

the normal distribution
(1) The marginal distributions of Xi,i = 1 , 2 ,areN ( ̄μ i , /theta ii ).
(2) The distribution of Xiconditioned on Xj = x ̄ jis N ( ̄μ i | j , /theta i | j ) where

μ ̄ i | j =μ ̄ i + /theta ij  /theta− jj^1 ( ̄ xj −μ ̄ j ) and  /theta i | j = /theta jj − /theta Tij  /theta− ii^1  /theta ij.
(3) X 1 and X 2 are independent if and only if the matrix  /theta 12 = 0 (i.e. all its entries
are 0).

9.5 Application: Generating Normally Distributed Random Values
One natural question is how to produce random values that follow a normal distribution.
More specifically, given random values distributed uniformly in (0,1), which is a useful
and standard primitive operation to assume that we have available, can we generate
random values that follow a normal distribution? As we have seen, it suffices to generate
random values with a standard normal distribution. These values can then be scaled to
follow a normal distribution with meanμand varianceσ^2.
The standard approach to convert a random variable X uniformly distributed in (0,1)
to a random variable Z with cumulative distribution function F ( Z ) is to set Z = F −^1 ( X ).
That is, for X = x we set Z = z where F ( z )= x. Indeed

Pr( Z ≤ z )=Pr( F −^1 ( X )≤ F −^1 ( x ))=Pr( F ( Z )≤ F ( z ))=Pr( X ≤ F ( z ))= F ( z ).
This method cannot be readily used for the normal distribution because the cumula-
tive distribution function( X ) does not have a closed form, and therefore we cannot
directly compute−^1 ( X ), although there are various means to approximate−^1 ( X )
accurately at some expense in computation or memory.
One simple approach to approximating a standard normal random variable, sug-
gested by the central limit theorem, is to generate U 1 , U 2 ,..., U 12 , with the Ui inde-
pendent and uniform on (0,1), and compute

X =
( 12
∑
i = 1
Ui
)
− 6.
In this case X is obviously not distributed as a standard normal; in particular, it can only
take on values in the range (− 6 ,6). It is, however, a surprisingly good approximation,
as shown further in Exercise9.6. You should note that X has both expectation 0 and
variance 1.
It turns out, however, that a standard normal random variable can be generated
exactly using random numbers from (0,1), with some additional mathematical oper-
ations. In fact, there are two natural related ways to do this. Both rely on the same
approach: instead of generating just one value, we produce two.
To start, consider the joint cumulative distribution for two independent standard nor-
mal random variables X and Y :

F ( x ′, y ′)=
∫ y ′
−∞
∫ x ′
−∞
1
2 π
e−( x
(^2) + y (^2) )/ 2
dx dy. (9.3)

9.5application: generating normally distributed random values
The x^2 + y^2 term in the double integral actually helps us; it allows us to naturally move
the problem into polar coordinates.
We can think of X and Y as being plotted as a pair ( X , Y ) on the two-dimensional
plane. Then we let R^2 = X^2 + Y^2 , where R is the radius of the circle centered at the
origin that the point ( X , Y ) lies on, and similarly let /eta=tan−^1 YX be the angle the point
makes with the x-axis. Thus

X = R cos /eta, Y = R sin /eta.
We use a change of variables in Eqn. (9.3) above; with x = r cosθand y = r sinθ,we
have

F ( r ′,θ′)=
∫θ′
0
∫ r ′
0
1
2 π
e− r
(^2) / 2
rdr d θ.
Here we have used that under this change of variables, dxdy = rdrd θ. More formally,
from multivariable calculus, the Jacobian for the transformation is
∂( x , y )
∂( r ,θ)

=
∣∣
∣∣
∣∣
∣
∣
∂ x
∂ r
∂ x
∂θ
∂ y
∂ r
∂ y
∂θ
∣∣
∣∣
∣∣
∣
∣
=
∣∣
∣∣cosθ − r sinθ
sinθ − r cosθ
∣∣
∣∣= r (cos^2 θ+sin^2 θ)= r.
Hence the additional factor of r when we change variables. This expression integrates
nicely to

F ( r ′,θ′)=
θ′
2 π
(
1 −e−( r
′) (^2) / 2 )
.
In particular, notice that F ( r ′,θ′)= G ( r ′) H (θ′) where H (θ′)=θ
′
2 π and G ( r

′)=
1 −e−( r ′)^2 /^2. This means that the radius R and the angle /etadetermined by X and Y
are independent random variables.
But now, thinking in the other direction, we can directly generate R and /etato obtain
our independent standard normal random variables X and Y. We can see from the form
of H (θ′) that the angle /etais uniformly distributed over [0, 2 π]; this also follows nat-
urally from symmetry considerations. Hence, given a uniform random variable U on
(0,1), we can take /eta= 2 π U .For R ,wehave

Pr( R ≥ r ′)=e−( r
′) (^2) / 2
,
while for a uniform (0,1) random variable V ,
Pr( V ≥v)= 1 −v.
Setting the right-hand sides equal, we find r ′=

√
−2ln(1−v). Hence, we can take
R =

√
−2ln(1− V ) to obtain a suitably distributed radius R. Since 1− V and V both
have uniform (0,1) distributions, equivalently we can take R =

√
−2ln V instead. To
conclude, given two uniform (0,1) random variables U and V , we can take

X =
√
−2ln V cos(2π U ), Y =
√
−2ln V sin(2π U )
to obtain two independent standard normal random variables. This is commonly
referred to as the Box–Muller transform.

the normal distribution
A common variation avoids using the sine and cosine functions. Let U ′and V ′be
independent and uniform on (0,1), and let U = 2 U ′−1 and V = 2 V ′−1, so that U
and V are independent and uniform on (− 1 ,1). If S = U^2 + V^2 ≥1, we throw these
values away and start over again. Otherwise, generate

X = U
√
−2ln S
S
, Y = V
√
−2ln S
S
The result is that X and Y are independent standard normal random variables. Think
of the point ( U , V ) in the xy -plane, and note that U /

√
S and V /
√
S play the role of
cos /etaand sin /etain the Box–Muller transform; note here that the corresponding /etais
independent of the value of S itself. Also, S is uniformly distributed over [0,1), because
( U , V ) is uniform over the circle of radius 1 centered at (0,0), and hence

Pr( S ≤ s )=Pr( U^2 + V^2 ≤ s )
is the probabilility that a random point in the unit circle lies within a circle of radius√
s around the origin. But this probability is just

Pr( S ≤ s )=Pr( U^2 + V^2 ≤ s )=
π s
π
= s ,
so S has a uniform distribution. Hence

√
−2ln S takes the role of
√
−2ln V in the Box–
Muller transform. (As S takes on the value 0 with probability 0, we can equivalently
think of S as uniform on (0,1).)

9.6 Maximum Likelihood Point Estimates
Normal distributions are often used to model observed data. We see a sample of data
points, such as the heights of individuals, and then we try to fit a normal distribution to
that data. How do we characterize and find the best fit? This is similar in some ways to
the question of the confidence interval. We have seen in Chapter 4 and in this chapter
that when obtaining a sample by polling for a yes–no question, the natural estimate is
to take the fraction of responders that answer “yes” and use that as our guess for the
fraction of the population that would answer yes to the question. We could also find
probabilistic guarantees that this fraction was within some interval of the true answer,
under assumptions about the independence of our samples.
Here we also consider the question of finding the best parameter for a distribution
given a collection of data points, but more generally. Our goal is to find the best rep-
resentative value for the parameter. Maximum likelihood (ML) estimators give such a
value.^3

(^3) Here we follow a classical statistics approach, where the ML estimator assumes no prior knowledge on the
parameter and just reports the value that maximizes the probability or the density of the observed data, as
described in Definition9.4. In contrast, the Bayesian statistics approach starts with a prior distribution on the
space of possible parameter choices and computes a maximum a posterior estimation , which is the value of the
parameter conditioned on the observed data. The two estimators give the same value when the prior distribution
is uniform on the range of the parameter, as discussed further in Exercise9.13.

9.6maximum likelihood point estimates
Definition 9.4:

1. LetPX ( x ,θ) betheprobabilityfunctionofadiscreterandomvariableX thatdepends
on a parameter θ .Letx 1 ,..., xnbe n independent observations of X. The Maximum
Likelihood (ML) estimator of θ is

arg max
θ
∏ n
i = 1
PX ( xi ,θ).
2. Let fX ( x ,θ) be the density function of a continuous random variable X that depends
on a parameter θ .Letx 1 ,..., xnbe n independent observations of X. The Maximum
Likelihood (ML) estimator of θ is

arg max
θ
∏ n
i = 1
fX ( xi ,θ).
Note thatθin Definition9.4can correspond to a single parameter (such as for the
exponential distribution) or a vector of parameters (such as the mean and variance for
the normal distribution).
As a simple example consider a sequence of n independent, identically distributed,
Bernoulli experiments with k successes. What is the maximum likelihood estimate for
the probability of success p? Intuitively, it seems that it should be the fraction of flips
that came up heads. We prove this intuition holds.
For a success probability of p , the probability of obtaining k heads is

f ( p )=
(
n
k
)
pk (1− p ) n − k.
We therefore need to compute

arg max p f ( p )=arg max p
(
n
k
)
pk (1− p ) n − k.
Computing the first derivative of f ( p ) yields

f ′( p )= k
(
n
k
)
pk −^1 (1− p ) n − k −( n − k )
(
n
k
)
pk (1− p ) n − k −^1.
We see that f ′( p )=0 when p = k / n. A further check shows that k / n provides the local
maximum, not a local minimum. It follows that the maximum likelihood value is the
fraction of successes, matching our intuition.
We turn now to computing an ML estimator for the expectation and the variance of
the normal distribution. In the case of continuous random variables it is often easier
to maximize the logarithm of the likelihood function, which is referred to as the log-
likelihood function,

∑ n
i = 1 ln( fX ( xi ,θ)). This is equivalent to maximizing the likelihood
function, since

arg max
θ
∏ n
i = 1
fX ( xi ,θ)=arg max
θ
∑ n
i = 1
ln( fX ( xi ,θ)).
the normal distribution
Let x 1 ,..., xn be n independent observations from a normal distribution with
unknown expectation and variance. The ML estimate is given by

arg max
(μ,σ)
∏ n
i = 1
1
√
2 πσ
e−( xi −μ)
(^2) /(2σ (^2) )
=arg max
(μ,σ)

1
(2πσ^2 ) n /^2
e−
∑ n
i = 1 ( xi −μ)^2 /(2σ^2 )
=arg min
(μ,σ)
[
n
2
log(2πσ^2 )+
∑ n
i = 1 ( xi −μ)
2
2 σ^2
]
,
where here we restrictσ>0.
Let Mn =^1 n

∑ n
i = 1 xi and S
2
n =
1
n
∑ n
i = 1 ( xi − Mn )
(^2) , and observe that
∑ n
i = 1
( xi −μ)^2 =
∑ n
i = 1
(( xi − Mn )+( Mn −μ))^2

=
∑ n
i = 1
( xi − Mn )^2 + n ( Mn −μ)^2 + 2
∑ n
i = 1
( xi − Mn )( Mn −μ)
= nS^2 n + n ( Mn −μ)^2 +2( Mn −μ)
∑ n
i = 1
( xi − Mn )
= nS^2 n + n ( Mn −μ)^2.
Thus we need to compute
arg min
μ,σ
[
n
2
log(2πσ^2 )+
nS^2 n
2 σ^2
+
n ( Mn −μ)^2
2 σ^2
]
.
For any value ofσ>0 the argument above is minimized whenμ= Mn. Hence it
suffices to minimize

f (σ)=
n
2
log(2πσ^2 )+
nS^2 n
2 σ^2
.
We find

f ′(σ)=−
n
2
2 π
2 πσ^2
+
nS^2 n
2(σ^2 )^2
=
n
2 σ^2
(
S^2 n
σ^2
− 1
)
.
The derivative is 0 whenσ^2 = S^2 n. We therefore find thatμ= Mn ,σ^2 = S^2 n provide the
maximum likelihood estimates of the parameters.
The estimator of a parameter is itself a random variable that is a function of the
observed data. It seems reasonable that its expectation would be equal to the value it
estimates, but this is not always the case.

Definition 9.5: Let  /eta nbe a function of n observations X 1 ,..., Xnthat depend on a
parameter θ.

 /theta /eta nis an unbiased estimator for a parameter θ if
E [ /eta n ]=θ.
9.7 Application: EM Algorithm For a Mixture of Gaussians
 /theta /eta nis an asymptotically unbiased estimator for a parameter θ if
n lim→∞ E [ /eta n ]=θ.
For example, for samples Xi taken from any distribution with finite expectation,
the estimator Mn =^1 n

∑ n
i = 1 Xi gives an unbiased estimate for the expectation. How-
ever, the ML estimate for the variance of a normal distribution we found above,
S^2 n =^1 n

∑ n
i = 1 ( Xi − Mn )
(^2) , is not an unbiased estimator for the variance. Since the Xi
are independent and identically distributed, let E [ X ]= E [ Xi ], and note that for i = j ,
E [ XiXj ]=( E [ X ])^2. Then,

E
[
S^2 n
]
=
1
n
∑ n
i = 1
(
E
[
Xi^2
]
− 2 E [ MnXi ]+ E [( Mn )^2 ]
)
= E [ X^2 ]− E
[
( Mn )^2
]
= E [ X^2 ]−
1
n^2
⎛
⎝ E
[ n
∑
i = 1
Xi^2
]
+
∑
i = j
E
[
XiXj
]
⎞
⎠
=
n − 1
n
( E [ X^2 ]−( E [ X ])^2 )
=
n − 1
n
σ^2.
Thus, the ML estimator for the variance of the normal distribution is only asymptoti-
cally unbiased. However, n − n 1 S^2 n is an unbiased estimator forσ^2.
Indeed, in statistical analysis, when sampling one generally uses
n
n − 1

S^2 n =
1
n − 1
∑ n
i = 1
( xi − Mn )^2
as the estimate of the variance because it is unbiased. This quantity is called the sample
variance , while

1
n
∑ n
i = 1
( xi − Mn )^2
is called the population variance. The population variance is the correct formula for
the variance when you have the entire population.

9.7. Application: EM Algorithm For a Mixture of Gaussians
Assume that ̄ x = x 1 ,..., xn is a sample of the heights of n students chosen indepen-
dently and uniformly at random. All we know about the students is their height; we do
not know whether each student is male or female. Assume that the height distribution
of females follows a normal distribution N (μ 1 ,σ 1 ) and the height of males follows a
normal distribution N (μ 2 ,σ 2 ). Letγbe the fraction of female students and 1−γthe
fraction of male students; we assume in what follows thatγis not known in advance.
We also assume the student population is large enough that we do not worry about the

the normal distribution
distinction of sampling with replacement or without replacement; concretely, we sam-
ple without replacement, but we treatγas a fixed parameter throughout the sampling.
The density function of each observation in the sample is

D ( xi ,γ,μ 1 ,μ 2 ,σ 1 ,σ 2 )=γ
1
√
2 πσ 12
e−( xi −μ^1 )
(^2) /(2σ 12 )
+(1−γ)

1
√
2 πσ 22
e−( xi −μ^2 )
(^2) /(2σ 22 )
.
To estimate the parameters of the two normal distributions for the sample ̄ x we want
to compute a maximum likelihood estimator for the vector (γ,μ 1 ,μ 2 ,σ 1 ,σ 2 ). The
likelihood function is
L ( ̄ x ,γ,μ 1 ,μ 2 ,σ 1 ,σ 2 )=
∏ n
i = 1

⎛
⎝γ√^1
2 πσ 12
e−( xi −μ^1 )
(^2) /(2σ 12 )
+(1−γ)

1
√
2 πσ 22
e−( xi −μ^2 )
(^2) /(2σ 22 )

⎞
⎠
and our goal is to find the maximum likelihood vector
(
γ ML ,μ ML 1 ,μ ML 2 ,σ 1 ML ,σ 2 ML

)
=arg max
(γ,μ 1 ,μ 2 ,σ 1 ,σ 2 )
L ( ̄ x ,γ,μ 1 ,μ 2 ,σ 1 ,σ 2 ).
This problem is a simple example of an important task in statistical data analysis –
estimating the parameters of a mixture of distributions. The intuitive difficulty behind
solving this problem is that it is not clear which distribution each observation belongs
to; if we knew which students were male and which were female, we could separate
them and solve for each distribution separately. Natural approaches for the problem
would simultaneously assign each observation to one of the distributions, or at least
provide a probability for each observation for which distribution it came from, while
also estimating the parameters of these distributions. The two distributions will over-
lap, making it hard to disentangle which data points correspond to each distribution.
Intuitively, the more overlap, the harder it is to find the most likely parameters.
The Expectation–Maximization (EM) algorithm is a simple iterative heuristic for
estimating the maximum likelihood of a distribution that depends on unobserved vari-
ables, such as the assignments of each observation to the distributions that generated
it. We describe and analyze here the EM algorithm for the problem of learning the
parameters of a mixture of two univariate Gaussian distributions. Extensions of the
algorithm to multivariate Gaussians and to more than two distributions are discussed
in Exercises9.11and9.12. The EM algorithm is also used in many other contexts.
The EM algorithm starts with arbitrary values for the parametersμ 1 ,μ 2 ,σ 12 ,σ 22.
We say more about the initialization below. Each iteration of the algorithm starts with
an E xpectation step followed by a M aximization step. The expectation step computes,
for each sample point xi , the probabilitities p 1 ( xi ) and p 2 ( xi ) that xi was generated
by the first or second distribution, respectively, conditioned on the current estimates
of the parameters of the two distributions. Here p 2 ( xi )= 1 − p 1 ( xi ), but for a larger
number of distributions one would compute all of these conditional probabilities. This
step provides a probabilistic assignment of samples to distributions. The maximization
step then computes a new ML estimate for the parametersμ 1 ,μ 2 ,σ 12 ,σ 22 based on this
assignment (see Algorithm9.1). The ML estimate generalizes the ML estimate we used
for a single Gaussian in Section9.6.

9.7application: em algorithm for a mixture of gaussians
EM Algorithm for Mixture of Two Gaussian Distributions
Input: n samples x 1 ,..., xn.
Output: ML-estimate (μ 1 ,μ 2 ,σ 12 ,σ 22 ,γ)
1. Start with arbitrary values forμ 1 ,μ 2 ,σ 12 ,σ 22 , and withγ= 1 /2.
2. For j = 1 ,2 let f ( x ,μ j ,σ j )=√ 21 πσ 2
j

e−( x −μ j )
(^2) /(2σ j (^2) )
.

3. Repeat until convergence
(or no significant improvement in L ( ̄ x ,γ,μ 1 ,μ 2 ,σ 1 ,σ 2 )):
(a) For i =1to n
i. p 1 ( xi )=γ f ( xi ,μ 1 σγ 1 ) f +( x (1 i ,μ−^1 γσ)^1 f )( xi ,μ 2 ,σ 2 )
ii. p 2 ( xi )= 1 − p 1 ( xi )
(b) For j = 1 , 2
i. μ i =

∑ n
∑ i = n^1 pj ( xi ) xi
i = 1 pj ( xi )
ii. σ^2 j =
∑ n
i = (^1) ∑ pnj ( xi )( xi −μ j )^2
i = 1 pj ( xi )
(c) γ=^1 n
∑ n
i = 1 p^1 ( xi ).
Algorithm 9.1: EM Algorithm for a Mixture of Two Gaussians.
In practice the initialization can affect the running time. One approach would be to
choose two random observation values as the initial means, and calculate the initial
varianceσ 12 by assuming all observations came from a single Gaussian with meanμ 1
(and similarly forσ 22 ). An initial value ofγ= 1 /2 is natural as it corresponds to each
point coming with equal probability from each distribution. More complex methods
for initialization can be used.
The following theorem shows that the likelihood function is nondecreasing through-
out the execution of the algorithm. Thus, the EM algorithm always terminates at a local
maximum. The algorithm is not guaranteed to find the maximum likelihood estimate,
which is a global maximum. Nevertheless, the algorithm gives good results in practice.
Theorem 9.11: Let γ t ,μ t 1 ,μ t 2 ,σ 1 t ,σ 2 tbe the estimated parameters at the end of the
tth iteration of the EM algorithm for a mixture of two Gaussians, with the initial values
corresponding to t = 0_. Then for all t_ ≥ 0 ,
L

(
x ̄,γ t +^1 ,μ t 1 +^1 ,μ t 2 +^1 ,σ 1 t +^1 ,σ 2 t +^1
)
≥ L
(
x ̄,γ t ,μ t 1 ,μ t 2 ,σ 1 t ,σ 2 t
)
.
Proof: The proof has two parts. We first show that, given valuesμ t 1 ,μ t 2 ,σ 1 t ,σ 2 t , the
algorithm’s choice ofγ t +^1 maximizes the likelihood, so that
γ t +^1 =arg maxγ L
(
x ̄,γ,μ t 1 ,μ t 2 ,σ 1 t ,σ 2 t
)
.
In particular, this implies
L
(
x ̄,γ t +^1 ,μ t 1 ,μ t 2 ,σ 1 t ,σ 2 t
)
≥ L
(
x ̄,γ t ,μ t 1 ,μ t 2 ,σ 1 t ,σ 2 t
)
.
Next we show that, givenγ t +^1 ,
(
μ t 1 +^1 ,μ t 2 +^1 ,σ 1 t +^1 ,σ 2 t +^1
)
=arg max
(μ 1 ,μ 2 ,σ 1 ,σ 2 )
L ( ̄ x ,γ t +^1 ,μ 1 ,μ 2 ,σ 1 ,σ 2 ).
the normal distribution
Thus

L
(
x ̄,γ t +^1 ,μ t 1 +^1 ,μ t 2 +^1 ,σ 1 t +^1 ,σ 2 t +^1
)
≥ L
(
x ̄,γ t +^1 ,μ t 1 ,μ t 2 ,σ 1 t ,σ 2 t
)
.
Let f ( x ,μ j ,σ j )=√ 21 πσ 2
j
e−( x −μ j )
(^2) /(2σ j (^2) )
, and let
L ( ̄ x ,γ,μ 1 ,μ 2 ,σ 1 ,σ 2 )=
∏ n
i = 1
(γ f ( xi ,μ 1 ,σ 1 )+(1−γ) f ( xi ,μ 2 ,σ 2 )).
Taking the derivative of L ( ̄ x ,γ,μ 1 ,μ 2 ,σ 1 ,σ 2 ) with respect toγwe have

∂ L
∂γ
=
∑ n
i = 1
⎡
⎣
∏
j = i
(
γ f ( xj ,μ 1 ,σ 1 )+(1−γ) f ( xj ,μ 2 ,σ 2 )) f ( xi ,μ 1 ,σ 1 )− f ( xi ,μ 2 ,σ 2 )
)
⎤
⎦
= L ( ̄ x ,γ,μ 1 ,μ 2 ,σ 1 ,σ 2 )
∑ n
i = 1
f ( xi ,μ 1 ,σ 1 )− f ( xi ,μ 2 ,σ 2 )
γ f ( xi ,μ 1 ,σ 1 )+(1−γ) f ( xi ,μ 2 ,σ 2 )
.
The likelihood function is never 0. So, when the derivative is 0,

∑ n
i = 1
f ( xi ,μ 1 ,σ 1 )
γ f ( xi ,μ 1 ,σ 1 )+(1−γ) f ( xi ,μ 2 ,σ 2 )
=
∑ n
i = 1
f ( xi ,μ 2 ,σ 2 )
γ f ( xi ,μ 1 ,σ 1 )+(1−γ) f ( xi ,μ 2 ,σ 2 )
=λ.
Now

λ=γλ+(1−γ)λ
=
∑ n
i = 1
γ f ( xi ,μ 1 ,σ 1 )
γ f ( xi ,μ 1 ,σ 1 )+(1−γ) f ( xi ,μ 2 ,σ 2 )
+
∑ n
i = 1
(1−γ) f ( xi ,μ 2 ,σ 2 )
γ f ( xi ,μ 1 ,σ 1 )+(1−γ) f ( xi ,μ 2 ,σ 2 )
= n.
Sinceλ= n ,

n =
∑ n
i = 1
f ( xi ,μ 1 ,σ 1 )
γ f ( xi ,μ 1 ,σ 1 )+(1−γ) f ( xi ,μ 2 ,σ 2 )
,
and hence

γ=
1
n
∑ n
i = 1
γ f ( xi ,μ 1 ,σ 1 )
γ f ( xi ,μ 1 ,σ 1 )+(1−γ) f ( xi ,μ 2 ,σ 2 )
=
1
n
∑ n
i = 1
p 1 ( xi ).
Thus, the choice of probabilities p 1 ( xi ) in each iteration maximizes the likelihood
function with respect toγ.

9.8 Exercises
Next we show that
(
μ t 1 +^1 ,μ t 2 +^1 ,σ 1 t +^1 ,σ 2 t +^1

)
=arg max
(μ 1 ,μ 2 ,σ 1 ,σ 2 )
L ( ̄ x ,γ t +^1 ,μ 1 ,μ 2 ,σ 1 ,σ 2 )
=arg max
(μ 1 ,μ 2 ,σ 1 ,σ 2 )
∏ n
i = 1
(γ f ( xi ,μ 1 ,σ 1 )+(1−γ) f ( xi ,μ 2 ,σ 2 )).
Taking the derivative with respect toμ 1 we have
∂ L
∂μ 1
=
∑ n
i = 1
2 γ( xi −μ 1 )
2 σ 12
f ( xi ,μ i ,σ 1 )
∏
j = i
(γ f ( xi ,μ 1 ,σ 1 )+(1−γ) f ( xi ,μ 2 ,σ 2 ))
=−
∑ n
i = 1
2 p 1 ( xi )( xi −μ 1 )
2 σ 12
L ( ̄ x ,γ t +^1 ,μ 1 ,μ 2 ,σ 1 ,σ 2 ).
When the derivative is 0, independent of the choice ofμ 2 ,σ 1 , andσ 2 ,wehave
∑ n
i = 1
2 p 1 ( xi )( xi −μ 1 )
2 σ 12
= 0 ,
or

μ 1 =
∑ n
∑ i =^1 p^1 ( xi ) xi
n
i = 1 p^1 ( xi )
.
Similar computations show the choices used in the algorithm forμ 1 ,μ 2 ,σ 1 , andσ 2
all individually maximize the likelihood function over that iteration givenγ t +^1 and
regardless of the values of the other parameters. We conclude

L
(
x ̄,γ t +^1 ,μ t 1 +^1 ,μ t 2 +^1 ,σ 1 t +^1 ,σ 2 t +^1
)
≥ L
(
x ̄,γ t +^1 ,μ t 1 ,μ t 2 ,σ 1 t ,σ 2 t
)
.  /theta
9.8. Exercises
Exercise 9.1: Prove that for anyμandσ>0,
∫∞

−∞
1
√
2 πσ
e−( x −μ)
(^2) /(2σ (^2) )
dx = 1.
Hint: As a lemma, use a transformation to polar coordinates to prove that
∫∞
−∞

∫∞
−∞
e−( u
(^2) +v (^2) )/(2σ (^2) )
du d v= 2 πσ^2.
Exercise 9.2: Let X be a standard normal random variable. Prove that E [ Xn ]=0for
odd n ≥1, and E [ Xn ]≥1 for even n ≥2. ( Hint: you can use integration by parts to
derive an expression for E [ Xn ] in terms of E [ Xn −^2 ].)

the normal distribution
Exercise 9.3: Recall that in discussing multivariate normal distributions we consid-
ered random variables

Y 1 = a 11 X 1 + a 12 X 2 +···+ a 1 nXn +μ 1 ;
Y 2 = a 21 X 1 + a 22 X 2 +···+ a 2 nXn +μ 2 ;
...
Ym = am 1 X 1 + am 2 X 2 +···+ amnXn +μ m.
Here the aij andμ i are constants, and the Xi are independent standard normal random
variables. Prove that

Cov ( Yi , Yj )=
∑ n
k = 1
ai , kaj , k.
Exercise 9.4: Letρ XY = Cov σ X ( X σ Y , Y )be the correlation coefficient of X and Y.

(a) Prove that for any two random variables X and Y ,|ρ XY |≤1.
(b) Prove that if X and Y are independent thenρ XY =0.
(c) Give an example of two random variables X and Y that are not independent but
ρ XY =0.

Exercise 9.5: Prove that for the bivariate normal distribution the right hand side of
Eqn. (9.1) and the expression given in (9.2) are equal.

Exercise 9.6: Let

X =
( 12
∑
i = 1
Ui
)
− 6 ,
where the Ui are independent and uniform on (0,1). Let Y be a random variable from
N (0,1). Using a computer program or other tools, find as good an approximation as you
can to max z |Pr( X ≤ z )−Pr( Y ≤ z )|. If you can, try to ensure that your approximation
is an upper bound.

Exercise 9.7: Write a program that generates independent uniform (0,1) random vari-
ables U and V , and compute

X =
√
−2ln V cos(2π U ), Y =
√
−2ln V sin(2π U )
to obtain two values according to the Box–Muller method. Repeat this experiment
100,000 times, and plot the corresponding distribution function for the 200,000 sam-
ples. Also, determine how many sampled values x satisfy| x |≤ k for k = 1 , 2 , 3 ,4. Do
your results seem reasonable? Explain.

Exercise 9.8: Write a program that generates independent uniform (0,1) random vari-
ables U and V , and repeats this process until finding a pair such that S = U^2 + V^2 <1.

9.8 exercises
Then compute

X = U
√
−2ln S
S
, Y = V
√
−2ln S
S
to obtain two values. Repeat this experiment 100,000 times, and plot the correspond-
ing distribution function for the 200,000 samples. Also, determine how many sampled
values x satisfy| x |≤ k for k = 1 , 2 , 3 ,4. Do your results seem reasonable? Explain.

Exercise 9.9: Suppose that X is normally distributed with expectation 1 and standard
deviation 0.25, Y is normally distributed with expectation 1.5 and standard deviation
0.4, and X and Y have correlation coefficient 0.4. Calculate the following probabilities:

(a) Pr( X + Y ≥2);
(b) Pr( X + Y ≥3);
(c) Pr( Y ≤ X );
(d) Perform the same calculations above but with correlation coefficient 0.6;
(e) How do these probabilities seem to change as the correlation ceofficient increases?
Explain.

Exercise 9.10: In this exercise we prove Theorem9.10.

(a) Prove that the marginal distribution of X 1 is N (μ 1 , /theta 11 ).
(b) Use f ( X 1 | X 2 )= f ( fX (^1 X , 2 X )^2 )to prove that the distribution of X 1 conditioned on X 2 = x ̄ 2
is N ( ̄μ 1 | 2 , /theta 1 | 2 ) where

μ ̄ 1 | 2 =μ ̄ 1 + /theta 12  /theta− 221 ( ̄ x 2 −μ ̄ 2 ) and  /theta 1 | 2 = /theta 22 − /theta T 12  /theta− 111  /theta 12.
(This is relatively easy to show for the bivariate distribution, more challenging for
a higher dimension multivariate distribution.)
(c) Show that when /theta 12 =0 the joint density function of X 1 and X 2 can be written
as a product of the marginal densities of X 1 and X 2 , proving that the two random
variables are independent.
Exercise 9.11: Assume that ( x 1 , yi ),...,( xn , yn )are n independent samples from a
mixture of two bivariate normal distributions. Write and analyze an EM algorithm for
computing a maximum likelihood estimate of the parameters of the mixed distribution.

Exercise 9.12: Assume that x 1 ,..., xn are n independent samples from a mixture
of three normal distributions. Write and analyze an EM algorithm for computing a
maximum likelihood estimate of the parameters of the mixed distribution.

Exercise 9.13: We briefly consider the setting of maximum a posterior estimation.
For convenience, assume that we are working over discrete spaces. Let PX ( x ,θ) be the
probability function of a discrete random variable X that depends on a parameterθ. Let
x 1 ,..., xn be n independent observations of X. We think now in terms of the following
setting: the parameter /etawas chosen according to some initial distribution, and we wish

the normal distribution
to find the valueθthat maximizes

Pr( /eta=θ| X 1 = x 1 , X 2 = x 2 ,..., Xn = xn ),
where the Xi correspond to random variables whose values are that of the observed
data. That is, we seek theθvalue that is most likely given our observations.

(a) Argue that
Pr( /eta=θ| X 1 = x 1 , X 2 = x 2 ,..., Xn = xn )
=
Pr( X 1 = x 1 , X 2 = x 2 ,..., Xn = xn | /eta=θ)Pr( /eta=θ)
Pr( X 1 = x 1 , X 2 = x 2 ,..., Xn = xn )
,
where here Pr( /eta=θ) is the distribution governing the initial choice ofθ.
(b) Suppose that Pr( /eta=θ) is uniform over all possible values. Then explain why
maximizing Pr( /eta=θ| X 1 = x 1 , X 2 = x 2 ,..., Xn = xn ) yields the same result as
maximizing Pr( X 1 = x 1 , X 2 = x 2 ,..., Xn = xn | /eta=θ).
(c) Modify the above argument to hold for settings where the observations X and the
parameter /etaare governed by continuous probability distributions.

Exercise 9.14: Let X be a standard normal random variable, and let Y = XZ , where
Z is a random variable independent of X that takes on the value 1 with probability 1/2
and the value−1 with probability 1/2.

(a) Show that Y is also a standard normal random variable.
(b) Explain why X and Y are not independent.
(c) Provide a reasoning that shows that X and Y are not jointly normal.
(d) Calculate the correlation coefficient of X and Y.

Exercise 9.15: The standard Cauchy distribution has density function

f ( x )=
1
π(1+ x^2 )
.
(a) Show that the standard Cauchy distribution is actually a probability distribution.
(b) Find the distribution function for the standard Cauchy distribution.
(c) Show that the standard Cauchy distribution does not have a finite expectation.
(d) Let X and Y be independent standard normal random variables. Show that the dis-
tribution of X / Y is the standard Cauchy distribution.

chapter ten

Entropy, Randomness,

and Information

Suppose that we have two biased coins. One comes up heads with probability 3/4, and
the other comes up heads with probability 7/8. Which coin produces more randomness
per flip? In this chapter, we introduce the entropy function as a universal measure of
randomness. In particular, we show that the number of independent unbiased random
bits that can be extracted from a sequence of biased coin flips corresponds to the entropy
of the coin. Entropy also plays a fundamental role in information and communication.
To demonstrate this role, we examine some basic results in compression and coding
and see how they relate to entropy. The main result we prove is Shannon’s coding
theorem for the binary symmetric channel, one of the fundamental results of the field
of information theory. Our proof of Shannon’s theorem uses several ideas that we have
developed in previous chapters, including Chernoff bounds, Markov’s inequality, and
the probabilistic method.

10.1 The Entropy Function
The entropy of a random variable is a function of its distribution that, as we shall see,
gives a measure of the randomness of the distribution.

Definition 10.1:

1. The entropy in bits of a discrete random variable X is given by

H ( X )=−
∑
x
Pr( X = x ) log 2 Pr( X = x ),
where the summation is over all values x in the range of X. Equivalently, we may
write
H ( X )= E
[
log 2
1
Pr( X )
]
.
2. The binary entropy function H ( p ) for a random variable that assumes only two
possible outcomes, one of which occurs with probability p, is
H ( p )=− p log 2 p −(1− p ) log 2 (1− p ).

entropy, randomness, and information
Figure 10.1: The binary entropy function.
We define H (0)= H (1)=0, so the binary entropy function is continuous in the inter-
val [0,1]. The function is drawn in Figure10.1.
For our two biased coins, the entropy of the coin that comes up heads with proba-
bility 3/4is

H
(
3
4
)
=−
3
4
log 2
3
4
−
1
4
log 2
1
4
= 2 −
3
4
log 23 ≈ 0. 8113 ,
while the entropy of the coin that comes up heads with probability 7/8is

H
(
7
8
)
=−
7
8
log 2
7
8
−
1
8
log 2
1
8
= 3 −
7
8
log 27 ≈ 0. 5436.
Hence the coin that comes up heads with probability 3/4 has a larger entropy.
Taking the derivative of H ( p ),
dH ( p )
dp

=−log 2 p +log 2 (1− p )=log 2
1 − p
p
,
we see that H ( p ) is maximized when p = 1 /2 and that H (1/2)=1 bit. One way of
interpreting this statement is to say: each time we flip a two-sided coin, we get out at
most 1 bit worth of randomness, and we obtain exactly 1 bit of randomness when the
coin is fair. Although this seems quite clear, it is not yet clear in what sense H (3/4)=
2 −^34 log 2 3 means that we obtain H (3/4) random bits each time we flip a coin that
lands heads with probability 3/4. We clarify this later in the chapter.
As another example, the entropy of a standard six-sided die that comes up on each
side with probability 1/6 has entropy log 2 6. In general, a random variable that has n
equally likely outcomes has entropy

−
∑ n
i = 1
1
n
log 2
1
n
=log 2 n.
10.1the entropy function
The entropy of an eight-sided die is therefore 3 bits. This result should seem quite
natural; if the faces of the die were numbered from 0 to 7 written in binary, then the
outcome of the die roll would give a sequence of 3 bits uniform over the set{ 0 , 1 }^3 ,
which is equivalent to 3 bits generated independently and uniformly at random.
It is worth emphasizing that the entropy of a random variable X depends not on the
values that X can take but only on the probability distribution of X over those values.
The entropy of an eight-sided die does not depend on what numbers are on the faces of
the die; it only matters that all eight sides are equally likely to come up. This property
does not hold for the expectation or variance of X , but it does makes sense for a measure
of randomness. To measure the randomness in a die, we should not care about what
numbers are on the faces but only about how often the die comes up on each side.
Often in this chapter we consider the entropy of a sequence of independent random
variables, such as the entropy of a sequence of independent coin flips. For such situa-
tions, the following lemma allows us to consider the entropy of each random variable
to find the entropy of the sequence.

Lemma 10.1: Let X 1 and X 2 be independent random variables, and let Y =( X 1 , X 2 ).
Then

H ( Y )= H ( X 1 )+ H ( X 2 ).
Of course, the lemma is trivially extended by induction to the case where Y is any finite
sequence of independent random variables.

Proof: In what follows, the summations are to be taken over all possible values that
can be assumed by X 1 and X 2. The result follows by using the independence of X 1 and
X 2 to simplify the expression:

H ( Y )=−
∑
x 1 , x 2
Pr(( X 1 , X 2 )=( x 1 , x 2 )) log 2 Pr(( X 1 , X 2 )=( x 1 , x 2 ))
=−
∑
x 1 , x 2
Pr( X 1 = x 1 )Pr( X 2 = x 2 ) log 2 (Pr( X 1 = x 1 )Pr( X 2 = x 2 ))
=−
∑
x 1 , x 2
Pr( X 1 = x 1 )Pr( X 2 = x 2 )(log 2 Pr( X 1 = x 1 )+log 2 Pr( X 2 = x 2 ))
=−
∑
x 1
∑
x 2
Pr( X 2 = x 2 )Pr( X 1 = x 1 ) log 2 Pr( X 1 = x 1 )
−
∑
x 2
∑
x 1
Pr( X 1 = x 1 )Pr( X 2 = x 2 ) log 2 Pr( X 2 = x 2 )
=−
(
∑
x 1
Pr( X 1 = x 1 ) log 2 Pr( X 1 = x 1 )
)(
∑
x 2
Pr( X 2 = x 2 )
)
−
(
∑
x 2
Pr( X 2 = x 2 ) log 2 Pr( X 2 = x 2 )
)(
∑
x 1
Pr( X 1 = x 1 )
)
=−
∑
x 1
Pr( X 1 = x 1 ) log 2 Pr( X 1 = x 1 )−
∑
x 2
Pr( X 2 = x 2 ) log 2 Pr( X 2 = x 2 )
= H ( X 1 )+ H ( X 2 ).  /theta
entropy, randomness, and information
10.2 Entropy and Binomial Coefficients
As a prelude to showing the various applications of entropy, we first demonstrate how
it naturally arises in a purely combinatorial context.

Lemma 10.2: Suppose that nq is an integer in the range [0, n ]. Then

2 nH ( q )
n + 1
≤
(
n
nq
)
≤ 2 nH ( q ).
Proof: The statement is trivial if q =0or q =1, so assume that 0< q <1. To prove
the upper bound, notice that by the binomial theorem we have
(
n
nq

)
qqn (1− q )(1− q ) n ≤
∑ n
k = 0
(
n
k
)
qk (1− q ) n − k ≤( q +(1− q )) n = 1.
Hence,
(
n
nq

)
≤ q − qn (1− q )−(1− q ) n = 2 − qn log^2 q 2 −(1− q ) n log^2 (1− q )= 2 nH ( q ).
For the lower bound, we know that

( n
nq
)
∑ qqn (1− q )(1− q ) n is one term of the expression
n
k = 0

( n
k
)
qk (1− q ) n − k. We show that it is the largest term. Consider the difference
between two consecutive terms as follows:
(
n
k

)
qk (1− q ) n − k −
(
n
k + 1
)
qk +^1 (1− q ) n − k −^1
=
(
n
k
)
qk (1− q ) n − k
(
1 −
n − k
k + 1
q
1 − q
)
.
This difference is nonnegative whenever

1 −
n − k
k + 1
q
1 − q
≥ 0
or (equivalently, after some algebra) whenever

k ≥ qn − 1 + q.
The terms are therefore increasing up to k = qn and decreasing after that point. Thus
k = qn gives the largest term in the summation.
Since the summation has n +1 terms and since

( n
nq
)
qqn (1− q )(1− q ) n is the largest
term, we have
(
n
nq

)
qqn (1− q )(1− q ) n ≥
1
n + 1
or
(
n
nq

)
≥
q − qn (1− q )−(1− q ) n
n + 1
=
2 nH ( q )
n + 1
.  /theta
10.2 entropy and binomial coefficients
We often use the following slightly more specific corollary.

Corollary 10.3: When 0 ≤ q ≤ 1 / 2 ,

(
n
nq 
)
≤ 2 nH ( q ); (10.1)
similarly, when 1 / 2 ≤ q ≤ 1 ,

(
n
 nq 
)
≤ 2 nH ( q ). (10.2)
For 1 / 2 ≤ q ≤ 1 ,

2 nH ( q )
n + 1
≤
(
n
nq 
)
; (10.3)
similarly, when 0 ≤ q ≤ 1 / 2 ,

2 nH ( q )
n + 1
≤
(
n
 nq 
)
. (10.4)
Proof: We first prove Eqn. (10.1); the proof of Eqn. (10.2) is entirely similar. When
0 ≤ q ≤ 1 /2,

(
n
nq 

)
qqn (1− q )(1− q ) n ≤
(
n
nq 
)
q^ qn (1− q ) n −
 qn ≤
∑ n
k = 0
( n
k
)
qk (1− q ) n − k = 1 ,
from which we can proceed exactly as in Lemma10.2.
Eqn. (10.3) holds because, when q ≥ 1 /2, Lemma10.2gives
(
n
nq 

)
≥
2 nH (^ nq / n )
n + 1
≥
2 nH ( q )
n + 1
;
Eqn. (10.4) is derived similarly.  /theta

Although these bounds are loose, they are sufficient for our purposes. The relation
between the combinatorial coefficients and the entropy function arises repeatedly in
the proofs of this chapter when we consider a sequence of biased coin tosses, where
the coin lands heads with probability p > 1 /2. Applying the Chernoff bound, we know
that, for sufficiently large n , the number of heads will almost always be close to np. Thus
the sequence will almost always be one of roughly

( n
np
)
≈ 2 nH ( p )sequences, where the
approximation follows from Lemma10.2. Moreover, each such sequence occurs with
probability roughly

pnp (1− p ) n (1− p )≈ 2 − nH ( p ).
entropy, randomness, and information
Hence, when we consider the outcome of n flips with a biased coin, we can essen-
tially restrict ourselves to the roughly 2 nH ( p )outcomes that occur with roughly equal
probability.

10.3 Entropy: A Measure of Randomness
One way of interpreting the entropy of a random variable is as a measure of how many
unbiased, independent bits can be extracted, on average, from one instantiation of the
random variable. We consider this question in the context of a biased coin, showing
that, for sufficiently large n , the expected number of bits that can be extracted from n
flips of a coin that comes up heads with probability p > 1 /2 is essentially nH ( p ). In
other words, on average, one can generate approximately H ( p ) independent bits from
each flip of a coin with entropy H ( p ). This result can be generalized to other random
variables, but we focus on the specific case of biased coins here (and throughout this
chapter) to keep the arguments more transparent.
We begin with a definition that clarifies what we mean by extracting random bits.

Definition 10.2: Let | y | be the number of bits in a sequence of bits y. An extraction
function Ext takes as input the value of a random variable X and outputs a sequence
of bits y such that

Pr(Ext( X )= y || y |= k )= 1 / 2 k
whenever Pr(| y |= k )> 0_._

In the case of a biased coin, the input X is the outcome of n flips of our biased coin.
The number of bits in the output is not fixed but instead can depend on the input. If
the extraction function outputs k bits, we can think of these bits as having been gener-
ated independently and uniformly at random, since each sequence of k bits is equally
likely to appear. Also, there is nothing in the definition that requires that the extraction
function be efficient to compute. We do not concern ourselves with efficiency here,
although we do consider an efficient extraction algorithm in Exercise10.12.
As a first step toward proving our results about extracting unbiased bits from biased
coins, we consider the problem of extracting random bits from a uniformly distributed
integer random variable. For example, let X be an integer chosen uniformly at ran-
dom from{ 0 ,..., 7 }, and let Y be the sequence of 3 bits obtained when we write
X as a binary number. If X =0 then Y =000, and if X =7 then Y =111. It is
easy to check that every sequence of 3 bits is equally likely to arise, so we have
a trivial extraction function Ext by associating any input X with the corresponding
output Y.
Things are slightly harder when X is uniform over{ 0 ,..., 11 }.If X ≤7, then we can
again let Y be the sequence of 3 bits obtained when we write X in binary. This leaves the
case X ∈{ 8 , 9 , 10 , 11 }. We can associate each of these four possibilities with a distinct
sequence of 2 bits, for example, by letting Y be the sequence of 2 bits obtained from
writing X −8 as a binary number. Thus, if X =8 then Y =00, and if X =11 then

10.3 entropy: a measure of randomness
Figure 10.2: Extraction functions for numbers that are chosen uniformly at random from{ 0 ,..., 7 }
and{ 0 ,..., 11 }.
Y =11. The entire extraction function is shown in Figure10.2. Every 3-bit sequence
arises with the same probability 1/12, and every 2-bit sequence arises with the same
probability 1/12, so Definition10.2is satisfied.
We generalize from these examples to the following theorem.

Theorem 10.4: Suppose that the value of a random variable X is chosen uniformly
at random from the integers { 0 ,..., m − 1 } , so that H ( X )=log 2 m. Then there is an
extraction function for X that outputs on average at least log 2 m − 1 =
 H ( X )− 1
independent and unbiased bits.
Proof: If m >1 is a power of 2, then the extraction function can simply output the
bit representation of the input X using log 2 m bits. (If m =1, then it outputs nothing
or, equivalently, an empty sequence.) All output sequences have log 2 m bits, and all
sequences of log 2 m bits are equally likely to appear, so this satisfies Definition10.2.If
m is not a power of 2, then matters become more complicated. We describe the extrac-
tion function recursively. (A nonrecursive description is given in Exercise10.8.) Let
α=
log 2 m .If X ≤ 2 α−1, then the function outputs theα-bit binary representation
of X ; all sequences ofαbits are equally likely to be output in this case. If X ≥ 2 α,
then X − 2 αis uniformly distributed in the set{ 0 ,..., m − 2 α− 1 }, which is smaller
than the set{ 0 ,..., m }. The extraction function can then recursively produce the output
from the extraction function for the variable X − 2 α.
The recursive extraction function maintains the property that, for every k , each of the
2 k sequences of k bits is output with the same probability. We claim by induction that
the expected number of unbiased, independent bits produced by this extraction function
is at least log 2 m −1. The cases where m is a power of 2 are trivial. Otherwise, by
induction, the number of bits Y in the output satisfies
E [ Y ]≥
2 α
m
α+
m − 2 α
m
( log 2 ( m − 2 α)−1)
=α−
m − 2 α
m
(α−
log 2 ( m − 2 α)+1).
entropy, randomness, and information
Suppose log 2 ( m − 2 α)=β, where 0≤β≤α−1. Then ( m − 2 α)/ m is maximized
when m is as large as possible, which corresponds to m = 2 α+ 2 β+^1 −1. Hence

E [ Y ]≥α−
2 β+^1 − 1
2 α+ 2 β+^1 − 1
(α−β+1)
≥α−
1
2 α−β−^1 + 1
(α−β+1)
≥α− 1 ,
where we use 2α+ 2 β+^1 − 1 ≥(2β+^1 −1)(2α−β−^1 +1) for the second line, and
x /(2 x −^2 +1)≤1 for integers x ≥2 for the third line.
This completes the induction.  /theta

We use Theorem10.4in our proof of the main result of this section.

Theorem 10.5: Consider a coin that comes up heads with probability p > 1 / 2 .For
any constant δ> 0 and for n sufficiently large:

1. there exists an extraction function Ext that outputs, on an input sequence of n inde-
pendent flips, an average of at least (1−δ) nH ( p ) independent random bits; and
2. the average number of bits output by any extraction function Ext on an input
sequence of n independent flips is at most nH ( p ).

Proof: We begin by describing an extraction function that generates, on average, at
least (1−δ) nH ( p ) random bits from n flips of the biased coin. We saw before that, in
the case of a biased coin, the outcome of n flips is most likely to be one of roughly
2 nH ( p )sequences, each occurring with probability roughly 2− nH ( p ). If we actually had
a uniform distribution of this type, we could use the extraction function that we have
just described for numbers chosen uniformly at random to obtain on average almost
nH ( p ) uniform random bits. In what follows, we handle the technical details that arise
because the distribution is not exactly uniform.
There are

( n
j
)
possible sequences with exactly j heads, and each of them has the same
probability of occurring, pj (1− p ) n − j. For each value of j ,0≤ j ≤ n , we map each of
the

( n
j
)
sequences with j heads to a unique integer in the set
{
0 ,...,
( n
j
)
− 1
}
. When
j heads come up, we map the sequence to the corresponding number. Conditioned on
there being j heads, this number is uniform on the integers

{
0 ,...,
( n
j
)
− 1
}
, and hence
we can apply the extraction function of Theorem10.4designed for this case. Let Z be
a random variable representing the number of heads flipped, and let B be the random
variable representing the number of bits our extraction function produces. Then

E [ B ]=
∑ n
k = 0
Pr( Z = k ) E [ B | Z = k ]
and, by Theorem10.4,

E [ B | Z = k ]≥
⌊
log 2
(
n
k
)⌋
− 1.
Letε<min( p − 1 / 2 , 1 − p ) represent a constant to be determined. We compute a
lower bound for E [ B ] by considering only values of k with n ( p −ε)≤ k ≤ n ( p +ε).

10.3 entropy: a measure of randomness
For every such k ,
(
n
k

)
≥
(
n
n ( p +ε)
)
≥
2 nH ( p +ε)
n + 1
,
where the last inequality follows from Corollary10.3. Hence

E [ B ]≥
 n (∑ p +ε)
k =
 n ( p −ε)
Pr( Z = k ) E [ B | Z = k ]
≥
 n (∑ p +ε)
k =
 n ( p −ε)
Pr( Z = k )
(⌊
log 2
(
n
k
)⌋
− 1
)
≥
(
log 2
2 nH ( p +ε)
n + 1
− 2
)  n (∑ p +ε)
k =
 n ( p −ε)
Pr( Z = k )
≥( nH ( p +ε)−log 2 ( n +1)−2)Pr(| Z − np |≤ε n ).
Now E [ Z ]= np , and Pr(| Z − np |>ε n ) can be bounded by using the Chernoff bound
of Eqn. (4.6), giving

Pr(| Z − np |>ε n )≤2e− n ε
(^2) / 3 p
.
Hence
E [ B ]≥( nH ( p +ε)−log 2 ( n +1)−2)(1−2e− n ε
(^2) / 3 p
).
We conclude that, for any constantδ>0, we can have
E [ B ]≥(1−δ) nH ( p )
by choosingεsufficiently small and n sufficiently large. For example, for sufficiently
smallε,
nH ( p +ε)≥(1−δ/3) nH ( p ),
and when n >(3 p /ε^2 )ln(6/δ)wehave
1 −2e− n ε
(^2) / 3 p
≥ 1 −δ/ 3.
Hence, with these choices,
E [ B ]≥((1−δ/3) nH ( p )−log 2 ( n +1)−2)(1−δ/3).
As long as we now also choose n sufficiently large that (δ/3) nH ( p ) is greater than
log 2 ( n +1)+2, we have
E [ B ]≥((1− 2 δ/3) nH ( p ))(1−δ/3)≥(1−δ) nH ( p ),
proving there exists an extraction function that can extract (1−δ) nH ( p ) independent
and uniform bits on average from n flips of the biased coin.
We now show that no extraction function can obtain more than nH ( p ) bits on
average. The proof relies on the following basic fact: If an input sequence x occurs
with probability q , then the corresponding output sequence Ext( x ) can have at most

entropy, randomness, and information
|Ext( x )|≤log 2 (1/ q ) bits. This is because all sequences with|Ext( x )|bits would have
probability at least q ,so

2 |Ext( x )| q ≤ 1 ,
giving the desired bound on Ext( x ). Given any extraction function, if B is a random
variable representing the number of bits our extraction function produces on input X ,
then

E [ B ]=
∑
x
Pr( X = x )|Ext( x )|
≤
∑
x
Pr( X = x ) log 2
1
Pr( X = x )
= E
[
log 2
1
Pr( X )
]
= H ( X ).  /theta
Another natural question to ask is how we can generate biased bits from an unbiased
coin. This question is partially answered in Exercise10.11.

10.4 Compression
A second way of interpreting the entropy value comes from compression. Again sup-
pose we have a coin that comes up heads with probability p > 1 /2 and that we flip
it n times, keeping track of which flips are heads and which flips are tails. We could
represent every outcome by using one bit per flip, with 0 representing heads and 1 rep-
resenting tails, and use a total of n bits. If we take advantage of the fact that the coin is
biased, we can do better on average. For example, suppose that p = 3 /4. For a pair of
consecutive flips, we use 0 to represent that both flips were heads, 10 to represent that
the first flip was heads and the second tails, 110 to represent that the first flip was tails
and the second heads, and 111 to represent that both flips were tails. Then the average
number of bits we use per pair of flips is

1 ·
9
16
+ 2 ·
3
16
+ 3 ·
3
16
+ 3 ·
1
16
=
27
16
< 2.
Hence, on average, we can use less than the 1 bit per flip of the standard scheme by
breaking a sequence of n flips into pairs and representing each pair in the manner shown.
This is an example of compression.
It is worth emphasizing that the representation that we used here has a special prop-
erty: if we write the representation of a sequence of flips, it can be uniquely decoded
simply by parsing it from left to right. For example, the sequence

011110
corresponds to two heads, followed by two tails, followed by a heads and a tails.
There is no ambiguity, because no other sequence of flips could produce this output.

10.4compression
Our representation has this property because no bit sequence we use to represent a pair
of flips is the prefix of another bit sequence used in the representation. Representations
with this property are called prefixcodes, which are discussed further in Exercise10.15.
Compression continues to be a subject of considerable study. When storing or trans-
mitting information, saving bits usually corresponds to saving resources, so finding
ways to reduce the number of used bits by taking advantage of the data’s structure is
often worthwhile.
We consider here the special case of compressing the outcome of a sequence of
biased coin flips. For a biased coin with entropy H ( p ), we show (a) that the outcome
of n flips of the coin can be represented by approximately nH ( p ) bits on average and
(b) that approximately nH ( p ) bits on average are necessary. In particular, any represen-
tation of the outcome of n flips of a fair coin essentially requires n bits. The entropy is
therefore a measure of the average number of bits generated by each coin flip after com-
pression. This argument can be generalized to any discrete random variable X , so that
n independent, identically distributed random variables X 1 , X 2 ,..., Xn with the same
distribution X can be represented using approximately nH ( X ) bits on average. In the
setting of compression, entropy can be viewed as measuring the amount of information
in the input sequence. The larger the entropy of the sequence, the more bits are needed
in order to represent it.
We begin with a definition that clarifies what we mean by compression in this con-
text.

Definition 10.3: A compression function Com takes as input a sequence of n coin
flips, given as an element of { H , T } n, and outputs a sequence of bits such that each
input sequence of n flips yields a distinct output sequence.

Definition10.3is rather weak, but it will prove sufficient for our purposes. Usually,
compression functions must satisfy stronger requirements; for example, we may require
a prefix code to simplify decoding. Using this weaker definition makes our lower-bound
proof stronger. Also, though we are not concerned here with the efficiency of compress-
ing and decompressing procedures, there are very efficient compression schemes that
perform nearly optimally in many situations. We will consider an efficient compression
scheme in Exercise10.17.
The following theorem formalizes the relationship between the entropy of a biased
coin and compression.

Theorem 10.6: Consider a coin that comes up heads with probability p > 1 / 2 .For
any constant δ> 0 , when n is sufficiently large :

1. thereexistsacompressionfunction Com suchthattheexpectednumberofbitsoutput
by Com on an input sequence of n independent coin flips is at most (1+δ) nH ( p );
and
2. the expected number of bits output by any compression function on an input
sequence of n independent coin flips is at least (1−δ) nH ( p ).

Theorem10.6is quite similar to Theorem10.5. The lower bound on the expected num-
ber of bits output by any compression function is slightly weaker. In fact, we could raise

entropy, randomness, and information
this lower bound to nH ( p ) if we insisted that the code be a prefix code – so that no out-
put is the prefix of any other – but we do not prove this here. The compression function
we design to prove an upper bound on the expected number of output bits does yield
a prefix code. Our construction of this compression function follows roughly the same
intuition as Theorem10.5. We know that, with high probability, the outcome from the
n flips will be one of roughly 2 nH ( p )sequences with roughly np heads. We can use
about nH ( p ) bits to represent each one of these sequences, yielding the existence of an
appropriate compression function.

Proof of Theorem10.6: We first show that there exists a compression function as guar-
anteed by the theorem. Letε>0 be a suitably small constant with p −ε> 1 /2. Let
X be the number of heads in n flips of the coin. The first bit output by the compres-
sion function we use as a flag. We set it to 0 if there are at least n ( p −ε) heads in the
sequence and to 1 otherwise. When the first bit is a 1, the compression function uses
the expensive default scheme, using 1 bit for each of the n flips. This requires that n + 1
total bits be output; however, by the Chernoff bound of Eqn. (4.5), the probability that
this case happens is bounded by

Pr( X < n ( p −ε))≤e− n ε
(^2) / 2 p
.
Now let us consider the case where there are at least n ( p −ε) heads. The number of
coin-flip sequences of this form is
∑ n
j = n ( p −ε)

(
n
j
)
≤
∑ n
j = n ( p −ε)
(
n
 n ( p −ε)
)
≤
n
2
2 nH ( p −ε).
The first inequality arises because the binomial terms are decreasing as long as j > n /2,
and the second is a consequence of Corollary10.3. For each such sequence of coin
flips, the compression function can assign a unique sequence of exactly nH ( p −ε)+
log 2 n bits to represent it, since

2
 nH ( p −ε)+log^2 n ≥
n
2
2 nH ( p −ε).
Including the flag bit, it therefore takes at most nH ( p −ε)+log 2 n +1 bits to represent
the sequences of coin flips with this many heads.
Totaling these results, we find that the expected number of bits required by the com-
pression function is at most

e− n ε
(^2) / 2 p
( n +1)+(1−e− n ε
(^2) / 2 p
)( nH ( p −ε)+log 2 n +1)≤(1+δ) nH ( p ),
where the inequality holds by first takingεsufficiently small and then taking n suffi-
ciently large in a manner similar to that of Theorem10.5.
We now show the lower bound. To begin, recall that the probability that a specific
sequence with k heads is flipped is pk (1− p ) n − k. Because p > 1 /2, if sequence S 1 has
more heads than another sequence S 2 , then S 1 is more likely to appear than S 2. Also,
we have the following lemma.

10.5 ∗Coding: Shannon’s Theorem
Lemma 10.7: If sequence S 1 is more likely than S 2 , then the compression function that
minimizes the expected number of bits in the output assigns a bit sequence to S 2 that is
at least as long as S 1_._

Proof: Suppose that a compression function assigns a bit sequence to S 2 that is shorter
than the bit sequence it assigns to S 1. We can improve the expected number of bits
output by the compression function by switching the output sequences associated with
S 1 and S 2 , and therefore this compression function is not optimal.  /theta

Hence sequences with more heads should get shorter strings from an optimal compres-
sion function.
We also make use of the following simple fact. If the compression function assigns
distinct sequences of bits to represent each of s coin-flip sequences, then one of the out-
put bit sequences for the s input sequences must have length at least log 2 s −1 bits. This
is because there are at most 1+ 2 + 4 +···+ 2 b = 2 b +^1 −1 distinct bit sequences
with up to b bits, so if each of s sequences of coin flips is assigned a bit sequence of at
most b bits, then we must have 2 b +^1 > s and hence b >log 2 s −1.
Fix a suitably smallε>0 and count the number of input sequences that have
( p +ε) n heads. There are

( n
( p +ε) n 
)
sequences with ( p +ε) n heads and, by
Corollary10.3,
(
n
( p +ε) n 

)
≥
2 nH ( p +ε)
n + 1
.
Hence any compression function must output at least log 2 (2 nH ( p +ε)/( n +1))− 1 =
nH ( p +ε)−log 2 ( n +1)−1 bits on at least one of the sequences of coin flips with
( p +ε) n heads. The compression function that minimizes the expected output length
must therefore use at least this many bits to represent any sequence with fewer heads,
by Lemma10.7.
By the Chernoff bound of Eqn. (4.2), the number of heads X satisfies
Pr( X ≥ n ( p +ε))≤Pr( X ≥ n ( p +ε− 1 / n ))≤e− n (ε−^1 / n )

(^2) / 3 p
≤e− n ε
(^2) / 12 p
as long as n is sufficiently large (specifically, n > 2 /ε). We thus obtain, with probability
at least 1−e− n ε
(^2) / 12 p
, an input sequence with fewer than n ( p +ε)heads, and by our
previous reasoning the compression function that minimizes the expected output length
must still output at least nH ( p +ε)−log 2 ( n +1)−1 bits in this case. The expected
number of output bits is therefore at least
(1−e− n ε
(^2) / 12 p
)( nH ( p +ε)−log 2 ( n +1)−1).
This can be made to be at least (1−δ) nH ( p ) by first takingεto be sufficiently small
and then taking n to be sufficiently large.  /theta

10.5. ∗ Coding: Shannon’s Theorem
We have seen how compression can reduce the expected number of bits required to
represent data by changing the representation of the data. Coding also changes the

entropy, randomness, and information
representation of the data. Instead of reducing the number of bits required to represent
the data, however, coding adds redundancy in order to protect the data against loss
or errors.
In coding theory, we model the information being passed from a sender to a receiver
through a channel. The channel may introduce noise, distorting the value of some of
the bits during the transmission. The channel can be a wired connection, a wireless
connection, or a storage network. For example, if I store data on a recordable medium
and later try to read it back, then I am both the sender and the receiver, and the storage
medium acts as the channel. In this section, we focus on one specific type of channel.

Definition 10.4: The input to a binary symmetric channel with parameter p is a
sequence of bits x 1 , x 2 ,... , and the output is a sequence of bits y 1 , y 2 ,... , such that
Pr( xi = yi )= 1 − p independently for each i. Informally, each bit sent is flipped to the
wrong value independently with probability p.

To get useful information out of the channel, we may introduce redundancy to help
protect against the introduction of errors. As an extreme example, suppose the sender
wants to send the receiver a single bit of information over a binary symmetric channel.
To protect against the possibility of error, the sender and receiver agree to repeat the bit
n times. If p < 1 /2, a natural decoding scheme for the receiver is to look at the n bits
received and decide that the value that was received more frequently is the bit value
the sender intended. The larger n is, the more likely the receiver determines the correct
bit; by repeating the bit enough times, the probability of error can be made arbitrarily
small. This example is considered more extensively in Exercise10.18.
Coding theory studies the trade-off between the amount of redundancy required and
the probability of a decoding error over various types of channels. For the binary sym-
metric channel, simply repeating bits may not be the best use of redundancy. Instead
we consider more general encoding functions.

Definition 10.5: A ( k , n ) encoding function Enc:{ 0 , 1 } k →{ 0 , 1 } ntakes as input
a sequence of k bits and outputs a sequence of n bits. A ( k , n ) decoding function
Dec:{ 0 , 1 } n →{ 0 , 1 } ktakes as input a sequence of n bits and outputs a sequence of
k bits.

With coding, the sender takes a k -bit message and encodes it into a block of n ≥ k
bits via the encoding function. These bits are then sent over the channel. The receiver
examines the n bits received and attempts to determine the original k -bit message using
the decoding function.
Given a binary channel with parameter p and a target encoding length of n , we wish
to determine the largest value of k so that there exist ( k , n ) encoding and decoding
functions with the property that, for any input sequence of k bits, with suitably large
probability the receiver decodes the correct input from the corresponding n -bit encod-
ing sequence after it has been distorted by the channel.
Let m ∈{ 0 , 1 } k be the message to be sent and Enc( m ) the sequence of bits sent over
the channel. Let the random variable X denote the sequence of received bits. We require
that Dec( X )= m with probability at least 1−γfor all possible messages m and a pre-
chosen constantγ. If there were no noise, then we could send the original k bits over

10.5 ∗ coding: shannon’s theorem
the channel. The noise reduces the information that the receiver can extract from each
bit sent, and so the sender can reliably send messages of only about k = n (1− H ( p ))
bits within each block of n bits. This result is known as Shannon’s theorem, which we
prove in the following form.

Theorem 10.8: For a binary symmetric channel with parameter p < 1 / 2 and for any
constants δ, γ > 0 , when n is sufficiently large :

1. for any k ≤ n (1− H ( p )−δ) , there exist ( k , n ) encoding and decoding functions
such that the probability the receiver fails to obtain the correct message is at most
γ for every possible k-bit input message ; and
2. there are no ( k , n ) encoding and decoding functions with k ≥ n (1− H ( p )+δ) such
that the probability of decoding correctly is at least γ for a k-bit input message
chosen uniformly at random.

Proof: We first prove the existence of suitable ( k , n ) encoding and decoding functions
when k ≤ n (1− H ( p )−δ) by using the probabilistic method. In the end, we want our
encoding and decoding functions to have error probability at mostγon every possi-
ble input. We begin with a weaker result, showing that there exist appropriate coding
functions when the input is chosen uniformly at random from all k -bit inputs.
The encoding function assigns to each of the 2 k strings an n -bit codeword chosen
independently and uniformly at random from the space of all n -bit sequences. Label
these codewords X 0 , X 1 ,..., X 2 k − 1. The encoding function simply outputs the code-
word assigned to the k -bit message using a large lookup table containing an entry for
each k -bit string. (You may be concerned that two codewords may turn out to be the
same; the probability of this is very small and is handled in the analysis that follows.)
To describe the decoding function, we provide a decoding algorithm based on the
lookup table for the encoding function, which we may assume the receiver possesses.
The decoding algorithm makes use of the fact that the receiver expects the channel to
make roughly pn errors. The receiver therefore looks for a codeword that differs from
the n bits received in between ( p −ε) n and ( p +ε) n places for some suitably small
constantε>0. If just one codeword has this property, then the receiver will assume
that this was the codeword sent and will recover the message accordingly. If more than
one codeword has this property, the decoding algorithm fails. The decoding algorithm
described here requires exponential time and space. As in the rest of this chapter, we
are not now concerned with efficiency issues.
The corresponding ( k , n ) decoding function can be obtained from the algorithm by
simply running through all possible n -bit sequences. Whenever a sequence decodes
properly with the foregoing algorithm, the output of the decoding function for that
sequence is set to the k -bit sequence associated with the corresponding codeword.
Whenever the algorithm fails, the output for the sequence can be any arbitrary sequence
of k bits. For the decoding function to fail, at least one of the two following events must
occur:

 /thetathe channel does not make between ( p −ε) n and ( p +ε) n errors; or
 /thetawhen a codeword is sent, the received sequence differs from some other codeword in
between ( p −ε) n and ( p +ε) n places.
entropy, randomness, and information
The path of the proof is now clear. A Chernoff bound can be used to show that, with
high probability, the channel does not make too few or too many errors. Conditioning
on the number of errors being neither too few nor too many, the question becomes
how large k can be while ensuring that, with the required probability, the received
sequence does not differ from multiple codewords in between ( p −ε) n and ( p +ε) n
places.
Now that we have described the encoding and decoding functions, we establish the
notation to be used in the analysis. Let R be the received sequence of bits. For sequences
s 1 and s 2 of n bits, we write( s 1 , s 2 ) for the number of positions where these sequences
differ. This value( s 1 , s 2 ) is referred to as the Hamming distance between the two
strings. We say that the pair ( s 1 , s 2 ) has weight

w( s 1 , s 2 )= p ( s^1 , s^2 )(1− p ) n −( s^1 , s^2 ).
The weight corresponds to the probability that s 2 is received when s 1 is sent over
the channel. We introduce random variables S 0 , S 1 ,..., S 2 k − 1 and W 0 , W 1 ,..., W 2 k − 1
defined as follows. The set Si is the set of all received sequences that decode to Xi. The
value Wi is given by

Wi =
∑
r ∈/ Si
w( Xi , r ).
The Si and Wi are random variables that depend only on the random choices of
X 0 , X 1 ,..., X 2 k − 1. The variable Wi represents the probability that, when Xi is sent, the
received sequence R does not lie in Si and hence is decoded incorrectly. It is also helpful
to express Wi in the following way: letting Ii , s be an indicator random variable that is 1
if s ∈/ Si and 0 otherwise, we can write

Wi =
∑
r
Ii , r w( Xi , r ).
We start by bounding E [ Wi ]. By symmetry, E [ Wi ] is the same for all i , so we bound
E [ W 0 ]. Now

E [ W 0 ]= E
[
∑
r
I 0 , r w( X 0 , r )
]
=
∑
r
E [w( X 0 , r ) I 0 , r ].
We split the sum into two parts. Let T 1 ={ s :|( X 0 , s )− pn |>ε n }and T 2 ={ s :
|( X 0 , s )− pn |≤ε n }, whereε>0 is some constant to be determined. Then

∑
r
E [w( X 0 , r ) I 0 , r ]=
∑
r ∈ T 1
E [w( X 0 , r ) I 0 , r ]+
∑
r ∈ T 2
E [w( X 0 , r ) I 0 , r ],
and we bound each term.

10.5 ∗ coding: shannon’s theorem
We first bound
∑
r ∈ T 1
E [w( X 0 , r ) I 0 , r ]≤
∑
r ∈ T 1
w( X 0 , r )
=
∑
r :|( X 0 , r )− pn |>ε n
p ( X^0 , r )(1− p ) n −( X^0 , r )
=Pr(|( X 0 , R )− np |>ε n ).
That is, to bound the first term, we simply bound the probability that the receiver fails
to decode correctly and the number of errors is not in the range [( p −ε) n ,( p +ε) n ]
by the probability that the number of errors is not in this range. Equivalently, we obtain
our bound by assuming that, whenever there are too many or too few errors introduced
by the channel, we fail to decode correctly. This probability is very small, as we can
see by using the Chernoff bound of Eqn. (4.6):

Pr(|( X 0 , R )− np |>ε n )≤2e−ε
(^2) n / 3 p
.
For any∑ ε>0, we can choose n sufficiently large so that this probability, and hence
r ∈ T 1 E [w( X^0 , r ) I^0 , r ], is less thanγ/2.
We now find an upper bound for

∑
r ∈ T 2 E [w( X^0 , r ) I^0 , r ]. For every r ∈ T^2 , the decod-
ing algorithm will be successful when r is received unless r differs from some other
codeword Xi in between ( p −ε) n and ( p +ε) n places. Hence I 0 , r will be 1 only if such
an Xi exists, and thus for any values of X 0 and r ∈ T 2 we have

E [w( X 0 , r ) I 0 , r ]
=w( X 0 , r ) Pr(for some Xi with 1≤ i ≤ 2 k − 1 ,|( Xi , r )− pn |≤ε n ).
It follows that if we obtain an upper bound

Pr(for some Xi with 1≤ i ≤ 2 k − 1 ,|( Xi , r )− pn |≤ε n )≤γ/ 2
for any values of X 0 and r ∈ T 2 , then

∑
r ∈ T 2
E [w( X 0 , r ) I 0 , r ]≤
∑
r ∈ T 2
w( X 0 , r )
γ
2
≤
γ
2
.
To obtain this upper bound, we recall that the other codewords X 1 , X 2 ,..., X 2 k − 1 are
chosen independently and uniformly at random. The probability that any other specific
codeword Xi , i >0, differs from any given string r of length n in between ( p −ε) n and
( p +ε) n places is therefore at most

n (∑ p +ε)
j = n ( p −ε)
( n
j
)
2 n
≤ n
( n
n ( p +ε)
)
2 n
.
Here we have bounded the summation by n times its largest term;

( n
j
)
is largest when
j = n ( p +ε)over the range of j in the summation, as long asεis chosen so that
p +ε< 1 /2.

entropy, randomness, and information
Using Corollary10.3,
(
n
n ( p +ε)
)
2 n
≤
2 H ( p +ε) n
2 n
= 2 − n (1− H ( p +ε)).
Hence the probability that any specific Xi matches a string r on a number of bits so as
to cause a decoding failure is at most n 2 − n (1− H ( p +ε)). By a union bound, the probability
that any of the 2 k −1 other codewords cause a decoding failure when X 0 is sent is at
most

n 2 − n (1− H ( p +ε))(2 k −1)≤ n 2 n ( H ( p +ε)− H ( p )−δ),
where we have used the fact that k ≤ n (1− H ( p )−δ). By choosingεsmall enough
so that H ( p +ε)− H ( p )−δis negative and then choosing n sufficiently large, we can
make this term as small as desired, and in particular we can make it less thanγ/2.
By summing the bounds over the two sets T 1 and T 2 , which correspond to the two
types of error in the decoding algorithm, we find that E [ W 0 ]≤γ.
We can bootstrap this result to show that there exists a specific code such that, if
the k -bit message to be sent is chosen uniformly at random, then the code fails with
probabilityγ. We use the linearity of expectations and the probabilistic method. We
have that

(^2) ∑ k − 1
j = 0
E [ Wj ]= E

⎡
⎣
(^2) ∑ k − 1
j = 0
Wj

⎤
⎦≤ 2 k γ,
where again the expectation is over the random choices of the codewords X 0 , X 1 ,...,
X 2 k − 1. By the probabilistic method, there must exist a specific set of codewords
x 0 , x 1 ,..., x 2 k − 1 such that

(^2) ∑ k − 1
j = 0
Wj ≤ 2 k γ.
When a k -bit message to be sent is chosen uniformly at random, the probability of
error is
1
2 k
(^2) ∑ k − 1
j = 0
Wj ≤γ
for this set of codewords, proving the claim.
We now prove the stronger statement in the theorem: we can choose the codewords
so that the probability of failure for each individual codeword is bounded above byγ.
Notice that this is not implied by the previous analysis, which simply shows that the
average probability of failure over the codewords is bounded above byγ.

10.5 ∗ coding: shannon’s theorem
We have shown that there exists a set of codewords x 0 , x 1 ,..., x 2 k − 1 for which
(^2) ∑ k − 1
j = 0
Wj ≤ 2 k γ.
Without loss of generality, let us assume that the xi are sorted in increasing order of
Wi. Suppose that we remove the half of the codewords that have the largest values Wi ;
that is, we remove the codewords that have the highest probability of yielding an error
when being sent. We claim that each xi , i < 2 k −^1 , must satisfy Wi ≤ 2 γ. Otherwise we
would have
(^2) ∑ k − 1
j = 2 k −^1
Wj > 2 k −^1 (2γ)= 2 k γ,
a contradiction. (We used similar reasoning in the proof of Markov’s inequality in Sec-
tion3.1.)
We can set up new encoding and decoding functions on all ( k −1)-bit strings using
just these 2 k −^1 codewords, and now the error probability for every codeword is simulta-
neously at most 2γ. Hence we have shown that, when k − 1 ≤ n (1− H ( p )−δ), there
exists a code such that the probability that the receiver fails to obtain the correct mes-
sage is at most 2γfor any message that is sent. Sinceδandγwere arbitrary constants,
we see that this implies the first half of the theorem.
Having completed the first half of the theorem, we now move to the second
half: for any constantsδ, γ >0 and for n sufficiently large, there do not exist ( k , n )
encoding and decoding functions with k ≥ n (1− H ( p )+δ) such that the probabil-
ity of decoding correctly is at leastγ for a k -bit input message chosen uniformly at
random.
Before giving the proof, let us first consider some helpful intuition. We know that
the number of errors introduced by the channel is, with high probability, between
( p −ε) n and ( p +ε) n for a suitable constantε>0. Suppose that we try to set
up the decoding function so that each codeword is decoded properly whenever the
number of errors is between ( p −ε) n and ( p +ε) n. Then each codeword is associated
with
n (∑ p +ε)
k = n ( p −ε)
( n
k

)
≥
(
n
 np 
)
≥
2 nH ( p )
n + 1
bit sequences by the decoding function; the last inequality follows from Corollary10.3.
But there are 2 k different codewords, and
2 k
2 nH ( p )
n + 1
≥ 2 n (1− H ( p )+δ)
2 nH ( p )
n + 1
> 2 n
when n is sufficiently large. Since there are only 2 n possible bit sequences that can be
received, we cannot create a decoding function that always decodes properly whenever
the number of errors is between ( p −ε) n and ( p +ε) n.
entropy, randomness, and information
We now need to extend the argument for any encoding and decoding functions.
This argument is more complex, since we cannot assume that the decoding func-
tion necessarily tries to decode properly whenever the number of errors is between
( p −ε) n and ( p +ε) n , even though this would seem to be the best strategy to
pursue.
Given any fixed encoding function with codewords x 0 , x 1 ,..., x 2 k − 1 and any fixed
decoding function, let z be the probability of successful decoding. Define Si to be the
set of all received sequences that decode to xi. Then

z =
(^2) ∑ k − 1
i = 0

∑
s ∈ Si
Pr(( xi is sent)∩( R = s ))
=
(^2) ∑ k − 1
i = 0

∑
s ∈ Si
Pr( xi is sent) Pr( R = s | xi is sent)
=
1
2 k
(^2) ∑ k − 1
i = 0

∑
s ∈ Si
Pr( R = s | xi is sent)
=
1
2 k
(^2) ∑ k − 1
i = 0

∑
s ∈ Si
w( xi , s ).
The second line follows from the definition of conditional probability. The third line
uses the fact that the message sent and hence the codeword sent is chosen uniformly
at random from all codewords. The fourth line is just the definition of the weight
function.
To bound this last line, we again split the summation

∑ 2 k − 1
i = 0
∑
s ∈ Si w( xi , s ) into two
parts. Let Si , 1 ={ s ∈ Si :|( xi , s )− pn |>ε n }and Si , 2 ={ s ∈ Si :|( xi , s )− pn |≤
ε n }, where againε>0 is some constant to be determined. Then

∑
s ∈ Si
w( xi , s )=
∑
s ∈ Si , 1
w( xi , s )+
∑
s ∈ Si , 2
w( xi , s ).
Now

∑
s ∈ Si , 1
w( xi , s )≤
∑
s :|( xi , s )− pn |>ε n
w( xi , s ),
which can be bounded using Chernoff bounds. The summation on the right is simply
the probability that the number of errors introduced by the channel is not between
( p −ε) n and ( p +ε) n , which we know from previous arguments is at most 2e−ε^2 n /^3 p.
This bound is equivalent to assuming that decoding is successful even if there are too
many or too few errors introduced by the channel; since the probability of too many or
too few errors is small, this assumption still yields a good bound.

10.5 ∗ coding: shannon’s theorem
To bound
∑
s ∈ Si , 2 w( xi , s ), we note thatw( xi , s ) is decreasing in( xi , s ). Hence, for
s ∈ Si , 2 ,

w( xi , s )≤ p ( p −ε) n (1− p )(1− p +ε) n
= ppn (1− p )(1− p ) n
(
1 − p
p
)ε n
= 2 − H ( p ) n
(
1 − p
p
)ε n
.
Therefore,

∑
s ∈ Si , 2
w( xi , s )≤
∑
s ∈ Si , 2
2 − H ( p ) n
(
1 − p
p
)ε n
= 2 − H ( p ) n
(
1 − p
p
)ε n
| Si , 2 |.
We continue with
z =
1
2 k
(^2) ∑ k − 1
i = 0

∑
s ∈ Si
w( xi , s )
=
1
2 k
(^2) ∑ k − 1
i = 0

⎛
⎝
∑
s ∈ Si , 1
w( xi , s )+
∑
s ∈ Si , 2
w( xi , s )
⎞
⎠
≤
1
2 k
(^2) ∑ k − 1
i = 0

(
2e−ε
(^2) n / 3 p

2 − H ( p ) n
(
1 − p
p
)ε n
| Si , 2 |
)
=2e−ε
(^2) n / 3 p

1
2 k
2 − H ( p ) n
(
1 − p
p
)ε n (^2) ∑ k − 1
i = 0
| Si , 2 |
≤2e−ε
(^2) n / 3 p

1
2 k
2 − H ( p ) n
(
1 − p
p
)ε n
2 n.
In this last line, we have used the important fact that the sets of bit sequences Si and
hence all the Si , 2 are disjoint, so their total size is at most 2 n. This is where the fact
that we are using a decoding function comes into play, allowing us to establish a useful
bound.
To conclude,

z ≤2e−ε
(^2) n / 3 p

2 n −(1− H ( p )+δ) n − H ( p ) n
(
1 − p
p
)ε n
=2e−ε
(^2) n / 3 p

((
1 − p
p
)ε
2 −δ
) n
.
entropy, randomness, and information
As long as we chooseεsufficiently small that
(
1 − p
p

)ε
2 −δ< 1 ,
then, when n is sufficiently large, z <γ, which proves Theorem10.8.  /theta

Shannon’s theorem demonstrates the existence of codes that transmit arbitrarily closely
to the capacity of the binary symmetric channel over long enough blocks. It does not
give explicit codes, nor does it say that such codes can be encoded and decoded effi-
ciently. It took decades after Shannon’s original work before practical codes with near-
optimal performance were found.

10.6 Exercises
Exercise 10.1: (a) Let S =

∑ 10
k = 11 / k
(^2). Consider a random variable X such that Pr( X =
k )= 1 / Sk^2 for integers k = 1 ,...,10. Find H ( X ).
(b) Let S =

∑ 10
k = 11 / k
(^3). Consider a random variable X such that Pr( X = k )= 1 / Sk 3
for integers k = 1 ,...,10. Find H ( X ).
(c) Consider S α=

∑ 10
k = 11 / k α, whereα>1 is a constant. Consider random vari-
ables X α such that Pr( X α= k )= 1 / S α k αfor integers k = 1 ,...,10. Give an intu-
itive explanation explaining whether H ( X α) is increasing or decreasing withαand
why.

Exercise 10.2: Consider an n -sided die, where the i th face comes up with probability
pi. Show that the entropy of a die roll is maximized when each face comes up with
equal probability 1/ n.

Exercise 10.3: (a) A fair coin is repeatedly flipped until the first heads occurs. Let X
be the number of flips required. Find H ( X ).
(b) Your friend flips a fair coin repeatedly until the first heads occurs. You want
to determine how many flips were required. You are allowed to ask a series of yes–
no questions of the following form: you give your friend a set of integers, and your
friend answers “yes” if the number of flips is in that set and “no” otherwise. Describe
a strategy so that the expected number of questions you must ask before determining
the number of flips is H ( X ).
(c) Give an intuitive explanation of why you cannot come up with a strategy that
would allow you to ask fewer than H ( X ) questions on average.

Exercise 10.4: (a) Show that

S =
∑∞
k = 2
1
k ln^2 k
is finite.

10.6exercises
(b) Consider the integer-valued discrete random variable X given by
Pr( X = k )=
1
Sk ln^2 k
, k ≥ 2.
Show that H ( X ) is unbounded.

Exercise 10.5: Suppose p is chosen uniformly at random from the real interval [0,1].
Calculate E [ H ( p )].

Exercise 10.6: The conditional entropy H ( Y | X ) is defined by

H ( Y | X )=
∑
x , y
Pr(( X = x )∩( Y = y )) log 2 Pr( Y = y | X = x ).
If Z =( X , Y ), show that

H ( Z )= H ( X )+ H ( Y | X ).
Exercise 10.7: One form of Stirling’s formula is

√
2 π n
(
n
e
) n
< n !<
√
2 π n
(
n
e
) n
e^1 /(12 n ).
Using this, prove
(
n
qn

)
≥
2 nH ( q )
2
√
n
,
which is a tighter bound than that of Lemma10.2.

Exercise 10.8: We have shown in Theorem10.5that we can use a recursive proce-
dure to extract, on average, at least log 2 m −1 independent, unbiased bits from a
number X chosen uniformly at random from S ={ 0 ,..., m − 1 }. Consider the follow-
ing extraction function: letα= log 2 m , and write

m =βα 2 α+βα− 12 α−^1 +···+β 020 ,
where eachβ i is either 0 or 1.
Let k be the number of values of i for whichβ i equals 1. Then we split S into k disjoint
subsets in the following manner: there is one set for each value ofβ i that equals 1, and
the set for this i has 2 i elements. The assignment of S to sets can be arbitrary, as long
as the resulting sets are disjoint. To get an extraction function, we map the elements
of the subset with 2 i elements in a one-to-one manner with the 2 i binary strings of
length i.
Show that this mapping is equivalent to the recursive extraction procedure given in
Theorem10.5in that both produce i bits with the same probability for all i.

Exercise 10.9: We have shown that we can extract, on average, at least log 2 m − 1
independent, unbiased bits from a number chosen uniformly at random from

entropy, randomness, and information
{ 0 ,..., m − 1 }. It follows that if we have k numbers chosen independently and uni-
formly at random from { 0 ,..., m − 1 }then we can extract, on average, at least
k log 2 m − k independent, unbiased bits from them. Give a better procedure that
extracts, on average, at least k log 2 m −1 independent, unbiased bits from these num-
bers.

Exercise 10.10: Suppose that we have a means of generating independent, fair coin
flips.

(a) Give an algorithm using the coin to generate a number uniformly from
{ 0 , 1 ,..., n − 1 }, where n is a power of 2, using exactly log 2 n flips.
(b) Argue that, if n is not a power of 2, then no algorithm can generate a number
uniformly from{ 0 , 1 ,..., n − 1 }using exactly k coin flips for any fixed k.
(c) Argue that, if n is not a power of 2, then no algorithm can generate a number
uniformly from{ 0 , 1 ,..., n − 1 }using at most k coin flips for any fixed k.
(d) Give an algorithm using the coin to generate a number uniformly from
{ 0 , 1 ,..., n − 1 }, even when n is not a power of 2, using at most 2log 2 n  expected
flips.

Exercise 10.11: Suppose that we have a means of generating independent, fair coin
flips.

(a) Give an algorithm using the fair coin that simulates flipping a biased coin that
comes up heads with probability p. The expected number of flips your algorithm
uses should be at most 2. ( Hint: Think of p written as a decimal in binary, and use
the fair coin to generate binary decimal digits.)
(b) Give an algorithm using the coin to generate a number uniformly from
{ 0 , 1 ,..., n − 1 }. The expected number of flips your algorithm uses should be at
mostlog 2 n +2.

Exercise 10.12: Here is an extraction algorithmAwhose input is a sequence X =
x 1 , x 2 ,..., xn of n independent flips of a coin that comes up heads with probability
p > 1 /2. Break the sequence into n / 2 pairs, ai =( x 2 i − 1 , x 2 i )for i = 1 ,..., n / 2 .
Consider the pairs in order. If yi =(heads,tails) then output a 0; if ai =(tails,heads)
then output a 1; otherwise, move on to the next pair.

(a) Show that the bits extracted are independent and unbiased.
(b) Show that the expected number of extracted bits is n / 2  2 p (1− p )≈ np (1− p ).
(c) We can derive another set of flips Y = y 1 , y 2 ,...from the sequence X as fol-
lows. Start with j , k =1. Repeat the following operations until j = n / 2 :If
aj =(heads,heads), set yk to heads and increment j and k ;if aj =(tails,tails),
set yk to tails and increment j and k ; otherwise, increment j. See Figure10.3for an
example.
The intuition here is that we take some of the randomness thatAwas unable
to use effectively and re-use it. Show that the bits produced by runningAon Y

10.6exercises
Figure 10.3: After runningAon the input sequence X , we can derive further sequences Y and Z ;after
runningAon each of Y and Z , we can derive further sequences from them; and so on.

are independent and unbiased, and further argue that they are independent of those
produced from runningAon X.
(d) We can derive a second set of flips Z = z 1 , z 2 ,..., z n / 2 from the sequence X as
follows: let zi be heads if ai =(heads,heads) or (tails, tails), and let zi be tails
otherwise. See Figure10.3for an example. Show that the bits produced by running
Aon Z are independent and unbiased, and further argue that they are independent
of those produced from runningAon X and Y.
(e) After we derive and runAon Y and Z , we can recursively derive two further
sequences from each of these sequences in the same way, runAon those, and
so on. See Figure10.3for an example. Let A ( p ) be the average number of bits
extracted for each flip (with probability p of coming up heads) in the sequence X ,
in the limit as the length of the sequence X goes to infinity. Argue that A ( p ) satisfies
the recurrence

A ( p )= p (1− p )+
1
2
( p^2 + q^2 ) A
(
p^2
p^2 + q^2
)
+
1
2
A ( p^2 +(1− p )^2 ).
(f) Show that the entropy function H ( p ) satisfies this recurrence for A ( p ).
(g) Implement the recursive extraction procedure explained in part (e). Run it 1000
times on sequences of 1024 bits generated by a coin that comes up heads with
probability p = 0 .7. Give the distribution of the number of flips extracted over the
1000 runs and discuss how close your results are to 1024· H (0.7).

Exercise 10.13: Suppose that, instead of a biased coin, we have a biased six-sided
die with entropy h >0. Modify our extraction function for the case of biased coins
so that it extracts, on average, almost h random bits per roll from a sequence of die
rolls. Prove formally that your extraction function works by modifying Theorem10.5
appropriately.

Exercise 10.14: Suppose that, instead of a biased coin, we have a biased six-sided die
with entropy h >0. Modify our compression function for the case of biased coins so
that it compresses a sequence of n die rolls to almost nh bits on average. Prove formally
that your compression function works by modifying Theorem10.6appropriately.

entropy, randomness, and information
Exercise 10.15: We wish to compress a sequence of independent, identically dis-
tributed random variables X 1 , X 2 ,.... Each Xj takes on one of n values. We map the
i th value to a codeword, which is a sequence of n i bits. We wish these codewords to
have the property that no codeword is the prefix of any other codeword.

(a) Explain how this property can be used to easily decompress the string created by
the compression algorithm when reading the bits sequentially.
(b) Prove that the n i must satisfy

∑ n
i = 1
2 − n i ≤ 1.
This is known as the Kraft inequality.
Exercise 10.16: We wish to compress a sequence of independent, identically dis-
tributed random variables X 1 , X 2 ,.... Each Xj takes on one of n values. The i th value
occurs with probability pi , where p 1 ≥ p 2 ≥ ··· ≥ pn. The result is compressed as fol-
lows. Set

Ti =
∑ i −^1
j = 1
pj ,
and let the i th codeword be the firstlog 2 (1/ pi )bits of Ti. Start with an empty string,
and consider the Xj in order. If Xj takes on the i th value, append the i th codeword to
the end of the string.

(a) Show that no codeword is the prefix of any other codeword.
(b) Let z be the average number of bits appended for each random variable Xj .Show
that

H ( X )≤ z ≤ H ( X )+ 1.
Exercise 10.17: Arithmetic coding is a standard compression method. In the case
where the string to be compressed is a sequence of biased coin flips, it can be described
as follows. Suppose that we have a sequence of bits X =( X 1 , X 2 ,..., Xn ), where each
Xi is independently 0 with probability p and 1 with probability 1− p. The sequences
can be ordered lexicographically, so for x =( x 1 , x 2 ,..., xn ) and y =( y 1 , y 2 ,..., yn )
we say x < y if xi =0 and yi =1 in the first coordinate i such that xi = yi .If zx is
the number of zeroes in the string∑ x , then define p ( x )= pzx (1− p ) n − zx and q ( x )=

y < xp ( y ).
(a) Suppose we are given X =( X 1 , X 2 ,..., Xn ) sequentially. Explain how to compute
q ( X ) in time O ( n ). (You may assume that any operation on real numbers takes
constant time.)
(b) Argue that the intervals [ q ( x ), q ( x )+ p ( x )) are disjoint subintervals of [0,1).
(c) Given (a) and (b), the sequence X can be represented by any point in the inter-
val [ q ( X ), q ( X )+ p ( X )). Show that we can choose a codeword in [ q ( X ), q ( X )+

10.6exercises
p ( X )) withlog 2 (1/ p ( X ))+1 binary decimal digits to represent X in such a way
that no codeword is the prefix of any other codeword.
(d) Given a codeword chosen as in (c), explain how to decompress it to determine the
corresponding sequence ( X 1 , X 2 ,..., Xn ).
(e) Using a Chernoff bound, argue that log 2 (1/ p ( X )) is close to nH ( p ) with high prob-
ability. Hence this approach yields an effective compression scheme.

Exercise 10.18: Alice wants to send Bob the result of a fair coin flip over a binary
symmetric channel that flips each bit with probability p < 1 /2. To avoid errors in trans-
mission, she encodes heads as a sequence of 2 k +1 zeroes and tails as a sequence of
2 k +1 ones.

(a) Consider the case where k =1, so heads is encoded as 000 and tails as 111. For
each of the eight possible sequences of 3 bits that can be received, determine the
probability that Alice flipped a heads conditioned on Bob receiving that sequence.
(b) Bob decodes by examining the 3 bits. If two or three of the bits are 0, then Bob
decides the corresponding coin flip was a heads. Prove that this rule minimizes the
probability of error for each flip.
(c) Argue that, for general k , Bob minimized the probability of error by deciding the
flip was heads if at least k +1 of the bits are 0.
(d) Give a formula for the probability that Bob makes an error that holds for general
k. Evaluate the formula for p = 0 .1 and k ranging from 1 to 6.
(e) Give a bound on the probability computed in part (d) using Chernoff bounds.

Exercise 10.19: Consider the following channel. The sender can send a symbol from
the set{ 0 , 1 , 2 , 3 , 4 }. The channel introduces errors; when the symbol k is sent, the
recipient receives k +1 mod 5 with probability 1/2 and receives k −1 mod 5 with
probability 1/2. The errors are mutually independent when multiple symbols are sent.
Let us define encoding and decoding functions for this channel. A ( j , n ) encoding
function Enc maps a number in{ 0 , 1 ,..., j − 1 }into sequences from{ 0 , 1 , 2 , 3 , 4 } n ,
and a ( j , n ) decoding function Dec maps sequences from{ 0 , 1 , 2 , 3 , 4 } n back into
{ 0 , 1 ,..., j − 1 }. Notice that this definition is slightly different than the one we used
for bit sequences over the binary symmetric channel.
There are (1,1) encoding and decoding functions with zero probability of error. The
encoding function maps 0 to 0 and 1 to 1. When a 0 is sent, the receiver will receive
either a 1 or 4, so the decoding function maps 1 and 4 back to 0. When a 1 is sent, the
receiver will receiver either a 2 or 0, so the decoding function maps 2 and 0 back to 1.
This guarantees that no error is made. Hence at least one bit can be sent without error
per channel use.

(a) Show that there are (5,2) encoding and decoding functions with zero probability
of error. Argue that this means more than one bit of information can be sent per
use of the channel.
(b) Show that if there are ( j , n ) encoding and decoding functions with zero probability
of error, then n ≥log 2 j /(log 25 −1).

entropy, randomness, and information
Exercise 10.20: A binary erasure channel transfers a sequence of n bits. Each bit
either arrives successfully without error or fails to arrive successfully and is replaced
by a ‘?’ symbol, denoting that it is not known if that bit is a 0 or a 1. Failures occur
independently with probability p. We can define ( k , n ) encoding and decoding functions
for the binary erasure channel in a similar manner as for the binary symmetric channel,
except here the decoding function Dec:{ 0 , 1 ,?} n →{ 0 , 1 } k must handle sequences
with the ‘?’ symbol.
Prove that, for any p >0 and any constantsδ, γ >0, if n is sufficiently large then
there exist ( k , n ) encoding and decoding functions with k ≤ n (1− p −δ) such that the
probability that the receiver fails to obtain the correct message is at mostγfor every
possible k -bit input message.

Exercise 10.21: In proving Shannon’s theorem, we used the following decoding
method: Look for a codeword that differs from the received sequence of bits in between
( p −ε) n and ( p +ε) n places, for an appropriate choice ofε; if there is only one such
codeword, the decoder concludes that that codeword was the one sent. Suppose instead
that the decoder looks for the codeword that differs from the received sequence in the
smallest number of bits (breaking ties arbitrarily), and concludes that that codeword
was the one sent. Show how to modify the proof of Shannon’s theorem for this decod-
ing technique to obtain a similar result.

chapter eleven

The Monte Carlo Method

The Monte Carlo method refers to a collection of tools for estimating values through
sampling and simulation. Monte Carlo techniques are used extensively in almost all
areas of physical sciences and engineering. In this chapter, we first present the basic
idea of estimating a value through sampling, using a simple experiment that gives an
estimate of the value of the constantπ. Estimating through sampling is often more com-
plex than this simple example suggests. We demonstrate the potential difficulties that
can arise in devising an efficient sampling procedure by considering how to appropri-
ately sample in order to estimate the number of satisfying assignments of a disjunctive
normal form (DNF) Boolean formula.
We then move to more general considerations, demonstrating a general reduction
from almost uniform sampling to approximate counting of combinatorial objects. This
leads us to consider how to obtain almost uniform samples. One method is the Markov
chain Monte Carlo (MCMC) technique, introduced in the last section of this chapter.

11.1 The Monte Carlo Method
Consider the following approach for estimating the value of the constantπ. Let ( X , Y )
be a point chosen uniformly at random in a 2×2 square centered at the origin (0,0).
This is equivalent to choosing X and Y independently from a uniform distribution on
[− 1 ,1]. The circle of radius 1 centered at (0,0) lies inside this square and has areaπ.
If we let

Z =
{
1if
√
X^2 + Y^2 ≤ 1 ,
0 otherwise,
then – because the point was chosen uniformly from the 2×2 square – the probability
that Z =1 is exactly the ratio of the area of the circle to the area of the square. See
Figure11.1. Hence

Pr( Z =1)=
π
4
.
the monte carlo method
Figure 11.1: A point chosen uniformly at random in the square has probabilityπ/4 of landing in the
circle.

Assume that we run this experiment m times (with X and Y chosen independently
among the runs), with Zi being the value of Z at the i th run. If W =

∑ m
i = 1 Zi , then
E [ W ]= E
[ m
∑
i = 1
Zi
]
=
∑ m
i = 1
E [ Zi ]=
m π
4
,
and hence W ′=(4/ m ) W is a natural estimate forπ. Applying the Chernoff bound of
Eqn. (4.6), we compute

Pr(| W ′−π|≥επ)=Pr
(∣∣
∣
∣ W −
m π
4
∣∣
∣
∣≥
ε m π
4
)
=Pr(| W − E [ W ]|≥ε E [ W ])
≤2e− m πε
(^2) / 12
.
Therefore, by using a sufficiently large number of samples we can obtain, with high
probability, as tight an approximation ofπas we wish.
This method for approximatingπis an example of a more general class of approx-
imation algorithms that we now characterize.
Definition 11.1: A randomized algorithm gives an (ε, δ) - approximation for the value
V if the output X of the algorithm satisfies
Pr(| X − V |≤ε V )≥ 1 −δ.
Our method for estimatingπgives an (ε, δ)-approximation, as long asε<1 and we
choose m large enough to make
2e− m πε
(^2) / 12
≤δ.
Algebraic manipulation yields that choosing
m ≥
12 ln(2/δ)
πε^2
is sufficient.
We may generalize the idea behind our technique for estimatingπ to provide a
relation between the number of samples and the quality of the approximation. We use
the following simple application of the Chernoff bound throughout this chapter.

11.1the monte carlo method
Theorem 11.1: Let X 1 ,..., Xmbe independent and identically distributed indicator
random variables, with μ= E [ Xi ] .Ifm ≥(3 ln(2/δ))/ε^2 μ , then

Pr
(∣∣
∣
∣∣
1
m
∑ m
i = 1
Xi −μ
∣∣
∣
∣∣≥εμ
)
≤δ.
That is, m samples provide an (ε, δ) -approximation for μ.

The proof is left as Exercise11.1.
More generally, we will want an algorithm that approximates not just a single value
but instead takes as input a problem instance and approximates the solution value for
that problem. Here we are considering problems that map inputs x to values V ( x ). For
example, given an input graph, we might want to know an approximation to the number
of independent sets in the graph.
You might ask why we should settle for an approximation; perhaps we should aim
for an exact answer. In the case ofπ, we cannot obtain an exact answer becauseπis an
irrational number. Another reason for seeking an approximation is that, as we shall see
shortly, there are problems for which the existence of an algorithm that gives an exact
answer would imply that P=NP, and hence it is unlikely that such an algorithm will
be found. This, however, does not preclude the possibility of an efficient approximation
algorithm.

Definition 11.2: A fully polynomial randomized approximation scheme (FPRAS) for
a problem is a randomized algorithm for which, given an input x and any parameters ε
and δ with 0 <ε,δ< 1 , the algorithm outputs an (ε, δ) -approximation toV ( x ) in time
that is polynomial in 1 /ε , lnδ−^1 , and the size of the input x.

Exercise11.3considers a seemingly weaker but actually equivalent definition of an
FPRAS that avoids the parameterδ.
The Monte Carlo method essentially consists of the approach we have outlined here
to obtain an efficient approximation for a value V. We require an efficient process
that generates a sequence of independent and identically distributed random samples
X 1 , X 2 ,..., Xn such that E [ Xi ]= V. We then take enough samples to get an (ε, δ)-
approximation to V. Generating a good sequence of samples is often a nontrivial task
and is a major focus of the Monte Carlo method.
The Monte Carlo method is also sometimes called Monte Carlo simulation. As an
example, suppose we want to estimate the expected price of a stock sometime in the
future. We may develop a model where the price p ( Y 1 ,..., Yk ) of the stock at that time
depends on random variables Y 1 , Y 2 ,..., Yk. If we can repeatedly generate indepen-
dent random vectors ( y 1 , y 2 ,..., yk ) from the joint distribution of the Yi , then we can
repeatedly generate independent random variables X 1 , X 2 ,..., where

Xi = p ( Y 1 ,..., Yk ).
We can then use the Xi to estimate the expected future price E [ p ( Y 1 ,..., Yk )] with the
Monte Carlo method. That is, by simulating the possible future outcomes of the Yi many
times, we can estimate the desired expectation.

the monte carlo method
11.2 Application: The DNF Counting Problem
As an example of an estimation problem that requires a nontrivial sampling technique,
we consider the problem of counting the number of satisfying assignments of a Boolean
formula in disjunctive normal form (DNF). A DNF formula is a disjunction (OR) of
clauses C 1 ∨ C 2 ∨···∨ Ct , where each clause is a conjunction (AND) of literals. For
example, the following is a DNF formula:

( x 1 ∧ x 2 ∧ x 3 )∨( x 2 ∧ x 4 )∨( x 1 ∧ x 3 ∧ x 4 ).
Recall from Section6.2.2that, in a standard satisfiability problem, the input formula
is a conjunction (AND) of a set of clauses, and each clause is the disjunction (OR) of
literals. This is commonly called conjunctive normal form (CNF). While determining
the satisfiability of a formula in CNF form is difficult, determining the satisfiability of
a formula in DNF form is simple. Since a satisfying assignment for a DNF formula
needs to satisfy only one clause, it is easy to find a satisfying assignment or prove that
it is not satisfiable.
How hard is it to exactly count the number of satisfying assignments of a DNF
formula? Given any CNF formula H , we can apply de Morgan’s laws to obtain a DNF
formula for H ̄, the negation of the formula H , with the same number of variables and
clauses as the original CNF formula. The formula H has a satisfying assignment if and
only if there is some assignment for the variables that does not satisfy H ̄. Thus, H has
a satisfying assignment if and only if the number of satisfying assignments of H ̄ is
strictly less than 2 n , the total number of possible assignments for n Boolean variables.
We conclude that counting the number of satisfying assignments of a DNF formula is
at least as hard as solving the NP-complete problem SAT.
There is a complexity class associated with the problem of counting solutions to
problems in NP, denoted byP and pronounced “sharp-P”. Formally, a problem is in
the classP if there is a polynomial time, nondeterministic Turing machine such that,
for any input I , the number of accepting computations equals the number of different
solutions associated with the input I. Counting the number of satisfying assignments
of a DNF formula is actuallyP-complete; that is, this problem is as hard as any other
problem in this class. Other complete problems for the classP include counting the
number of Hamiltonian cycles in a graph and counting the number of perfect matchings
in a bipartite graph.
It is unlikely that there is a polynomial time algorithm that computes the exact num-
ber of solutions of aP-complete problem, as at the very least such an algorithm would
imply that P=NP. It is therefore interesting to find an FPRAS for the number of sat-
isfying assignments of a DNF formula.

11.2.1 The Naïve Approach
We start by trying to generalize the approach that we used to approximateπ, and we
demonstrate why it is unsuitable in general. We then show how to improve our sampling
technique in order to solve the problem.

11.2 application: the dnf counting problem
DNF Counting Algorithm I :
Input: A DNF formula F with n variables.
Output: Y =an approximation of c ( F ).
1. X ←0.
2. For k =1to m , do:
(a) Generate a random assignment for the n variables, chosen uniformly at ran-
dom from all 2 n possible assignments.
(b) If the random assignment satisfies F , then X ← X +1.
3. Return Y ←( X / m )2 n.

Algorithm 11.1: DNF counting algorithm I.
Let c ( F ) be the number of satisfying assignments of a DNF formula F .Herewe
assume that c ( F )>0, since it is easy to check whether c ( F )=0 before running our
sampling algorithm. In Section11.1we approximatedπby generating points uniformly
at random from the 2×2 square and checking to see if they were in the target: a circle
of radius 1. We try a similar approach in Algorithm11.1: we generate assignments
uniformly at random for the n variables and then see if the resulting assignment is in
the target of satisfying assignments for F.
Let Xk be 1 if the k th iteration in the algorithm generated a satisfying assignment and
0 otherwise. Then X =

∑ m
k = 1 Xk , where the Xk are independent 0–1 random variables
that each take the value 1 with probability c ( F )/ 2 n. Hence, by linearity of expectations,

E [ Y ]=
E [ X ]2 n
m
= c ( F ).
Applying Theorem11.1, we see that X / m gives an (ε, δ)-approximation of c ( F )/ 2 n ,
and hence that Y gives an (ε, δ)-approximation of c ( F ), when

m ≥
3 · 2 n ln(2/δ)
ε^2 c ( F )
.
If c ( F )≥ 2 n /α( n ) for some polynomialα, then the foregoing analysis tells us we
only need a number of samples m that is polynomial in n ,1/ε, and ln(1/δ). We cannot,
however, exclude the possibility that c ( F ) is much less than 2 n. In particular, c ( F )
might be polynomial in n. Since our analysis requires a number of samples m that is
proportional to 2 n / c ( F ), our analysis does not yield that the run time of the algorithm
is always polynomial in the problem size.
This is not simply an artifact of the analysis. We provide a rough sketch of an argu-
ment that is elaborated on in Exercise11.4. If the number of satisfying assignments is
polynomial in n and if at each step we sample uniformly at random from all 2 n pos-
sible assignments, then with high probability we must sample an exponential number
of assignments before finding the first satisfying assignment. We can conclude, for
example, that we cannot distinguish between instances with n , n^2 , and n^3 satisfying

the monte carlo method
assignments without considering exponentially many random assignments, since with
high probability we would obtain zero satisfying assignments in all three cases.
The problem with this sampling approach is that the set of satisfying assignments
might not be sufficiently dense in the set of all assignments. This is an additional
requirement of our sampling technique that was not explicit before. In the phrasing
of Theorem11.1, the valueμthat we are attempting to approximate needs to be suffi-
ciently large that sampling is efficient.
To obtain an FPRAS for this problem, we need to devise a better sampling scheme
that avoids wasting so many steps on assignments that do not satisfy the formula. We
need to construct a sample space that includes all the satisfying assignments of F and,
moreover, has the property that these assignments are sufficiently dense in the sample
space to allow for efficient sampling.

11.2.2 A Fully Polynomial Randomized Scheme for DNF Counting
We now revise our sampling procedure to obtain an FPRAS. Let F = C 1 ∨ C 2 ∨···∨
Ct , and assume without loss of generality that no clause includes a variable and its
negation. (If there is such a clause, it is not satisfiable and we can eliminate it from
the formula.) A satisfying assignment of F needs to satisfy at least one of the clauses
C 1 ,..., Ct. Each clause is a conjunction of literals, so there is only one assignment of
the variables appearing in the clause that satisfies the clause. All other variables can
have arbitrary values. For example, for the clause ( x 1 ∧ x 2 ∧ x 3 ) to be satisfied, x 1 and
x 3 must be set to True and x 2 must be set to False.
It follows that if clause Ci has n i literals then there are exactly 2 n − n i satisfying assign-
ments for Ci. Let SCi denote the set of assignments that satisfy clause i , and let

U ={( i , a )| 1 ≤ i ≤ t and a ∈ SCi }.
Notice that we know the size of U , since

∑ t
i = 1
| SCi |=| U |,
and we can compute| SCi |.
The value that we want to estimate is given by

c ( F )=
∣∣
∣
∣∣
⋃ t
i = 1
SCi
∣∣
∣
∣∣.
Here c ( F )≤| U |, since an assignment can satisfy more than one clause and thus appear
in more than one pair in U.
To estimate c ( F ), we define a subset S of U with size c ( F ). We construct this set
by selecting, for each satisfying assignment of F , exactly one pair in U that has this
assignment; specifically, we can use the pair with the smallest clause index number,
giving

S ={( i , a )| 1 ≤ i ≤ t , a ∈ SCi , a ∈/ SCj for j < i }.
11.2 application: the dnf counting problem
DNF Counting Algorithm II :
Input: A DNF formula F with n variables.
Output: Y =an approximation of c ( F ).
1. X ←0.
2. For k =1to m , do:
(a) With probability| SCi |/

∑ t
i = 1 | SCi |choose, uniformly at random, an assign-
ment a ∈ SCi.
(b) If a is not in any SCj , j < i , then X ← X +1.
3. Return Y ←( X / m )

∑ t
i = 1 | SCi |.
Algorithm 11.2: DNF counting algorithm II.
Since we know the size of U , we can estimate the size of S by estimating the ratio
| S |/| U |. We can estimate this ratio efficiently if we sample uniformly at random from
U using our previous approach, choosing pairs uniformly at random from U and count-
ing how often they are in S. We can avoid the problem we encountered when simply
sampling assignments at random, because S is relatively dense in U. Specifically, since
each assignment can satisfy at most t different clauses,| S |/| U |≥ 1 / t.
The only question left is how to sample uniformly from U. Suppose that we first
choose the first coordinate, i. Because the i th clause has| SCi |satisfying assignments, we
should choose i with probability proportional to| SCi |. Specifically, we should choose
i with probability

| SCi |
∑ t
i = 1 | SCi |
=
| SCi |
| U |
.
We then can choose a satisfying assignment uniformly at random from SCi. This is easy
to do; we choose the value True or False independently and uniformly at random for
each literal not in clause i. Then the probability that we choose the pair ( i , a )is

Pr(( i , a ) is chosen)=Pr( i is chosen)·Pr( a is chosen| i is chosen)
=
| SCi |
| U |
1
| SCi |
=
1
| U |
,
giving a uniform distribution.
These observations are implemented in Algorithm11.2.

Theorem 11.2: DNF counting algorithm II is a fully polynomial randomized approx-
imation scheme (FPRAS) for the DNF counting problem when m =(3 t /ε^2 )ln(2/δ).

Proof: Step 2(a) of the algorithm chooses an element of U uniformly at random. The
probability that this element belongs to S is at least 1/ t. Fix anyε>0 andδ>0,

the monte carlo method
and let

m =
⌈
3 t
ε^2
ln
2
δ
⌉
Then m is polynomial in t ,ε, and ln(1/δ), and the processing time of each sample is
polynomial in t. By Theorem11.1, with this number of samples, X / m gives an (ε, δ)-
approximation of c ( F )/| U |and hence Y gives an (ε, δ)-approximation of c ( F ).  /theta

11.3 From Approximate Sampling to Approximate Counting
The example of DNF formulas demonstrates that there is a fundamental connection
between being able to sample from an appropriate space and being able to count the
number of objects with some property in that space. In this section we present the
outline of a general reduction that shows that, if you can sample almost uniformly a
solution to a “self-reducible” combinatorial problem, then you can construct a random-
ized algorithm that approximately counts the number of solutions to that problem. We
demonstrate this technique for the problem of counting the number of independent sets
in a graph. In the next chapter, we also consider the problem of counting the number
of proper colorings in a graph, applying this technique there as well.
We first need to formulate the concept of approximate uniform sampling. In this set-
ting we are given a problem instance in the form of an input x , and there is an underlying
finite sample space m( x ) associated with the input.

Definition 11.3: Let w be the (random) output of a sampling algorithm for a finite
sample space  m. The sampling algorithm generates an ε - uniform sample of  m if, for
any subset S of  m ,
∣
∣∣
∣Pr(w∈ S )−

| S |
| m|
∣
∣∣
∣≤ε.
A sampling algorithm is a fully polynomial almost uniform sampler (FPAUS) for a
problem if, given an input x and a parameter ε> 0 , it generates an ε -uniform sample
of  m( x ) and runs in time that is polynomial in lnε−^1 and the size of the input x.

In the next chapter, we introduce the notion of total variation distance , which allows
for an equivalent definition of anε-uniform sample.
As an example, an FPAUS for independent sets would take as input a graph G =
( V , E ) and a parameterε. The sample space would be all independent sets in the graph.
The output would be anε-uniform sample of the independent sets, and the time to
produce such a sample would be polynomial in the size of the graph and lnε−^1. In fact,
in the reduction that follows we only need the running time to be polynomial inε−^1 ,
but we use the standard definition given in Definition11.3.
Our goal is to show that, given an FPAUS for independent sets, we can construct an
FPRAS for counting the number of independent sets. Assume that the input G has m
edges, and let e 1 ,..., em be an arbitrary ordering of the edges. Let Ei be the set of the
first i edges in E and let Gi =( V , Ei ). Note that G = Gm and that Gi − 1 is obtained from
Gi by removing a single edge.

11.3 from approximate sampling to approximate counting
We let m( Gi ) denote the set of independent sets in Gi. The number of independent
sets in G can then be expressed as

| m( G )|=
| m( Gm )|
| m( Gm − 1 )|
×
| m( Gm − 1 )|
| m( Gm − 2 )|
×
| m( Gm − 2 )|
| m( Gm − 3 )|
×···×
| m( G 1 )|
| m( G 0 )|
×| m( G 0 )|.
Since G 0 has no edges, every subset of V is an independent set and m( G 0 )= 2 n .In
order to estimate| m( G )|, we need good estimates for the ratios

ri =
| m( Gi )|
| m( Gi − 1 )|
, i = 1 ,..., m.
More formally, we will develop estimates ̃ ri for the ratios ri , and then our estimate for
the number of independent sets in G will be

2 n
∏ m
i = 1
r ̃ i ,
while the true number is

| m( G )|= 2 n
∏ m
i = 1
ri.
To evaluate the error in our estimate, we need to bound the ratio

R =
∏ m
i = 1
r ̃ i
ri
.
Specifically, to have an (ε, δ)-approximation, we want Pr(| R − 1 |≤ε)≥ 1 −δ.We
will make use of the following lemma.

Lemma 11.3: Suppose that for all i, 1 ≤ i ≤ m,r ̃ iis an (ε/ 2 m ,δ/ m ) -approximation
for ri. Then

Pr(| R − 1 |≤ε)≥ 1 −δ.
Proof: For each 1≤ i ≤ m ,wehave

Pr
(
| r ̃ i − ri |≤
ε
2 m
ri
)
≥ 1 −
δ
m
.
Equivalently,

Pr
(
| r ̃ i − ri |>
ε
2 m
ri
)
<
δ
m
.
By the union bound, the probability that| ̃ ri − ri |>(ε/ 2 m ) ri for any i is at mostδ;
therefore,| ̃ ri − ri |≤(ε/ 2 m ) ri for all i with probability at least 1−δ. Equivalently,

1 −
ε
2 m
≤
r ̃ i
ri
≤ 1 +
ε
2 m
the monte carlo method
Estimating ri :
Input: Graphs Gi − 1 =( V , Ei − 1 ) and Gi =( V , Ei ).
Output: ̃ ri =an approximation of ri.
1. X ←0.
2. Repeat for M = 1296 m^2 ε−^2 ln(2 m /δ)independent trials:
(a) Generate an (ε/ 6 m )-uniform sample from m( Gi − 1 ).
(b) If the sample is an independent set in Gi , let X ← X +1.
3. Return ̃ ri ← X / M.

Algorithm 11.3: Estimating ri.
holds for all i with probability at least 1−δ. When these bounds hold for all i , we can
combine them to obtain

1 −ε≤
(
1 −
ε
2 m
) m
≤
∏ m
i = 1
r ̃ i
ri
≤
(
1 +
ε
2 m
) m
≤ 1 +ε,
giving the lemma.  /theta

Hence all we need is a method for obtaining an (ε/ 2 m ,δ/ m )-approximation for the
ri. We estimate each of these ratios by a Monte Carlo algorithm that uses the FPAUS
for sampling independent sets. To estimate ri , we sample independent sets in Gi − 1 and
compute the fraction of these sets that are also independent sets in Gi , as described in
Algorithm11.3. The constants in the procedure were chosen to facilitate the proof of
Lemma11.4.

Lemma 11.4: When m ≥ 1 and 0 <ε≤ 1 , the procedure for estimating riyields an
(ε/ 2 m ,δ/ m ) -approximation for ri.

Proof: We first show that ri is not too small, avoiding the problem that we found in
Section11.2.1. Suppose that Gi − 1 and Gi differ in that edge ( u ,v) is present in Gi but
not in Gi − 1. An independent set in Gi is also an independent set in Gi − 1 ,so

 m( Gi )⊆ m( Gi − 1 ).
An independent set in m( Gi − 1 )\ m( Gi ) contains both u andv. To bound the size of
the set m( Gi − 1 )\ m( Gi ), we associate each I ∈ m( Gi − 1 )\ m( Gi ) with an independent
set I {v}∈ m( Gi ). In this mapping an independent set I ′∈ m( Gi ) is associated with
no more than one independent set I ′∪{v}∈ m( Gi − 1 )\ m( Gi ), and thus| m( Gi − 1 )
 m( Gi )|≤| m( Gi )|. It follows that

ri =
| m( Gi )|
| m( Gi − 1 )|
=
| m( Gi )|
| m( Gi )|+| m( Gi − 1 )\ m( Gi )|
≥
1
2
.
11.3 from approximate sampling to approximate counting
Now consider our M samples, and let Xk =1 if the k th sample is in m( Gi ) and 0
otherwise. Because our samples are generated by an (ε/ 6 m )-uniform sampler, by Def-
inition11.3each Xi must satisfy
∣∣
∣∣Pr( Xk =1)− | m( Gi )|
| m( Gi − 1 )|

∣∣
∣∣≤ ε
6 m
.
Since the Xk are indicator random variables, it follows that
∣∣
∣∣ E [ Xk ]− | m( Gi )|
| m( Gi − 1 )|

∣∣
∣∣≤ ε
6 m
and further, by linearity of expectations,
∣
∣∣
∣∣ E

[∑ M
k = 1 Xk
M
]
−
| m( Gi )|
| m( Gi − 1 )|
∣
∣∣
∣∣≤
ε
6 m
.
We therefore have

| E [ ̃ ri ]− ri |=
∣∣
∣∣
∣
E
[∑ M
i = kXk
M
]
−
| m( Gi )|
| m( Gi − 1 )|
∣∣
∣∣
∣
≤
ε
6 m
.
We now complete the lemma by combining (a) the fact just shown that E [ ̃ ri ] is close
to ri and (b) the fact that ̃ ri will be close to E [ ̃ ri ] for a sufficiently large number of
samples. Using ri ≥ 1 /2, we have

E [ ̃ ri ]≥ ri −
ε
6 m
≥
1
2
−
ε
6 m
≥
1
3
.
Applying Theorem11.1yields that, if the number of samples M satisfies

M ≥
3ln(2 m /δ)
(ε/ 12 m )^2 (1/3)
= 1296 m^2 ε−^2 ln
2 m
δ
,
then

Pr
(∣∣
∣∣ r ̃ i
E [ ̃ ri ]
− 1
∣
∣∣
∣≥
ε
12 m
)
=Pr
(
| r ̃ i − E [ ̃ ri ]|≥
ε
12 m
E [ ̃ ri ]
)
≤
δ
m
.
Equivalently, with probability 1−δ/ m ,

1 −
ε
12 m
≤
r ̃ i
E [ ̃ ri ]
≤ 1 +
ε
12 m
. (11.1)
As| E [ ̃ ri ]− ri |≤ε/ 6 m , we have that
1 −
ε
6 mri
≤
E [ ̃ ri ]
ri
≤ 1 +
ε
6 mri
.
Using that ri ≥ 1 /2 then yields

1 −
ε
3 m
≤
E [ ̃ ri ]
ri
≤ 1 +
ε
3 m
. (11.2)
the monte carlo method
Combining Eqns. (11.1) and (11.2), it follows that, with probability 1−δ/ m ,
1 −
ε
2 m
≤
(
1 −
ε
3 m
)(
1 −
ε
12 m
)
≤
r ̃ i
ri
≤
(
1 +
ε
3 m
)(
1 +
ε
12 m
)
≤ 1 +
ε
2 m
This gives the desired (ε/ 2 m ,δ/ m )-approximation.  /theta

The number of samples M is polynomial in m ,ε, and lnδ−^1 , and the time for each sam-
ple is polynomial in the size of the graph and lnε−^1. We therefore have the following
theorem.

Theorem 11.5: Given a fully polynomial almost uniform sampler (FPAUS) for inde-
pendent sets in any graph, we can construct a fully polynomial randomized approxi-
mation scheme (FPRAS) for the number of independent sets in a graph G.

In fact, this theorem is more often used in the following form.

Theorem 11.6: Given a fully polynomial almost uniform sampler (FPAUS) for inde-
pendent sets in any graph with maximum degree at most  , we can construct a fully
polynomialrandomizedapproximationscheme(FPRAS)forthenumberofindependent
sets in a graph G with maximum degree at most .

This version of the theorem follows from our previous argument, since our graphs Gi
are subgraphs of the initial graph G. Hence, if we start with a graph of maximum degree
at most, then our FPAUS need only work on graphs with maximum degree at most
. In the next chapter, we will see how to create an FPAUS for graphs with maximum
degree 4.
This technique can be applied to a broad range of combinatorial counting problems.
For example, in Chapter 12 we consider its application to finding proper colorings of
a graph G. The only requirement is that we can construct a sequence of refinements of
the problem, starting with an instance that is easy to count (the number of independent
sets in a graph with no edges, in our example) and ending with the actual counting
problem, and such that the ratio between the counts in successive instances is at most
polynomial in the size of the problem.

11.4 The Markov Chain Monte Carlo Method
The Monte Carlo method is based on sampling. It is often difficult to generate a random
sample with the required probability distribution. For example, we saw in the previous
section that we can count the number of independent sets in a graph if we can generate
an almost uniform sample from the set of independent sets. But how can we generate
an almost uniform sample?
The Markov chain Monte Carlo (MCMC) method provides a very general approach
to sampling from a desired probability distribution. The basic idea is to define an
ergodic Markov chain whose set of states is the sample space and whose stationary dis-
tribution is the required sampling distribution. Let X 0 , X 1 ,..., Xn be a run of the chain.
The Markov chain converges to the stationary distribution from any starting state X 0

11.4the markov chain monte carlo method
and so, after a sufficiently large number of steps r , the distribution of the state Xr will be
close to the stationary distribution, so it can be used as a sample. Similarly, repeating
this argument with Xr as the starting point, we can use X 2 r as a sample, and so on. We
can therefore use the sequence of states Xr , X 2 r , X 3 r ,...as almost independent samples
from the stationary distribution of the Markov chain. The efficiency of this approach
depends on (a) how large r must be to ensure a suitably good sample and (b) how much
computation is required for each step of the Markov chain. In this section, we focus on
finding efficient Markov chains with the appropriate stationary distribution and ignore
the issue of how large r needs to be. Coupling, which is one method for determining
the relationship between the value of r and the quality of the sample, is discussed in
the next chapter.
In the simplest case, the goal is to construct a Markov chain with a stationary distri-
bution that is uniform over the state space m. The first step is to design a set of moves
that ensures the state space is irreducible under the Markov chain. Let us call the set of
states reachable in one step from a state x (but excluding x ) the neighbors of x , denoted
by N ( x ). We adopt the restriction that if y ∈ N ( x ) then also x ∈ N ( y ). Generally N ( x )
will be a small set, so that performing each move is simple computationally.
We again use the setting of independent sets in a graph G =( V , E ) as an example.
The state space is all of the independent sets of G. A natural neighborhood framework
is to say that states x and y , which are independent sets, are neighbors if they differ
in just one vertex. That is, x can be obtained from y by adding or deleting just one
vertex. This neighbor relationship guarantees that the state space is irreducible, since all
independent sets can reach (respectively, can be reached from) the empty independent
set by a sequence of vertex deletions (respectively, vertex additions).
Once the neighborhoods are established, we need to establish transition probabil-
ities. One natural approach to try would be performing a random walk on the graph
of the state space. This might not lead to a uniform distribution, however. We saw in
Theorem7.13that, in the stationary distribution of a random walk, the probability of
a vertex is proportional to the degree of the vertex. Nothing in our previous discussion
requires all states to have the same number of neighbors, which is equivalent to all
vertices in the graph of the state space having the same degree.
The following lemma shows that, if we modify the random walk by giving each
vertex an appropriate self-loop probability, then we can obtain a uniform stationary
distribution.

Lemma 11.7: For a finite state space  m and neighborhood structure { N ( X )| x ∈ m} ,
let N =max x ∈ m| N ( x )|. Let M be any number such that M ≥ N. Consider a Markov
chain where

Px , y =
⎧
⎨
⎩
1 / Mifx = y and y ∈ N ( x ),
0 if x = y and y ∈/ N ( x ),
1 − N ( x )/ Mifx = y.
If this chain is irreducible and aperiodic, then the stationary distribution is the uniform
distribution.

the monte carlo method
Proof: We show that the chain is time reversible and then apply Theorem7.10.For
any x = y ,ifπ x =π y then

π xPx , y =π yPy , x ,
since Px , y = Py , x = 1 / M. It follows that the uniform distributionπ x = 1 /| m|is the sta-
tionary distribution.  /theta

Consider now the following simple Markov chain, whose states are independent sets
in a graph G =( V , E ).

1. X 0 is an arbitrary independent set in G.
2. To compute Xi + 1 :
(a) choose a vertex v uniformly at random from V ;
(b) ifv∈ Xi then Xi + 1 = Xi {v};
(c) ifv/∈ Xi and if adding v to Xi still gives an independent set, then Xi + 1 =
Xi ∪{v};
(d) otherwise, Xi + 1 = Xi.

This chain has the property that the neighbors of a state Xi are all independent sets
that differ from Xi in just one vertex. Since every state can reach and is reachable
from the empty set, the chain is irreducible. Assuming that G has at least one edge
( u ,v), then the state{v}has a self-loop ( P v,v>0), and the chain is aperiodic. Further,
when y = x , it follows that Px , y = 1 /| V |or 0. Lemma11.7therefore applies, and the
stationary distribution is the uniform distribution.

11.4.1 The Metropolis Algorithm
We have seen how to construct chains with a uniform stationary distribution. In some
cases, however, we may want to sample from a chain with a nonuniform stationary
distribution. The Metropolis algorithm refers to a general construction that transforms
any irreducible Markov chain on a state space mto a time-reversible Markov chain with
a required stationary distribution. The approach generalizes the idea we used before to
create chains with uniform stationary distributions: add self-loop probabilities to states
in order to obtain the desired stationary distribution.
Let us again assume that we have designed an irreducible state space for our Markov
chain; now we want to construct a Markov chain on this state space with a station-
ary distribution∑ π x = b ( x )/ B , where for all x ∈ mwe have b ( x )>0 and such that B =

x ∈ m b ( x ) is finite. As we see in the following lemma (which generalizes Lemma11.7),
we only need the ratios between the required probabilities; the sum B can be
unknown.

Lemma 11.8: For a finite state space  m and neighborhood structure { N ( X )| x ∈ m} ,
let N =max x ∈ m| N ( x )|. Let M be any number such that M ≥ N. For all x ∈ m , let π x >
0 be the desired probability of state x in the stationary distribution. Consider a Markov

11.4the markov chain monte carlo method
chain where

Px , y =
⎧
⎨
⎩
(1/ M )min(1,π y /π x ) if x = y and y ∈ N ( x ),
0 if x = y and y ∈/ N ( x ),
1 −
∑
y = xPx , y if x = y.
Then, if this chain is irreducible and aperiodic, the stationary distribution is given by
the probabilities π x.

Proof: As in the proof of Lemma11.7, we show that the chain is time reversible and
apply Theorem7.10. For any x = y ,ifπ x ≤π y then Px , y =1 and Py , x =π x /π y. It fol-
lows thatπ xPx , y =π yPy , x. Similarly, ifπ x >π y then Px , y =π y /π x and Py , x =1, and it
follows thatπ xPx , y =π yPy , x. By Theorem7.10, the stationary distribution is given by
the valuesπ x.  /theta

As an example of how to apply Lemma11.8, let us consider how to modify our previ-
ous Markov chains on independent sets. Let us suppose that now we want to create a
Markov chain where, in the stationary distribution, each independent set I has probabil-
ity proportional toλ| I |for some constant parameterλ>0. That is,π x =λ| Ix |/ B , where
Ix is the independent set corresponding to state x and where B =

∑
x λ| Ix |. Whenλ=1,
this is the uniform distribution; whenλ>1, larger independent sets have a larger prob-
ability than smaller independent sets; and whenλ<1, larger independent sets have a
smaller probability than smaller independent sets.
Consider now the following variation on the previous Markov chain for independent
sets in a graph G =( V , E ).

1. X 0 is an arbitrary independent set in G.
2. To compute Xi + 1 :
(a) choose a vertex v uniformly at random from V ;
(b) ifv∈ Xi , set Xi + 1 = Xi {v}with probability min(1, 1 /λ);
(c) ifv/∈ Xi and if adding v to Xi still gives an independent set, then put Xi + 1 =
Xi ∪{v}with probability min(1,λ);
(d) otherwise, set Xi + 1 = Xi.

We now follow a two-step approach. We first propose a move by choosing a vertex v
to add or delete, where each vertex is chosen with probability 1/ M ;here M =| V |. This
proposal is then accepted with probability min(1,π y /π x ), where x is the current state
and y is the proposed state to which the chain will move. Here,π y /π x isλif the chain
attempts to add a vertex and is 1/λif the chain attempts to delete a vertex. This two-
step approach is the hallmark of the Metropolis algorithm: each neighbor is selected
with probability 1/ M , and then it is accepted with probability min(1,π y /π x ). Using
this two-step approach, we naturally obtain that the transition probability Px , y is

Px , y =
1
M
min
(
1 ,
π y
π x
)
,
so Lemma11.8applies.

∑It is important that, in designing this Markov chain, we never needed to know B =
x λ

| Ix |. A graph with n vertices can have exponentially many independent sets, and
the monte carlo method
calculating this sum directly would be too expensive computationally for many graphs.
Our Markov chain gives the correct stationary distribution by using the ratiosπ y /π x ,
which are much easier to deal with.

11.5 Exercises
Exercise 11.1: Formally prove Theorem11.1.

Exercise 11.2: Another method for approximating π using Monte Carlo tech-
niques is based on Buffon’s needle experiment. Research and explain Buffon’s nee-
dle experiment, and further explain how it can be used to obtain an approximation
forπ.

Exercise 11.3: Show that the following alternative definition is equivalent to the defi-
nition of an FPRAS given in the chapter: A fullypolynomialrandomizedapproximation
scheme ( FPRAS ) for a problem is a randomized algorithm for which, given an input x
and any parameterεwith 0<ε<1, the algorithm outputs an (ε, 1 /4)-approximation
in time that is polynomial in 1/εand the size of the input x .( Hint: To boost the prob-
ability of success from 3/4to1−δ, consider the median of several independent runs
of the algorithm. Why is the median a better choice than the mean?)

Exercise 11.4: Suppose we have a class of instances of the DNF satisfiability prob-
lem, each withα( n ) satisfying truth assignments for some polynomialα. Suppose we
apply the naïve approach of sampling assignments and checking whether they sat-
isfy the formula. Show that, after sampling 2 n /^2 assignments, the probability of find-
ing even a single satisfying assignment for a given instance is exponentially small
in n.

Exercise 11.5: (a) Let S 1 , S 2 ,..., Sm be subsets of a finite universe U. We know| Si |
for 1≤ i ≤ m. We wish to obtain an (ε, δ)-approximation to the size of the set

S =
⋃ m
i = 1
Si.
We have available a procedure that can, in one step, choose an element uniformly at
random from a set Si. Also, given an element x ∈ U , we can determine the number of
sets Si for which x ∈ Si. We call this number c ( x ).
Define pi to be

pi =
| Si |
∑ m
j = 1 | Sj |
.
The j th trial consists of the following steps. We choose a set Sj , where the probability of
each set Si being chosen is pi , and then we choose an element xj uniformly at random
from Sj. In each trial the random choices are independent of all other trials. After t

11.5exercises
trials, we estimate| S |by
⎛
⎝^1
t
∑ t
j = 1
1
c ( xj )
⎞
⎠
( m
∑
i = 1
| Si |
)
.
Determine – as a function of m ,ε, andδ– the number of trials needed to obtain an
(ε, δ)-approximation to| S |.
(b) Explain how to use your results from part (a) to obtain an alternative approxi-
mation algorithm for counting the number of solutions to a DNF formula.
Exercise 11.6: The problem of counting the number of solutions to a knapsack
instance can be defined as follows: Given items with sizes a 1 , a 2 ,..., an >0 and
an integer∑ b >0, find the number of vectors ( x 1 , x 2 ,..., xn )∈{ 0 , 1 } n such that
n
i = 1 aixi ≤ b. The number b can be thought of as the size of a knapsack, and the
xi denote whether or not each item is put into the knapsack. Counting solutions cor-
responds to counting the number of different sets of items that can be placed in the
knapsack without exceeding its capacity.
(a) A naïve way of counting the number of solutions to this problem is to repeatedly
choose ( x 1 , x 2 ,..., xn )∈{ 0 , 1 } n uniformly at random, and return the 2 n times the
fraction of samples that yield valid solutions. Argue why this is not a good strategy
in general; in particular, argue that it will work poorly when each√ ai is 1 and b =
n.
(b) Consider a Markov chain X 0 , X 1 ,...on vectors ( x 1 , x 2 ,..., xn )∈{ 0 , 1 } n. Suppose
Xj is ( x 1 , x 2 ,..., xn ). At each step, the Markov chain chooses i ∈[1, n ] uniformly
at random. If xi =1, then Xj + 1 is obtained from Xj by setting xi to 0. If xi =0,
then∑ Xj + 1 is obtained from Xj by setting xi to 1 if doing so maintains the restriction
n
i = 1 aixi ≤ b. Otherwise, Xj +^1 = Xj.
∑Argue that this Markov chain has a uniform stationary distribution whenever
n
i = 1 ai > b. Be sure to argue that the chain is irreducible and aperiodic.
(c) Argue that, if we have an FPAUS for the knapsack problem, then we can derive
an FPRAS for the problem. To set the problem up properly, assume without loss
of generality that a 1 ≤ a 2 ≤ ··· ≤ an. Let b 0 =0 and bi =

∑ i
j = 1 aj. Let m( bi )be
the set of vectors ( x 1 , x 2 ,..., xn )∈{ 0 , 1 } n that satisfy
∑ n
i = 1 aixi ≤ bi. Let k be the
smallest integer such that bk ≥ b. Consider the equation
| m( b )|=
| m( b )|
| m( bk − 1 )|
×
| m( bk − 1 )|
| m( bk − 2 )|
×···×
| m( b 1 )|
| m( b 0 )|
×| m( b 0 )|.
You will need to argue that| m( bi − 1 )|/| m( bi )|is not too small. Specifically, argue
that| m( bi )|≤( n +1)| m( bi − 1 )|.
Exercise 11.7: An alternative definition for anε-uniform sample of mis as follows:
A sampling algorithm generates anε-uniform samplewif, for all x ∈ m,
|Pr(w= x )− 1 /| m||
1 /| m|
≤ε.
the monte carlo method
Show that anε-uniform sample under this definition yields anε-uniform sample as
given in Definition11.3

Exercise 11.8: Let S =

∑∞
i = 1 i
− (^2) =π (^2) /6. Design a Markov chain based on the
Metropolis algorithm on the positive integers such that, in the stationary distribution,
π i = 1 / Si^2. The neighbors of any integer i >1 for your chain should be only i −1 and
i +1, and the only neighbor of 1 should be the integer 2.
Exercise 11.9: Recall the Bubblesort algorithm of Exercise 2.22. Suppose we have n
cards labeled 1 through n. The order of the cards X can be the state of a Markov chain.
Let f ( X ) be the number of Bubblesort moves necessary to put the cards in increasing
sorted order. Design a Markov chain based on the Metropolis algorithm such that, in
the stationary distribution, the probability of an order X is proportional toλ f ( X )for a
given constantλ>0. Pairs of states of the chain are connected if they correspond to
pairs of orderings that can be obtained by interchanging at most two adjacent cards.
Exercise 11.10: A-coloring C of an undirected graph G =( V , E ) is an assignment
labeling each vertex with a number, representing a color, from the set{ 1 , 2 ,...,}.
An edge ( u ,v)is improper if both u and v are assigned the same color. Let I ( C )
be the number of improper edges of a coloring C. Design a Markov chain based on
the Metropolis algorithm such that, in the stationary distribution, the probability of a
coloring C is proportional toλ I ( C )for a given constantλ>0. Pairs of states of the
chain are connected if they correspond to pairs of colorings that differ in just one
vertex.
Exercise 11.11: In Section11.4.1we constructed a Markov chain on the indepen-
dent sets of a graph where, in the stationary distribution,π x =λ| Ix |/ B .Here Ix is the
independent set corresponding to state x and B =

∑
x λ
| Ix |. Using a similar approach,
construct a Markov chain on the independent sets of a graph excluding the empty set,
whereπ x =| Ix |/ B for a constant B. Because the chain excludes the empty set, you
should first design a neighborhood structure that ensures the state space is connected.

Exercise 11.12: The following generalization of the Metropolis algorithm is due to
Hastings. Suppose that we have a Markov chain on a state space mgiven by the transi-
tion matrix Q and that we want to construct a Markov chain on this state space with a
stationary distributionπ x = b ( x )/ B , where for all x ∈ m, b ( x )>0 and B =

∑
x ∈ m b ( x )
is finite. Define a new Markov chain as follows. When Xn = x , generate a random vari-
able Y with Pr( Y = y )= Qx , y. Notice that Y can be generated by simulating one step
of the original Markov chain. Set Xn + 1 to Y with probability

min
(
π yQy , x
π xQx , y
, 1
)
,
and otherwise set Xn + 1 to Xn. Argue that, if this chain is aperiodic and irreducible, then
it is also time reversible and has a stationary distribution given by theπ x.

11.6 An Exploratory Assignment on Minimum Spanning Trees
Exercise 11.13: Suppose we have a program that takes as input a number x on the
real interval [0,1] and outputs f ( x ) for some bounded function f taking on values in
the range [1, b ]. We want to estimate

∫ 1
x = 0
f ( x ) dx.
Assume that we have a random number generator that can generate independent uni-
form random variables X 1 , X 2 ,.... Show that

∑ m
i = 1
f ( Xi )
m
gives an (ε, δ)-approximation for the integral for a suitable value of m.

11.6. An Exploratory Assignment on Minimum Spanning Trees
Consider a complete, undirected graph with

( n
2
)
edges. Each edge has a weight, which
is a real number chosen uniformly at random on [0,1].
Your goal is to estimate how the expected weight of the minimum spanning tree
grows as a function of n for such graphs. This will require implementing a minimum
spanning tree algorithm as well as procedures that generate the appropriate random
graphs. (You should check to see what sorts of random number generators are available
on your system and determine how to seed them – say, with a value from the machine’s
clock.)
Depending on the algorithm you use and your implementation, you may find that
your program uses too much memory when n is large. To reduce memory when n is
large, we suggest the following approach. In this setting, the minimum spanning tree is
extremely unlikely to use any edge of weight greater than k ( n ) for some function k ( n ).
We can first estimate k ( n ) by using repeated runs for small values of n and then throw
away edges of weight larger than k ( n ) when n is large. If you use this approach, be sure
to explain why throwing away edges in this manner will not lead to a situation where
the program finds a spanning tree that is not actually minimal.
Run your program for n =16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, and
larger values, if your program runs fast enough. Run your program at least five times
for each value of n and take the average. (Make sure you re-seed the random number
generator appropriately!) You should present a table listing the average tree size for the
values of n that your program runs successfully. What seems to be happening to the
average size of the minimum spanning tree as n grows?
In addition, you should write one or two pages discussing your experiments in more
depth. The discussion should reflect what you have learned from this assignment and
might address the following topics.

 /thetaWhat minimum spanning tree algorithm did you use, and why?
 /thetaWhat is the running time of your algorithm?
the monte carlo method
 /thetaIf you chose to throw away edges, how did you determine k ( n ), and how effective

was this approach?
 /thetaCan you give a rough explanation for your results? (The limiting behavior as n grows

large can be proven rigorously, but it is very difficult; you need not attempt to prove
any exact result.)
 /thetaDid you have any interesting experiences with the random number generator? Do

you trust it?
chapter twelve ∗

Coupling of Markov Chains

In our study of discrete time Markov chains in Chapter 7 , we found that ergodic Markov
chains converge to a stationary distribution. However, we did not determine how quickly
they converge, which is important in a number of algorithmic applications, such as
sampling using the Markov chain Monte Carlo technique. In this chapter, we introduce
the concept of coupling, a powerful method for bounding the rate of convergence of
Markov chains. We demonstrate the coupling method in several applications, including
card-shuffling problems, random walks, and Markov chain Monte Carlo sampling of
independent sets and vertex coloring.

12.1 Variation Distance and Mixing Time
Consider the following method for shuffling n cards. At each step, a card is chosen
independently and uniformly at random and put on the top of the deck. We can think
of the shuffling process as a Markov chain, where the state is the current order of the
cards. You can check that the Markov chain is finite, irreducible, and aperiodic, and
hence it has a stationary distribution.
Let x be a state of the chain, and let N ( x ) be the set of states that can reach x in
one step. Here| N ( x )|= n ,since the top card in x could have been in n different places
in the previous step. Ifπ y is the probability associated with state y in the stationary
distribution, then for any state x we have

π x =
1
n
∑
y ∈ N ( x )
π y.
The uniform distribution satisfies these equations, and hence the unique stationary dis-
tribution is uniform over all possible permutations.
We know that the stationary distribution is the limiting distribution of the Markov
chain as the number of steps grows to infinity. If we could run the chain “forever”,
then in the limit we would obtain a state that was uniformly distributed. In practice,

coupling of markov chains
Figure 12.1: Example of variation distance. The areas shaded by upward diagonal lines correspond
to values x where D 1 ( x )< D 2 ( x ); the areas shaded by downward diagonal lines correspond to values
x where D 1 ( x )> D 2 ( x ). The total area shaded by upward diagonal lines must equal the total area
shaded by downward diagonal lines, and the variation distance equals one of these two areas.

we run the chain for a finite number of steps. If we want to use this Markov chain to
shuffle the deck, how many steps are necessary before we obtain a shuffle that is close
to uniformly distributed?
To quantify what we mean by “close to uniform”, we must introduce a distance
measure.

Definition 12.1: The variation distance between two distributions D 1 and D 2 on a
countable state space S is given by

‖ D 1 − D 2 ‖=
1
2
∑
x ∈ S
| D 1 ( x )− D 2 ( x )|.
A pictorial example of the variation distance is given in Figure12.1.
The factor 1/2 in the definition of variation distance guarantees that the variation
distance is between 0 and 1. It also allows the following useful alternative characteri-
zation.

Lemma 12.1: For any A ⊆ S, let Di ( A )=

∑
x ∈ ADi ( x ) for i =^1 ,^2. Then
‖ D 1 − D 2 ‖=max
A ⊆ S
| D 1 ( A )− D 2 ( A )|.
A careful examination of Figure12.1helps make the proof of this lemma transparent.

Proof: Let S +⊆ S be the set of states such that D 1 ( x )≥ D 2 ( x ), and let S −⊆ S be the
set of states such that D 2 ( x )> D 1 ( x ).
Clearly,
max
A ⊆ S

D 1 ( A )− D 2 ( A )= D 1 ( S +)− D 2 ( S +)
and

max
A ⊆ S
D 2 ( A )− D 1 ( A )= D 2 ( S −)− D 1 ( S −).
But since D 1 ( S )= D 2 ( S )=1, we have

D 1 ( S +)+ D 1 ( S −)= D 2 ( S +)+ D 2 ( S −)= 1 ,
12.1 variation distance and mixing time
which implies that

D 1 ( S +)− D 2 ( S +)= D 2 ( S −)− D 1 ( S −).
Hence

max
A ⊆ S
| D 1 ( A )− D 2 ( A )|=| D 1 ( S +)− D 2 ( S +)|=| D 1 ( S −)− D 2 ( S −)|.
Finally, since

| D 1 ( S +)− D 2 ( S +)|+| D 1 ( S −)− D 2 ( S −)|=
∑
x ∈ S
| D 1 ( x )− D 2 ( x )|= 2 ‖ D 1 − D 2 ‖,
we have

max
A ⊆ S
| D 1 ( A )− D 2 ( A )|=‖ D 1 − D 2 ‖,
completing the proof.  /theta

As an application of Lemma12.1, suppose that we run our shuffling Markov chain until
the variation distance between the distribution of the chain and the uniform distribution
is less thanε. This is a strong notion of close to uniform, because every permutation
of the cards must have probability at most 1/ n !+ε. In fact the bound on the variation
distance gives an even stronger statement: For any subset A ⊆ S , the probability that the
final permutation is from the set A is at mostπ( A )+ε. For example, suppose someone
is trying to make the top card in the deck an ace. If the variation distance from the
distribution to the uniform distribution is less thanε, we can safely say that probability
that an ace is the first card in the deck is at mostεgreater than if we had a perfect
shuffle.
As another example, suppose we take a 52-card deck and shuffle all the cards – but
leave the ace of spades on top. In this case, the variation distance between the resulting
distribution D 1 and the uniform distribution D 2 could be bounded by considering the
set B of states where the ace of spades is on the top of the deck:

‖ D 1 − D 2 ‖=max
A ⊆ S
| D 1 ( A )− D 2 ( A )|≥| D 1 ( B )− D 2 ( B )|= 1 −
1
52
=
51
52
.
The definition of variation distance coincides with the definition of anε-uniform
sample (given in Definition11.3). A sampling algorithm returns anε-uniform sample
on mif and only if the variation distance between its output distribution D and the
uniform distribution U satisfies

‖ D − U ‖≤ε.
Bounding the variation distance between the uniform distribution and the distribution
of the state of a Markov chain after some number of steps can therefore be a useful
way of proving the existence of efficientε-uniform samplers, which (as we showed in
Chapter 11 ) can in turn lead to efficient approximate counting algorithms.
We now consider how to bound this variation distance after t steps. In what follows,
we assume that the Markov chains under consideration are ergodic discrete space and

coupling of markov chains
discrete time chains with well-defined stationary distributions. The following defini-
tions will be useful.

Definition 12.2: Let π ̄ be the stationary distribution of an ergodic Markov chain with
state space S. Let ptxrepresent the distribution of the state of the chain starting at state
x after t steps. We define

 x ( t )=‖ ptx −π ̄‖; ( t )=max
x ∈ S
 x ( t ).
That is,  x ( t ) is the variation distance between the stationary distribution and ptx, and
( t ) is the maximum of these values over all states x.
We also define

τ x (ε)=min{ t : x ( t )≤ε}; τ(ε)=max
x ∈ S
τ x (ε).
That is, τ x (ε) is the first step t at which the variation distance between ptxand the
stationary distribution is less than ε , and τ(ε) is the maximum of these values over all
states x.

Whenτ(ε) is considered as a function ofε, it is generally called the mixing time of the
Markov chain. A chain is called rapidly mixing ifτ(ε) is polynomial in log(1/ε) and
the size of the problem. The size of the problem depends on the context; in the shuffling
example, the size would be the number of cards.

12.2 Coupling
Coupling of Markov chains is a general technique for bounding the mixing time of a
Markov chain.

Definition 12.3: A coupling of a Markov chain Mtwith state space S is a Markov
chain Zt =( Xt , Yt ) on the state space S × S such that :

Pr( Xt + 1 = x ′| Zt =( x , y ))=Pr( Mt + 1 = x ′| Mt = x );
Pr( Yt + 1 = y ′| Zt =( x , y ))=Pr( Mt + 1 = y ′| Mt = y ).
That is, a coupling consists of two copies of the Markov chain M running simultane-
ously. These two copies are not literal copies; the two chains are not necessarily in the
same state, nor do they necessarily make the same move. Instead, we mean that each
copy behaves exactly like the original Markov chain in terms of its transition probabil-
ities. One obvious way to obtain a coupling is simply to take two independent runs of
the Markov chain. As we shall see, such a coupling is generally not very useful for our
purposes.
Instead, we are interested in couplings that (a) bring the two copies of the chain to
the same state and then (b) keep them in the same state by having the two chains make
identical moves once they are in the same state. When the two copies of the chain reach
the same state, they are said to have coupled. The following lemma motivates why we
seek couplings that couple.

12.2coupling
Lemma 12.2 [ Coupling Lemma ] : Let Zt =( Xt , Yt ) be a coupling for a Markov chain
M on a state space S. Suppose that there exists a T such that, for every x , y ∈ S,
Pr( XT = YT | X 0 = x , Y 0 = y )≤ε.
Then
τ(ε)≤ T.
That is, for any initial state, the variation distance between the distribution of the state
of the chain after T steps and the stationary distribution is at most ε.
Proof: Consider the coupling when Y 0 is chosen according to the stationary distribution
and X 0 takes on any arbitrary value. For the given T andεand for any A ⊆ S ,
Pr( XT ∈ A )≥Pr(( XT = YT )∩( YT ∈ A ))
= 1 −Pr(( XT = YT )∪( YT ∈/ A ))
≥(1−Pr( YT ∈/ A ))−Pr( XT = YT )
≥Pr( YT ∈ A )−ε
=π( A )−ε.
Here the third line follows from the union bound. For the fourth line, we used the fact
that Pr( XT = YT )≤εfor any initial states X 0 and Y 0 ; in particular, this holds when
Y 0 is chosen according to the stationary distribution. For the last line, we used that
Pr( YT ∈ A )=π( A ), since YT is also distributed according to the stationary distribu-
tion. The same argument for the set S − A shows that Pr( XT ∈/ A )≥π( S − A )−ε,or
Pr( XT ∈ A )≤π( A )+ε.
It follows that
max
x , A

| pTx ( A )−π( A )|≤ε,
so by Lemma12.1the variation distance from the stationary distribution after the chain
runs for T steps is bounded above byε.  /theta
12.2.1 Example: Shuffling Cards
To apply the coupling lemma effectively to the card-shuffling Markov chain, we must
choose an appropriate coupling. Given two copies Xt and Yt of the chain in different
states, one possibility for the coupling is to choose a position j uniformly at random
from 1 to n and simultaneously move to the top the j th card from the top in both chains.
This is a valid coupling, because each chain individually acts as the original shuffling
Markov chain. Although this coupling is natural, it does not appear immediately useful.
Since the chains start in different states, the j th cards from the top in the two chains will
usually be different. Moving these two different cards to the top does not seem to bring
the two copies of the chain toward the same state.
A more useful coupling is to choose a position j uniformly at random from 1 to n
and then obtain Xt + 1 from Xt by moving the j th card to the top. Denote the value of this
card by C. To obtain Yt + 1 from Yt , move the card with value C to the top. The coupling
coupling of markov chains
is again valid, because in both chains the probability a specific card is moved to the top
at each step is 1/ n. With this coupling, it is easy to see by induction that, once a card C
is moved to the top, it is always in the same position in both copies of the chain. Hence,
the two copies are sure to become coupled once every card has been moved to the top
at least once.
Now our coupling problem for the shuffling Markov chain looks like a coupon col-
lector’s problem; to bound the number of steps until the chains couple, we simply bound
how many times cards must be chosen uniformly at random before every card is cho-
sen at least once. We know that when the Markov chain runs for n ln n + cn steps,
the probability that a specific card has not been moved to the top at least once is at
most

(
1 −
1
n
) n ln n + cn
≤e−(ln n + c )=
e− c
n
,
and thus (by the union bound) the probability that any card has not been moved to
the top at least once is at most e− c. Hence, after only n ln n + n ln(1/ε)= n ln( n /ε)
steps, the probability that the chains have not coupled is at mostε. The coupling
lemma allows us to conclude that the variation distance between the uniform distribu-
tion and the distribution of the state of the chain after n ln( n /ε) steps is bounded above
byε.

12.2.2 Example: Random Walks on the Hypercube
Recall from Section4.6.1that an n -dimensional hypercube, or n -cube, consists of N =
2 n nodes numbered from 0 to N −1. Let ̄ x =( x 1 ,..., xn ) be the binary representation
of x. Nodes x and y are connected by an edge if and only if ̄ x and ̄ y differ in exactly one
bit.
We consider the following Markov chain defined on the n -cube. At each step, choose
a coordinate i uniformly at random from [1, n ]. The new state x ′is obtained from the
current state x by keeping all coordinates of ̄ x the same, except possibly for xi. The
coordinate xi is set to 0 with probability 1/2 and to 1 with probability 1/2. This Markov
chain is exactly the random walk on the hypercube, except that with probability 1/ 2
the chain stays at the same vertex instead of moving to a new one, which removes the
potential problem of periodicity. It follows easily that the stationary distribution of the
chain is uniform over the vertices of the hypercube.
We bound the mixing timeτ(ε) of this Markov chain by using the obvious coupling
between two copies Xt and Yt of the Markov chain: at each step, we have both chains
make the same move. With this coupling, the two copies of the chain will surely agree
on the i th coordinate, once the i th coordinate has been chosen for a move of the Markov
chain. Hence the chains will have coupled after all n coordinates have each been chosen
at least once.
The mixing time can therefore be bounded by bounding the number of steps until
each coordinate has been chosen at least once by the Markov chain. This again reduces
to the coupon collector’s problem, just as in the case of the shuffling chain. By the same

12.2coupling
argument, the probability is less thanεthat after n ln( n ε−^1 ) steps the chains have not
coupled, and hence by the coupling lemma the mixing time satisfies

τ(ε)≤ n ln( n ε−^1 ).
12.2.3 Example: Independent Sets of Fixed Size
We consider a Markov chain whose states are all independent sets of size exactly k in
a graph G =( V , E ). Because we restrict ourselves to independent sets of a fixed size,
we need a different Markov chain than the chain for all independent sets developed in
Section11.4. A move is made from the independent set Xt by choosing a vertexv∈ Xt
uniformly at random and a vertexw∈ V uniformly at random. The move m (v, w, Xt )
can be described as follows: ifw/∈ Xt and ( Xt −{v})∪{w}is an independent set, then
Xt + 1 =( Xt −{v})∪{w}; otherwise, Xt + 1 = Xt. Let n be the number of vertices in the
graph and letbe the maximum degree of any vertex. We show here that this chain
is rapidly mixing whenever k ≤ n /(3+3). We leave the task of showing that the
Markov chain is ergodic and has a uniform stationary distribution as Exercise12.11,
and assume this in the following argument.
We consider a coupling on Zt =( Xt , Yt ). Our coupling will require an arbitrary per-
fect matching M between the vertices of Xt − Yt and Yt − Xt at each step; for example,
we may label the vertices 1 to n and match the elements of Xt − Yt in sorted order via a
one-to-one mapping with the elements of Yt − Xt in sorted order. For our coupling, we
first choose a transition for the chain Xt by choosingv∈ Xt andw∈ V uniformly at ran-
dom and then perform the move m (v, w, Xt ). Clearly, the copy of the chain Xt follows
the original Markov chain faithfully as required by Definition12.3. For the transition
of Yt ,ifv∈ Yt then we use the same pair of vertices v and w and perform the move
m (v, w, Yt ); ifv/∈ Yt , we perform the move m ( M (v),w, Yt ) (where M (v) denotes the
vertex matched tov). The copy of the chain Yt also follows the original Markov chain
faithfully, since each pair of vertices withv∈ Yt andw∈ V is chosen with probability
1 / kn.
An alternative way of establishing the coupling is as follows. We again choosev∈ Xt
andw∈ V uniformly at random and then perform the move m (v, w, Xt ) in the chain
Xt .Ifv∈ Yt , we perform the move m (v, w, Yt ) in the chain Yt ; otherwise, we choose
uniformly at random a vertexv′∈ Yt − Xt and perform the move m (v′,w, Yt ) in the
chain Yt. We see in Exercise12.10that this also satisfies Definition12.3.
Let dt =| Xt − Yt |measure the difference between the two independent sets after t
steps. Clearly dt can change by at most 1 at each step. We show that dt is more likely
to decrease than increase, and we use this fact to establish an upper bound on the prob-
ability that dt >0 for sufficiently large t.
Suppose that dt >0. In order for dt + 1 = dt +1, it must be that at time t the vertex
v is chosen from Xt ∩ Yt , and w is chosen so that there is a transition in exactly one
of the chains. Thus, w must be either a vertex or a neighbor of a vertex in the set
( Xt − Yt )∪( Yt − Xt ). It follows that

Pr( dt + 1 = dt + 1 | dt >0)≤
k − dt
k
2 dt (+1)
n
.
coupling of markov chains
Similarly, in order for dt + 1 = dt −1, it is sufficient that at time t we havev/∈ Yt and
w neither a vertex nor a neighbor of a vertex in the set Xt ∪ Yt −{v, v′}. Note that
| Xt ∪ Yt |= k + dt. Therefore,

Pr( dt + 1 = dt − 1 | dt >0)≥
dt
k
n −( k + dt −2)(+1)
n
We thus have, for dt >0,

E [ dt + 1 | dt ]= dt +Pr( dt + 1 = dt +1)−Pr( dt + 1 = dt −1)
≤ dt +
k − dt
k
2 dt (+1)
n
−
dt
k
n −( k + dt −2)(+1)
n
= dt
(
1 −
n −(3 k − dt −2)(+1)
kn
)
≤ dt
(
1 −
n −(3 k −3)(+1)
kn
)
Once dt =0, the two chains follow the same path and so E [ dt + 1 | dt =0]=0.
Using the conditional expectation equality, we have

E [ dt + 1 ]= E [ E [ dt + 1 | dt ]]≤ E [ dt ]
(
1 −
n −(3 k −3)(+1)
kn
)
By induction, we find that

E [ dt ]≤ d 0
(
1 −
n −(3 k −3)(+1)
kn
) t
Since d 0 ≤ k and since dt is a nonnegative integer, it follows that

Pr( dt ≥1)≤ E [ dt ]≤ k
(
1 −
n −(3 k −3)(+1)
kn
) t
≤ k e− t ( n −(3 k −3)(+1))/( kn ).
A consequence of this result is that the variation distance converges to zero whenever
k ≤ n /(3+3), and in this case

τ(ε)≤
kn ln( k ε−^1 )
n −(3 k −3)(+1)
We thus find thatτ(ε) is polynomial in n and ln(1/ε), implying that the chain is rapidly
mixing, whenever k ≤ n /(3+3).
We can actually improve upon this result. In Exercise12.12we use a slightly more
sophisticated coupling to obtain a bound that holds for any k ≤ n /(2+2).

12.3 Application: Variation Distance Is Nonincreasing
We know that an ergodic Markov chain eventually converges to its stationary distribu-
tion. In fact, the variation distance between the state of a Markov chain and its stationary
distribution is nonincreasing in time. To show this, we start with an interesting lemma
that gives another useful property of the variation distance.

12.3application: variation distance is nonincreasing
Lemma 12.3: Given distributions σ Xand σ Yon a state space S, let Z =( X , Y ) be a
random variable on S × S, where X is distributed according to σ Xand Y is distributed
according to σ Y. Then

Pr( X = Y )≥‖σ X −σ Y ‖. (12.1)
Moreover, there exists a joint distribution Z =( X , Y ) , where X is distributed according
to σ Xand Y is distributed according to σ Y, for which equality holds.

Again, examining a specific example (such as in Figure12.1) helps us understand the
following proof.

Proof: For each s ∈ S ,wehave

Pr( X = Y = x )≤min(Pr( X = x ),Pr( Y = x )).
Hence

Pr( X = Y )≤
∑
x ∈ S
min(Pr( X = x ),Pr( Y = x )),
and therefore

Pr( X = Y )≥ 1 −
∑
x ∈ S
min(Pr( X = x ),Pr( Y = x ))
=
∑
x ∈ S
(
Pr( X = x )−min(Pr( X = x ),Pr( Y = x ))
)
.
Hence we are done if we can show

‖σ X −σ Y ‖=
∑
x ∈ S
(
Pr( X = x )−min(Pr( X = x ),Pr( Y = x ))
)
. (12.2)
But Pr( X = x )−min(Pr( X = x ),Pr( Y = x ))=0 when σ X ( x )<σ Y ( x ), and when
σ X ( x )≥σ Y ( x )itis

Pr( X = x )−Pr( Y = x )=σ X ( x )−σ Y ( x ).
If we let S +be the set of all states for whichσ X ( x )≥σ Y ( x ), then the right-hand side of
Eqn. (12.2) is equal toσ X ( S +)−σ Y ( S +), which equals‖σ X −σ Y ‖from the argument
in Lemma12.1. This gives the first part of the lemma.
Equality holds in Eqn. (12.1) if we take a joint distribution where X = Y as much as
possible. Specifically, let m ( x )=min(Pr( X = x ),Pr( Y = x )). If

∑
xm ( x )=1, then X
and Y have the same distribution and we are done. Otherwise, let Z =( X , Y ) be defined
by

Pr( X = x , Y = y )=
⎧
⎪⎨
⎪⎩
m ( x )if x = y ;
(σ X ( x )− m ( x ))(σ Y ( y )− m ( y ))
1 −
∑
zm ( z )
otherwise.
The idea behind this choice of Z is to first match X and Y as much as possible and then
force X and Y to behave independently if they do not match.

coupling of markov chains
For this choice of Z ,
Pr( X = Y )=
∑
x
m ( x )= 1 −‖σ X −σ Y ‖.
It remains to show that, for this choice of Z ,Pr( X = x )=σ X ( x ); the same argument
will hold for Pr( Y = y ). If m ( x )=σ X ( x ) then Pr( X = x , Y = x )= m ( x ) and Pr( X =
x , Y = y )=0 when x = y ,soPr( X = x )=σ X ( x ). If m ( x )=σ Y ( x ), then

Pr( X = x )=
∑
y
Pr( X = x , Y = y )
= m ( x )+
∑
y = x
(σ X ( x )− m ( x ))(σ Y ( y )− m ( y ))
1 −
∑
zm ( z )
= m ( x )+
(σ X ( x )− m ( x ))
∑
y = x (σ Y ( y )− m ( y ))
1 −
∑
zm ( z )
= m ( x )+
(σ X ( x )− m ( x ))
(
1 −σ Y ( x )−
(∑
zm ( z )− m ( x )
))
1 −
∑
zm ( z )
= m ( x )+(σ X ( x )− m ( x ))
=σ X ( x ),
completing the proof.  /theta

Recall that( t )=max x  x ( t ), where x ( t ) is the variation distance between the sta-
tionary distribution and the distribution of the state of the Markov chain after t steps
when starting at state x. Using Lemma12.3, we can prove that( t ) is nonincreasing
over time.

Theorem 12.4: For any ergodic Markov chain Mt, ( T +1)≤( T ).

Proof: Let x be any given state, and let y be a state chosen from the stationary distri-
bution. Then

 x ( T )=‖ pTx − pTy ‖.
Indeed, if XT is distributed according to pTx and if YT is distributed according to pTy ,
then by Lemma12.3there exists a random variable ZT =( XT , YT ) with Pr( XT = YT )=
 x ( T ). From this state ZT , consider any one-step coupling for the Markov chain that
takes ZT =( XT , YT )to ZT + 1 =( XT + 1 , YT + 1 ) in such a way that, whenever XT = YT ,
the coupling makes the same move, so that XT + 1 = YT + 1 .Now XT + 1 is distributed
according to pTx +^1 and YT + 1 is distributed according to pTy +^1 , which is the stationary
distribution. Hence, by Lemma12.3,

 x ( T )=Pr( XT = YT )
≥Pr( XT + 1 = YT + 1 )
≥‖ pTx +^1 − pTy +^1 ‖
= x ( T +1).
12.4 Geometric Convergence
The second line follows from the first because the one-step coupling assures XT + 1 =
YT + 1 whenever XT = YT. The result follows since the foregoing relations hold for every
state x.  /theta

12.4. Geometric Convergence
The following general result, derived from a trivial coupling, is useful for bounding the
mixing time of some Markov chains.
Theorem 12.5: Let P be the transition matrix for a finite, irreducible, aperiodic
Markov chain. Let mjbe the smallest entry in the j th column of the matrix, and let
m =
∑
jmj. Then, for all x and t,
‖ ptx −π‖≤(1− m ) t.
Proof: If the minimum entry in column j is mj , then in one step the chain reaches state
j with probability at least mj from every state. Hence we can design a coupling where
the two copies of the chain both move to state j together with probability at least mj in
every step. Since this holds for all j , at each step the two chains can be made to couple
with probability at least m. Hence the probability they have not coupled after m steps
is at most (1− m ) t , yielding the theorem via the coupling lemma.  /theta
Theorem12.5is not immediately helpful if there is a zero entry in each column, in
which case m =0. In Exercise12.6, we consider how to make it useful for any finite,
irreducible, aperiodic Markov chain. Theorem12.5shows that, under very general con-
ditions, Markov chains converge quickly to their stationary distributions, with the vari-
ation distance converging geometrically in the number of steps.
A more general related result is the following. Suppose that we can obtain an upper
bound onτ( c ) for some constant c < 1 /2. For example, such a bound might be found
by a coupling. This is sufficient to bootstrap a bound forτ(ε) for anyε>0.
Theorem 12.6: Let P be the transition matrix for a finite, irreducible, aperiodic
Markov chain Mtwith τ( c )≤ T for some c < 1 / 2. Then, for this Markov chain,
τ(ε)≤lnε/ln(2 c ) T.
Proof: Consider any two initial states X 0 = x and Y 0 = y. By the definition ofτ( c ),
we have‖ pTx −π‖≤ c and‖ pTy −π‖≤ c. It follows that‖ pTx − pTy ‖≤ 2 c and hence,
by Lemma12.3, there exists a random variable ZT , x , y =( XT , YT ) with XT distributed
according to pTx and YT distributed according to pTy such that Pr( XT = YT )≤ 2 c.
Now consider the Markov chain M ′ t given by the transition matrix P T , which corre-
sponds to a chain that takes T steps of Mt for each of its steps; the ZT , x , y give a coupling
for this new chain. That is, given two copies of the chain M ′ t in the paired state ( x , y ),
we can let the next paired state be given by the distribution ZT , x , y , which guarantees that
the probability the two states have not coupled in one step is at most 2 c. The probability
that this coupling of the chain Mt ′has not coupled over k steps is then at most (2 c ) k by
induction. By the coupling lemma, Mt ′is within variation distanceεof its stationary
coupling of markov chains
distribution after k steps if

(2 c ) k ≤ε.
It follows that, after at mostlnε/ln(2 c )steps, M ′ t is within variation distanceεof its
stationary distribution. But Mt ′and Mt have the same stationary distribution, and each
step of M ′ t corresponds to T steps of Mt. Therefore,

τ(ε)≤
⌈
lnε
ln(2 c )
⌉
T
for the Markov chain Mt.  /theta

12.5. Application: Approximately Sampling Proper Colorings
A vertex coloring of a graph gives each vertex v a color from a set C , which we can
assume without loss of generality is the set{ 1 , 2 ,..., c }.Ina proper coloring, the two
endpoints of every edge are colored by two different colors. Any graph with maximum
degreecan be colored properly with+1 colors by the following procedure: choose
an arbitrary ordering of the vertices, and color them one at a time, labeling each vertex
with a color not already used by any of its neighbors.
Here we are interested in sampling almost uniformly at random a proper coloring
of a graph. We present a Markov chain Monte Carlo (MCMC) process that generates
such a sample and then use a coupling technique to show that it is rapidly mixing.
In the terminology of Chapter 11 , this gives an FPAUS for proper colorings. Apply-
ing the general reduction from approximate counting to almost uniform sampling, as
in Theorem11.5, we can use the FPAUS for sampling proper colorings to obtain an
FPRAS for the number of proper colorings. The details of this reduction are left as part
of Exercise12.15.
To begin, we present a straightforward coupling that allows us to approximately
sample colorings efficiently when there are c > 4 +1 colors. We then show how to
improve the coupling to reduce the number of colors necessary to 2+1.
Our Markov chain on proper colorings is the simplest one possible. At each step,
choose a vertex v uniformly at random and a color nuniformly at random. Recolor
vertex v with color nif the new coloring is proper (that is, v does not have a neighbor
colored n), and otherwise let the state of the chain be unchanged. This finite Markov
chain is aperiodic because it has nonzero probability of staying in the same state. When
c ≥+2, it is also irreducible. To see how from any state X we can reach any other
state Y , consider an arbitrary ordering of the vertices. Recolor the vertices in X to match
Y in this order. If there is a conflict at any step, it must arise because a vertex v that needs
to be colored is blocked by some other vertexv′later in the ordering. Butv′can be
recolored to some other nonconflicting color, since c ≥+2, allowing the process to
continue. Hence, when c ≥+2, the Markov chain has a stationary distribution. The
fact that this stationary distribution is uniform over all proper colorings can be verified
by applying Lemma11.7.

12.5application: approximately sampling proper colorings
When there are 4+1 colors, we use a trivial coupling on the pair of chains ( Xt , Yt ):
choose the same vertex and color on both chains at each step.

Theorem 12.7: For any graph with n vertices and maximum degree  , the mixing
time of the graph-coloring Markov chain satisfies

τ(ε)≤
⌈
nc
c − 4 
ln
(
n
ε
)⌉
,
provided that c ≥ 4 + 1_._

Proof: Let Dt be the set of vertices that have different colors in the two chains at time
t , and let dt =| Dt |. At each step in which dt >0, either dt remains at the same value
or dt increases or decreases by at most 1. We show that dt is actually more likely to
decrease than increase; then we use this fact to bound the probability that dt is nonzero
for sufficiently large t.
Consider any vertex v that is colored differently in the two chains. Since the degree
of v is at most, there are at least c − 2 colors that do not appear on the neighbors
of v in either of the two chains. If the vertex is recolored to one of these c − 2 colors,
it will have the same color in both chains. Hence

Pr( dt + 1 = dt − 1 | dt >0)≥
dt
n
c − 2 
c
.
Now consider any vertex v that is colored the same in both chains. For v to be colored
differently at the next step, it must have some neighbor w that is differently colored in
the two chains; in that case, it is possible that trying to recolor v using a color that the
neighbor w has in one of the two chains will recolor the vertex v in one chain but not the
other. Every vertex colored differently in the two chains can affect at mostneighbors
in this way. Hence, when dt >0,

Pr( dt + 1 = dt + 1 | dt >0)≤
 dt
n
2
c
.
We find that
E [ dt + 1 | dt ]= dt +Pr( dt + 1 = dt +1)−Pr( dt + 1 = dt −1)
≤ dt +
 dt
n
2
c
−
dt
n
c − 2 
c
= dt
(
1 −
c − 4 
nc
)
,
which also holds if dt =0.
Using the conditional expectation equality, we have

E [ dt + 1 ]= E [ E [ dt + 1 | dt ]]≤ E [ dt ]
(
1 −
c − 4 
nc
)
.
By induction, we find

E [ dt ]≤ d 0
(
1 −
c − 4 
nc
) t
.
coupling of markov chains
Since d 0 ≤ n and since dt is a nonnegative integer, it follows that

Pr( dt ≥1)≤ E [ dt ]≤ n
(
1 −
c − 4 
nc
) t
≤ n e− t ( c −^4 )/ nc.
Hence the variation distance is at mostεafter

t =
⌈
nc
c − 4 
ln
(
n
ε
)⌉
steps.  /theta

Assuming that each step of the Markov chain can be accomplished efficiently in time
that is polynomial in n , Theorem12.7gives an FPAUS for proper colorings.
Theorem12.7is rather wasteful. For example, when bounding the probability that
dt decreases, we used the loose bound c − 2 . The number of colors that decrease dt
could be much higher if some of the vertices around v have the same color in both
chains. By being a bit more careful and slightly more clever with the coupling, we can
improve Theorem12.7to hold for any c ≥ 2 +1.

Theorem 12.8: Given an n-vertex graph with maximum degree  , the mixing time of
the graph-coloring Markov chain satisfies

τ(ε)≤
⌈
nc
c − 2 
ln
(
n
ε
)⌉
,
provided that c ≥ 2 + 1_._

Proof: As before, let Dt be the set of vertices that have different colors in the two
chains at time t , with| Dt |= dt. Let At be the set of vertices that have the same color in
the two chains at time t. For a vertex v in At , let d ′(v) be the number of vertices adjacent
to v that are in Dt ; similarly, for a vertex w in Dt , let d ′(w) be the number of vertices
adjacent to w that are in At. Note that
∑

v∈ At
d ′(v)=
∑
w∈ Dt
d ′(w),
since the two sums both count the number of edges connecting vertices in At to vertices
in Dt. Denote this summation by m ′.
Consider the following coupling: if a vertexv∈ Dt is chosen to be recolored, we
simply choose the same color in both chains. That is, when v is in Dt , we are using
the same coupling we used before. The vertex v will have the same color whenever the
color chosen is different from any color on any of the neighbors of v in both copies of
the chain. There are c − 2 + d ′(v) such colors; notice that this is a tighter bound than
we used in the proof of Theorem12.7. Hence the probability that dt + 1 = dt −1 when
dt >0 is at least

1
n
∑
v∈ Dt
c − 2 + d ′(v)
c
=
1
cn
(( c − 2 ) dt + m ′).
Assume now that the vertex to be recolored isv∈ At. In this case we change the cou-
pling slightly. Recall that, in the previous coupling, recoloring a vertexv∈ At results in
v becoming differently colored in the two chains if the randomly chosen color appears

12.5application: approximately sampling proper colorings
Figure 12.2: ( a ), original coupling; ( b ), improved coupling. In the original coupling of part ( a ), the
gray vertex has the same color in both chains and has a neighbor with different colors in the two
chains, one black and one white. If an attempt is made to recolor the gray vertex black, then the move
will succeed in one chain but not the other, increasing dt. Similarly, if an attempt is made to recolor
the gray vertex white, then the move will succeed in one chain but not the other, giving a second move
that increases dt. In the improved coupling of part ( b ), if the gray vertex is recolored white in Xt then
the gray vertex is recolored black in Yt and vice versa, giving just one move that increases dt.

on a neighbor of v in one chain but not the other. For example: if v is colored green, and
a neighbor w is colored red in one chain and blue in the other, and no other neighbor of
v is colored red or blue in either chain, then attempting to color v either red or blue will
cause v to be recolored in one chain but not the other. Hence there are two potential
choices for v ’s color that increase dt.
In this specific case where just one vertex w neighboring v has different colors in the
two chains, we could improve the coupling as follows: when we try to recolor v blue in
the first chain, we try to recolor it red in the second chain; and when we try to recolor it
red in the first chain, we try to recolor it blue in the second chain. Now v either changes
color in both chains or stays the same in both chains. By changing the coupling, we
have collapsed two potentially bad moves that increase dt into just one bad move. See
Figure12.2for an example.
More generally, if there are d ′(v) differently colored vertices around v then we can
couple the colors so that at most d ′(v) color choices cause dt to increase, instead of up
to 2 d ′(v) choices in the original coupling. Concretely, let S 1 (v) be the set of colors on
neighbors of v in the first chain but not the second, and similarly let S 2 (v) be the set
of colors on neighbors of v in the second chain but not the first. Couple pairs of colors
c 1 ∈ S 1 (v) and c 2 ∈ S 2 (v) as much as possible, so that when c 1 is chosen in one chain
c 2 is chosen in the other. Then the total number of ways to color v that increases dt is
at most max(| S 1 (v)|,| S 2 (v)|)≤ d ′(v).
As a result, the probability that dt + 1 = dt +1 when dt >0 is at most
1
n

∑
v∈ At
d ′(v)
c
=
m ′
cn
.
coupling of markov chains
We therefore find that

E [ dt + 1 | dt ]≤ dt
(
1 −
c − 2 
nc
)
Following the same reasoning as in the proof of Theorem12.7,wehave

Pr( dt ≥1)≤ E [ dt ]≤ n
(
1 −
c − 2 
nc
) t
≤ n e− t ( c −^2 )/ nc ,
and the variation distance is at mostεafter

t =
⌈
nc
c − 2 
ln
(
n
ε
)⌉
steps.  /theta

Hence we can use the Markov chain for proper colorings to give us an FPAUS whenever
c > 2 .

12.6 Path Coupling
In Section11.3we showed that, if we can obtain an FPAUS for independent sets for
graphs of degree at most, then we can approximately count the number of inde-
pendent sets in such graphs. Here we present a Markov chain on independent sets,
together with a coupling argument, to prove that the chain gives such an FPAUS when
≤4. The coupling argument uses a further technique, path coupling. We demon-
strate this technique specifically for the Markov chain sampling independent sets in a
graph, although with appropriate definitions the approach can be generalized to other
problems.
Interestingly, it is very difficult to prove that the simple Markov chain for sampling
independent sets given in Section11.4, which removes or attempts to add a random
vertex to the current independent set at each step, mixes quickly. Instead, we consider
here a different Markov chain that simplifies the analysis. We assume without loss of
generality that the graph consists of a single connected component. At each step, the
Markov chain chooses an edge ( u ,v) in the graph uniformly at random. If Xt is the
independent set at time t , then the move proceeds as follows.

 /thetaWith probability 1/3, set Xt + 1 = Xt −{ u ,v}. (This move removes u and v , if they are
in the set.)
 /thetaWith probability 1/3, let Y =( Xt −{ u })∪{v}.If Y is an independent set, then Xt + 1 =
Y ; otherwise, Xt + 1 = Xt. (This move tries to remove u if it is in the set and then add
v.)
 /thetaWith probability 1/3, let Y =( Xt −{v})∪{ u }.If Y is an independent set, then Xt + 1 =
Y ; otherwise, Xt + 1 = Xt. (This move tries to remove v if it is in the set and then add
u .)
12.6 path coupling
It is easy to verify that the chain has a stationary distribution that is uniform on all
independent sets. We now use the path coupling argument to bound the mixing time of
the chain.
The idea of path coupling is to start with a coupling for pairs of states ( Xt , Yt ) that
differ in just one vertex. This coupling is then extended to a general coupling over all
pairs of states. When it applies, path coupling is very powerful, because it is often much
easier to analyze the situation where the two states differ in a small way (here, in just
one vertex) than to analyze all possible pairs of states.
Consider a graph G =( V , E ). We say that a vertex is bad if it is an element of Xt or
Yt but not both; otherwise, the vertex is good. Let dt =| Xt − Yt |+| Yt − Xt |, so that dt
counts the number of bad vertices. Assume that Xt and Yt differ in exactly one vertex
(i.e., dt =1). We apply a simple coupling, performing the same move in both states,
and show that under this coupling E [ dt + 1 | dt ]≤ dt when dt =1 or, equivalently, that
E [ dt + 1 − dt | dt =1]≤0.
Without loss of generality, let Xt = I and Yt = I ∪{ x }. A change in dt can occur only
when a move involves a neighbor of x. Thus, in analyzing this coupling, we can restrict
our discussion to moves in which the chosen random edge is adjacent to a neighbor of
x. Letδ z =1 if the vertex z = x goes from good to bad between step t and step t +1.
Similarly, letδ x =−1 if the vertex x goes from bad to good between step t and step
t +1. By linearity of expectations,

E [ dt + 1 − dt | dt =1]= E
[∑
w
δw| dt = 1
]
=
∑
w
E [δw| dt =1].
As we shall see, in the summation we need only consider those w that are equal to x ,a
neighbor of x , or a neighbor of a neighbor of x , since these are the only vertices that can
change from good to bad or bad to good in one step of the chain. We shall demonstrate
how to balance the moves in such a way that it becomes clear that E [ dt + 1 − dt | dt =
1]≤0 as long as≤4.
Assume that x has k neighbors, and let y be one of these neighbors. For each vertex
y that is a neighbor of x , we consider all of the moves that choose an edge adjacent to
y. The subsequent analysis makes use of the restriction≤4. There are three cases,
as shown in Figure12.3.
1. Suppose that y has two or more neighbors in the independent set I = Xt. Then no
move that involves y can increase the number of bad vertices, and hence dt + 1 cannot
be larger than dt as a result of any such move.
2. Suppose that y has no neighbors in I. Then dt can increase by 1 if the edge ( y , zi )
(where 1≤ i ≤3) is chosen and an attempt is made to add y and remove zi. These
moves are successful on Xt but not on Yt , and henceδ y =1 with probability at most
3 · 1 / 3 | E |= 1 /| E |. No other move involving y increases dt.
The possible gain fromδ y is balanced by moves that decreaseδ x. Any of the three
possible moves on the edge ( x , y ) match the vertex x , so thatδ x =−1, and no other
bad vertices are created. Henceδ x =−1 with probability at least 1/| E |. We see that

coupling of markov chains
Figure 12.3: Three cases for the independent set Markov chain. Vertices colored black are in both
independent sets of the coupling. Vertex x is colored gray, to represent that it is a member of the
independent set of one chain in the coupling but not the other.

the total effect of all of these moves on
∑
w E [δw| dt =1] is
1 ·
1
| E |
− 1 ·
1
| E |
= 0 ,
so that the moves from this case do not increase E [ dt + 1 − dt | dt =1].
3. Suppose that y has one neighbor in I. If the edge ( x , y ) is chosen, then two moves
can giveδ x =−1: the move that removes both x and y , or the move that removes y
and adds x. The third move, which tries to add y and remove x , fails in both chains
because y has a neighbor in I. Henceδ x =−1 with probability at least^23 (1/| E |).
Let z be the neighbor of y in I. Both y and z can become bad in one step if the
edge ( y , z ) is chosen and an attempt is made to add y and remove z. This move is
successful on Xt but not on Yt , causing dt to increase by 2 sinceδ y andδ z both equal 1.
No other move increases dt. Hence the probability that the number of bad vertices
is increased in this case is 1/ 3 | E |, and the increase is by 2. Again, the total effect of
all of these moves on

∑
w E [δw| dt =1] is
2 ·
1
3 | E |
− 1 ·
2
3
1
| E |
= 0 ,
so that the moves from this case do not increase E [ dt + 1 − dt | dt =1].
The case analysis shows that if we consider moves that involve a specific neighbor y ,
they balance so that every move that increases dt + 1 − dt is matched by corresponding
moves that decrease dt + 1 − dt. Summing over all vertices, we can conclude that

E [ dt + 1 − dt | dt =1]= E
[∑
w
δw| dt = 1
]
=
∑
w
E [δw| dt =1]≤ 0.
We now use an appropriate coupling to argue that E [ dt + 1 | dt ]≤ dt for any pair of
states ( Xt , Yt ). The statement is trivial if dt =0, and we have just shown it to be true
if dt =1. If dt >1, then create a chain of states Z 0 , Z 1 ,..., Zdt as follows: Z 0 = Xt ,
and each successive Zi is obtained from Zi − 1 by either removing a vertex from Xt − Yt
or adding a vertex from Yt − Xt. This can be done, for example, by first removing all

12.6 path coupling
vertices in Xt − Yt one by one and then adding vertices from Yt − Xt one by one. Our
coupling now arises as follows. When a move is made in Xt = Z 0 , the coupling for the
case when dt =1 gives a corresponding move for the state Z 1. This move in Z 1 can
similarly be coupled with a move in state Z 2 , and so on, until the move in Zdt − 1 yields
a move for Zdt = Yt. Let Zi ′be the state after the move is made from state Zi , and let

( Zi ′− 1 , Zi ′)=| Zi ′− 1 − Zi ′|+| Zi ′− Zi ′− 1 |.
Note that Z 0 ′= Xt + 1 and Zd ′ t = Yt + 1. We have shown that E [ dt + 1 − dt | dt =1]≤0, so
we can conclude that

E [( Zi ′− 1 , Zi ′)]≤ 1 ;
that is, because the two states Zi − 1 and Zi differ in just one vertex, the expected number
of vertices in which they differ after one step is at most 1. Using the triangle inequality
for sets,

| A − B |≤| A − C |+| C − B |,
we obtain

| Xt + 1 − Yt + 1 |+| Yt + 1 − Xt + 1 |≤
∑ dt
i = 1
(| Zi ′− 1 − Zi ′|+| Zi ′− Zi ′− 1 |)
or

dt + 1 =| Xt + 1 − Yt + 1 |+| Yt + 1 − Xt + 1 |≤
∑ dt
i = 1
( Zi ′− 1 , Zi ′).
Hence,

E [ dt + 1 | dt ]≤ E
[∑ dt
i = 1
( Zi ′− 1 , Zi ′)
]
=
∑ dt
i = 1
E [( Zi ′− 1 , Zi ′)]
≤ dt.
In previous examples we were able to prove a strict inequality of the form
E [ dt + 1 | dt ]≤β dt
for someβ<1, and we used this strict inequality to bound the mixing time. How-
ever, the weaker condition E [ dt + 1 | dt ]≤ dt that we have here is sufficient for rapid
mixing, as we shall see in Exercise12.7. Thus, the Markov chain gives an FPAUS for
independent sets in graphs when the maximum degree is at most 4; as we showed in
Section11.3, this can be used to obtain an FPRAS for this problem.

coupling of markov chains
12.7 Exercises
Exercise 12.1: Write a program that takes as input two positive integers n 1 and n 2 and
two real numbers p 1 , p 2 with 0≤ p 1 , p 2 ≤1. The output of your program should be
the variation distance between the binomial random variables B ( n 1 , p 1 ) and B ( n 2 , p 2 ),
rounded to the nearest thousandth. Use your program to compute the variation distance
between the following pairs of distributions: B (20, 0 .5) and B (20, 0 .49); B (20, 0 .5) and
B (21, 0 .5); and B (21, 0 .5) and B (21, 0 .49).

Exercise 12.2: Consider the Markov chain for shuffling n cards, where at each step
a card is chosen uniformly at random and moved to the top. Suppose that, instead of
running the chain for a fixed number of steps, we stop the chain at the first step where
every card has been moved to the top at least once. Show that, at this stopping time, the
state of the chain is uniformly distributed on the n! possible permutations of the cards.

Exercise 12.3: Consider the Markov chain for shuffling n cards, where at each step
a card is chosen uniformly at random and moved to the top. Show that, if the chain is
run for only (1−ε) n ln n steps for some constantε>0, then the variation distance is
1 − o (1).

Exercise 12.4: (a) Consider the Markov chain given by the transition matrix

P =
⎡
⎢⎢
⎢
⎢⎣
1 / 201 /20 0
01 / 21 /20 0
1 / 41 / 401 / 41 / 4
001 / 21 / 20
001 / 201 / 2
⎤
⎥⎥
⎥
⎥⎦.
Explain why Theorem12.5is not useful when applied directly to P. Then apply The-
orem12.5to the Markov chain with transition matrix P^2 , and explain the implications
for the convergence of the original Markov chain to its stationary distribution.
(b) Consider the Markov chain given by the transition matrix

P =
⎡
⎢⎢
⎢⎢
⎣
1 / 201 /20 0
01 / 21 /20 0
1 / 51 / 51 / 51 / 51 / 5
001 / 21 / 20
001 / 201 / 2
⎤
⎥⎥
⎥⎥
⎦
.
Apply Theorem12.5to P. Then apply Theorem12.5to the Markov chain with transi-
tion matrix P^2 , and explain the implications for the convergence of the original Markov
chain to its stationary distribution. Which application gives better bounds on the vari-
ation distance?

Exercise 12.5: Suppose I repeatedly roll a standard six-sided die and obtain a sequence
of independent random variables X 1 , X 2 ,..., where Xi is the outcome of the i th roll.

12.7exercises
Let

Yj =
∑ j
i = 1
Xi mod 10
be the sum of the first j rolls considered modulo 10. The sequence Yj forms a Markov
chain. Determine its stationary distribution, and determine a bound onτ(ε) for this
chain. ( Hint: One approach is to use the method of Exercise12.4.)

Exercise 12.6: Theorem12.5is useful only if there exists a nonzero entry in at least
one column of the transition matrix P of the Markov chain. Argue that for any finite,
aperiodic, irreducible Markov chain, there exists a time T such that every entry of P T
is nonzero. Explain how this can be used in conjunction with Theorem12.5.

Exercise 12.7: A technique we use repeatedly in the chapter is to define a distance
function dt that represent the distance between the two states of our coupling after t
steps, and then show that when dt >0 there exists aβ<1 such that

E [ dt + 1 | dt ]≤β dt.
(a) Under this condition, give an upper bound forτ(ε) in terms ofβand d ∗, where d ∗
is the maximum distance over all possible pairs of initial states for the coupling.
(b) Suppose that instead we have

E [ dt + 1 | dt ]≤ dt.
Suppose we have the additional conditions that dt + 1 is one of dt , dt −1, or dt + 1
and that Pr( dt = dt + 1 )≥γ. Give an upper bound forτ(ε) in terms ofε, d ∗, and
γ. Your answer should by polynomial in d ∗and 1/γ.( Hint: Think of dt as being
similar to a random walk on the line.)
(c) Using (a) and (b), show that the mixing time of the coloring chain of Section12.5
is polynomial in the number of vertices in the graph and ln(1/ε), even when the
number of colors is only 2.
(d) By extending the argument of part (b), show that the mixing time of the Markov
chain for independent sets given in Section12.6is polynomial in the number of
vertices in the graph and ln(1/ε).

Exercise 12.8: Consider the random walk on a non-bipartite, connected graph on n
vertices, where each vertex has the same degree d > n /2, d < n. Show that

τ(ε)≤
lnε
ln(1−(2 d − n )/ d )
.
Exercise 12.9: Consider a Markov chain on n points [0, n −1] lying in order on a
circle. At each step, the chain stays at the current point with probability 1/2 or moves
to the next point in the clockwise direction with probability 1/2. Find the stationary
distribution and show that, for anyε>0, the mixing timeτ(ε)is O ( n^2 ln(1/ε)).

coupling of markov chains
Exercise 12.10: In Section12.2.3, we suggested the following coupling Zt =( Xt , Yt ).
First choose a transition for the chain Xt , withv∈ Xt andw∈ V .Ifv∈ Yt , use the
same vertices v and w for the transition of the chain Yt ; otherwise, choose uniformly at
random a vertexv′∈ Yt − Xt and then perform the transition in the chain Yt with the
pairv′and w. Show that this is a valid coupling that satisfies Definition12.3.

Exercise 12.11: Show that the Markov chain for sampling all independent sets of size
exactly k ≤ n /(3+3) in a graph with n nodes and maximum degree, as defined
in Section12.2.3, is ergodic and has a uniform stationary distribution.

Exercise 12.12: We wish to improve the coupling technique used in Section12.2.3in
order to obtain a better bound. The improvement here is related to the technique used
to prove Theorem12.8. As with the coupling in Section12.2.3, if an attempt is made to
movev∈ Xt − Yt to a vertex w then the same attempt is made with the matched vertex
in the other chain. If, however, an attempt is made to move a vertexv∈ Xt ∩ Yt in both
chains, we no longer attempt to make the same move.

(a) Assume there exists a set S 1 of exactly dt (+1) distinct vertices that are members
of or neighbors of vertices in Xt − Yt and, likewise, a set S 2 of exactly dt (+1)
distinct vertices that are members of or neighbors of vertices in Yt − Xt ; assume
further that S 2 and S 1 are disjoint. Suppose that we match up the vertices in S 1 and
S 2 in a one-to-one fashion. Argue that the moves can be coupled so that, when one
chain attempts and fails to move v to a vertex in S 1 in one chain, it also attempts
and fails to move v to the matching vertex in S 2 in the other chain. Similarly, argue
that the moves can be coupled so that, when one chain attempts and succeeds in
moving v to a vertex in S 1 in one chain, it also attempts and succeeds in moving v
to the matching vertex in S 2 in the other chain. Show that the coupling gives

Pr( dt + 1 = dt +1)≤
k − dt
k
dt (+1)
n
.
(b) In the general case, S 1 and S 2 are not necessarily disjoint or of equal size. Show
that in this case, by pairing up failing moves as much as possible, the number of
choices for w that can increase dt is max(| S 1 |,| S 2 |)≤ dt (+1). Then argue that

Pr( dt + 1 = dt +1)≤
k − dt
k
dt (+1)
n
holds in all cases.
(c) Use this coupling to obtain a polynomial bound onτ(ε) that holds for any k ≤
n /(2+2).
Exercise 12.13: Consider a Markov chain with state space S and a stationary dis-
tribution ̄π, and recall the definitions of ptx and( t ) from Definition12.2. For any
nonnegative integer t let

 ̄( t )=max
x , y ∈ S
‖ ptx − pty ‖.
Assume also that the Markov chain has a stationary distribution.

12.7exercises
(a) Prove ̄( s + t )≤ ̄( s ) ̄( t ) for any positive integers s and t.
(b) Prove( s + t )≤( s ) ̄( t ) for any positive integers s and t.
(c) Prove

( t )≤ ̄( t )≤ 2 ( t )
for any positive integer t.
Exercise 12.14: Consider the following variation on shuffling for a deck of n cards.
At each step, two specific cards are chosen uniformly at random from the deck, and
their positions are exchanged. (It is possible both choices give the same card, in which
case no change occurs.)

(a) Argue that the following is an equivalent process: at each step, a specific card is
chosen uniformly at random from the deck, and a position from [1, n ] is chosen
uniformly at random; then the card at position i exchanges positions with the spe-
cific card chosen.
(b) Consider the coupling where the two choices of card and position are the same
for both copies of the chain. Let Xt be the number of cards whose positions are
different in the two copies of the chain. Show that Xt is nonincreasing over time.
(c) Show that

Pr( Xt + 1 ≤ Xt − 1 | Xt >0)≥
(
Xt
n
) 2
.
(d) Argue that the expected time until Xt is 0 is O ( n^2 ), regardless of the starting state
of the two chains.

Exercise 12.15: Modify the arguments of Lemma11.3and Lemma11.4to show that,
if we have an FPAUS for proper colorings for any c ≥+2, then we also have an
FPRAS for this value of c.

Exercise 12.16: Consider the following simple Markov chain whose states are inde-
pendent sets in a graph G =( V , E ). To compute Xi + 1 from Xi :

 /thetachoose a vertex v uniformly at random from V , and flip a fair coin;
 /thetaif the flip is heads andv∈ Xi , then Xi + 1 = Xi \{v};
 /thetaif the flip is heads andv/∈ Xi , then Xi + 1 = Xi ;
 /thetaif the flip is tails,v/∈ Xi , and adding v to Xi still gives an independent set, then Xi + 1 =
Xi ∪{v};
 /thetaif the flip is tails andv∈ Xi , then Xi + 1 = Xi.
(a) Show that the stationary distribution of this chain is uniform over all independent
sets.
(b) We consider this Markov chain specifically on cycles and line graphs. For a line
graph with n vertices, the vertices are labeled 1 to n , and there is an edge from 1 to
2,2to3,..., n −1to n. A cycle graph on n vertices is the same with the addition
of an edge from n to 1.

coupling of markov chains
Devise a coupling ( Xt , Yt ) for this Markov chain such that, on line graphs and
cycle graphs, if dt =| Xt − Yt |+| Yt − Xt |is the number of vertices on which the
two independent sets disagree, then at each step the coupling is at least as likely to
reduce dt as to increase dt.
(c) With the coupling from part (b), argue that you can use this chain to obtain an
FPAUS for independent sets on a cycle graph or line graph. You may want to use
Exercise12.7.
(d) For the special cases of line graphs and cycle graphs, we can derive exact formulas
for the number of independent sets. Derive exact formulas for these cases and prove
that your formulas are correct. ( Hint: You may want to express your results in terms
of Fibonacci numbers.)

Exercise 12.17: For integers a and b ,an a × b grid is a graph whose vertices are all
ordered pairs of integers ( x , y ) with 0≤ x < a and 0≤ y < b. The edges of the graph
connect all pairs of distinct vertices ( x , y ) and ( x ′, y ′) such that| x − x ′|+| y − y ′|=

That is, every vertex is connected to the neighbors up, down, left, and right of it,
where vertices on the boundary are connected to the relevant points only. Consider the
following problems on the graph given by the 10×10 grid.
(a) Implement an FPAUS to generate anε-uniform proper 10-coloring of the graph,
whereεis given as an input. Discuss how many steps your Markov chain runs for,
what your starting state is, and any other relevant details.
(b) Using your FPAUS as a subroutine, implement an FPRAS to generate an (ε, δ)-
approximation to the number of proper 10-colorings of the graph. Test your code
by running it to obtain a (0. 1 , 0 .001)-approximation. ( Note: This may take a sig-
nificant amount of time to run.) Discuss the ordering you choose on the edges, how
many samples are required at each step, how many steps of the Markov chain you
perform in total throughout the process, and any other relevant details.

Exercise 12.18: In Section12.2.3we considered the following Markov chain on
independent sets: a move is made from the independent set Xt by choosing a vertex
v∈ Xt uniformly at random and picking a vertex w uniformly at random from the
graph. If Xt −{v}+{w}is an independent set, then Xt + 1 = Xt −{v}+{w}; otherwise,
Xt + 1 = Xt. We have shown that the chain converges quickly to its stationary distribu-
tion via boundingτ(ε) by an expression that is polynomial in n and ln(1/ε) whenever
k ≤ n /(2+2). Use the idea of path coupling to simplify the proof.

Exercise 12.19: In Section12.5, we considered a simple Markov chain for coloring.
Suppose that we can apply the path coupling technique. (You do not need to show this.)
In this case, we can just consider the case where dt =1. Give a simpler argument that,
when dt =1 and c > 2 , E [ dt + 1 | dt ]≤β dt for someβ<1. Also show that, when
dt =1 and c = 2 , E [ dt + 1 | dt ]≤ dt.

chapter thirteen

Martingales

Martingales are sequences of random variables satisfying certain conditions that arise
in numerous applications, such as random walks and gambling problems. We focus
here on three useful analysis tools related to martingales: the martingale stopping theo-
rem, Wald’s inequality, and the Azuma–Hoeffding inequality. The martingale stopping
theorem and Wald’s equation are important tools for computing the expectation of
compound stochastic processes. The Azuma–Hoeffding inequality is a powerful tech-
nique for deriving Chernoff-like tail bounds on the values of functions of dependent
random variables. We conclude this chapter with applications of the Azuma–Hoeffding
inequality to problems in pattern matching, balls and bins, and random graphs.

13.1 Martingales
Definition 13.1: A sequence of random variables Z 0 , Z 1 ,... is a martingale with
respect to the sequence X 0 , X 1 ,... if, for all n ≥ 0 , the following conditions hold:

 /theta Znis a function of X 0 , X 1 ,..., Xn;
 /theta E [| Zn |]<∞ ;
 /theta E [ Zn + 1 | X 0 ,..., Xn ]= Zn.
A sequence of random variables Z 0 , Z 1 ,... is called a martingale when it is a mar-
tingale with respect to itself. That is, E [| Zn |]<∞ , and E [ Zn + 1 | Z 0 ,..., Zn ]= Zn.

A martingale can have a finite or a countably infinite number of elements. The indexing
of the martingale sequence does not need to start at 0. In fact, in many applications it
is more convenient to start it at 1. When we say that Z 0 , Z 1 ,...is a martingale with
respect to X 1 , X 2 ,...,then we may consider X 0 to be a constant that is omitted.
For example, consider a gambler who plays a sequence of fair games. Let Xi be the
amount the gambler wins on the i th game ( Xi is negative if the gambler loses), and let
Zi be the gambler’s total winnings at the end of the i th game. Because each game is

martingales
fair, E [ Xi ]=0 and

E [ Zi + 1 | X 1 , X 2 ,..., Xi ]= Zi + E [ Xi + 1 ]= Zi.
Thus, Z 1 , Z 2 ,..., Zn is a martingale with respect to the sequence X 1 , X 2 ,..., Xn. Inter-
estingly, the sequence is a martingale regardless of the amount bet on each game, even
if these amounts are dependent upon previous results.
A Doob martingale refers to a martingale constructed using the following general
approach. Let X 0 , X 1 ,..., Xn be a sequence of random variables, and let Y be a random
variable with E [| Y |]<∞. (Generally, Y will depend on X 0 ,..., Xn .) Then

Zi = E [ Y | X 0 ,..., Xi ], i = 0 , 1 ,..., n ,
gives a martingale with respect to X 0 , X 1 ,..., Xn , since

E [ Zi + 1 | X 0 ,..., Xi ]= E [ E [ Y | X 0 ,..., Xi + 1 ]| X 0 ,..., Xi ]
= E [ Y | X 0 ,..., Xi ]
= Zi.
Here we have used the fact that E [ Y | X 0 ,..., Xi + 1 ] is itself a random variable and that
Definition2.7for conditional expectation yields

E [ V | W ]= E [ E [ V | U , W ]| W ].
In most applications we start the Doob martingale with Z 0 = E [ Y ], which corre-
sponds to X 0 being a trivial random variable that is independent of Y. To understand the
concept of the Doob martingale, assume that we want to predict the value of the random
variable Y and that the value of Y is a function of the values of the random variables
X 1 ,..., Xn. The sequence Z 0 , Z 1 ,..., Zn represents a sequence of refined estimates of
the value of Y , gradually using more information on the values of the random vari-
ables X 1 , X 2 ,..., Xn. The first element, Z 0 , is just the expectation of Y. Element Zi is
the expected value of Y when the values of X 1 ,..., Xi are known, and if Y is fully
determined by X 1 ,..., Xn then Zn = Y.
We now consider two examples of Doob martingales that arise in evaluating the
properties of random graphs. Let G be a random graph from Gn , p. Label the m =

( n
2
)
possible edge slots in some arbitrary order, and let

Xj =
{
1 if there is an edge in the j th edge slot,
0 otherwise.
Consider any finite-valued function F defined over graphs; for example, let F ( G )be
the size of the largest independent set in G. Now let Z 0 = E [ F ( G )] and

Zi = E [ F ( G )| X 1 ,..., Xi ], i = 1 ,..., m.
The sequence Z 0 , Z 1 ,..., Zm is a Doob martingale that represents the conditional
expectations of F ( G ) as we reveal whether each edge is in the graph, one edge at a
time. This process of revealing edges gives a martingale that is commonly called the
edge exposure martingale.

13.2 Stopping Times
Similarly, instead of revealing edges one at a time, we could reveal the set of edges
connected to a given vertex, one vertex at a time. Fix an arbitrary numbering of the
vertices 1 through n , and let Gi be the subgraph of G induced by the first i vertices.
Then, setting Z 0 = E [ F ( G )] and
Zi = E [ F ( G )| G 1 ,..., Gi ], i = 1 ,..., n ,
gives a Doob martingale that is commonly called the vertex exposure martingale.
13.2. Stopping Times
Returning to the gambler who participates in a sequence of fair gambling rounds, we
saw in the previous section that Z 1 , Z 2 ,...is a martingale, where Zi is the gambler’s
winnings after the i th game. If the player decides (before starting to play) to quit after
exactly k games, what are the gambler’s expected winnings?
Lemma 13.1: If the sequence Z 0 , Z 1 ,..., Zn is a martingale with respect to
X 0 , X 1 ,..., Xn, then
E [ Zn ]= E [ Z 0 ].
Proof: Since Z 0 , Z 1 ,...is a martingale with respect to X 0 , X 1 ,..., Xn , it follows that
Zi = E [ Zi + 1 | X 0 ,..., Xi ].
Taking the expectation of both sides and using the definition of conditional expectation,
we have
E [ Zi + 1 ]= E [ E [ Zi + 1 | X 0 ,..., Xi ]]= E [ Zi ].
Repeating this argument yields
E [ Zn ]= E [ Z 0 ].  /theta
Thus, if the number of games played is initially fixed then the expected gain from
the sequence of games is zero. Suppose now that the number of games played is not
fixed. For example, the gambler could choose to play a random number of games. An
even more complex (and realistic) situation arises when the gambler’s decision to quit
playing is based on the outcome of the games already played. For example, the gambler
could decide to keep playing until his winnings total at least a hundred dollars. The
following notion proves quite powerful.
Definition 13.2: A nonnegative integer-valued random variable T is a stopping time
for the sequence { Zi , i ≥ 0 } if the probability of the event T = n is independent of the
variables { Zn + j | Z 1 ,..., Zn , j ≥ 1 } (i.e. the variables Zn + 1 , Zn + 2 ,... conditioned on
the values of Z 1 ,..., Zn).^1
(^1) More formally, in the discrete case we define a “filtration”F 0 ,F 1 ,...such that the distribution of Z 0 is fully
defined by events inF 0 , and the joint distribution of Z 0 ,..., Zn is fully defined by events inF n .Theevent T = n
is a stopping time if it is an event inF n.

martingales
A stopping time corresponds to a strategy for determining when to stop a sequence
based only on the outcomes seen so far. For example, the first time the gambler wins
five games in a row is a stopping time, since this can be determined by looking at the
outcomes of the games played. Similarly, the first time the gambler has won at least
a hundred dollars is also a stopping time. Letting T be the last time the gambler wins
five games in a row, however, would not be a stopping time, since determining whether
T = n cannot be done without knowing Zn + 1 , Zn + 2 ,....
In order to fully utilize the martingale property, we need to characterize conditions
on the stopping time T that maintain the property E [ ZT ]= E [ Z 0 ]=0. It would seem,
if the game is fair, that E [ ZT ]=0 should always hold. But consider the case where
the gambler’s stopping time is the first T such that ZT > B , where B is a fixed constant
greater than 0. In this case, the expected gain when the gambler quits playing is greater
than 0. The subtle problem with this stopping time is that it might not be finite, so the
gambler may never finish playing. The martingale stopping theorem shows that, under
certain conditions and in particular when the stopping time is bounded or has bounded
expectation, the expected value of the martingale at the stopping time is equal to E [ Z 0 ].
We state a version of the martingale stopping theorem (sometimes called the optional
stopping theorem) without proof.

Theorem 13.2 [ Martingale Stopping Theorem ] : If Z 0 , Z 1 ,... is a martingale with
respect to X 1 , X 2 ,... and if T is a stopping time for X 1 , X 2 ,..., then

E [ ZT ]= E [ Z 0 ]
whenever one of the following holds:

 /theta the Ziare bounded, so there is a constant c such that, for all i, | Zi |≤ c;
 /theta T is bounded;
 /theta E [ T ]<∞ , and there is a constant c such that E [| Zi + 1 − Zi || X 1 ,..., Xi ]< c.
We use the martingale stopping theorem to derive a simple solution to the gambler’s
ruin problem introduced in Section7.2.1. Consider a sequence of independent, fair
gambling games. In each round, a player wins a dollar with probability 1/2 or loses a
dollar with probability 1/2. Let Z 0 =0, let Xi be the amount won on the i th game, and
let Zi be the total won by the player after i games (again, Xi and Zi are negative if the
player loses money). Assume that the player quits the game when she either loses n 1
dollars or wins n 2 dollars. What is the probability that the player wins n 2 dollars before
losing n 1 dollars?
Let the time T be the first time the player has either won n 2 or lost n 1. Then T is a
stopping time for X 1 , X 2 ,....The sequence Z 0 , Z 1 ,...is a martingale and, since the
values of the Zi are clearly bounded, we can apply the martingale stopping theorem.
We therefore have

E [ ZT ]= 0.
Let q be the probability that the gambler quits playing after winning n 2 dollars. Then

E [ ZT ]= n 2 q − n 1 (1− q )= 0 ,
13.2 stopping times
giving

q =
 n 1
 n 1 + n 2
,
matching the result found in Section7.2.1.

13.2.1 Example: A Ballot Theorem
The following ballot theorem is another application of the martingale stopping theo-
rem. Suppose that two candidates run for an election. Candidate A obtains a votes,
and candidate B obtains b < a votes. The votes are counted in a random order, chosen
uniformly at random from all permutations on the a + b votes. We show that the prob-
ability that candidate A is always ahead in the count is ( a − b )/( a + b ). Although this
can be determined combinatorially, we provide an elegant martingale argument.
Let n = a + b be the total number of votes, and let Sk be the number of votes by
which candidate A is leading after k votes are counted ( Sk can be negative). Then Sn =
a − b .For0≤ k ≤ n −1, define

Xk =
Sn − k
n − k
.
We first show that the sequence X 0 , X 1 ,..., Xn − 1 forms a martingale. Note that the
sequence X 0 , X 1 ,..., Xn relates to the counting process in a backward order; X 0 is a
function of Sn , Xn − 1 is a function of S 1 , and so on. Consider

E [ Xk | X 0 ,..., Xk − 1 ].
Conditioning on X 0 ,..., Xk − 1 is equivalent to conditioning on Sn , Sn − 1 ,..., Sn − k + 1 ,
which in turn is equivalent to conditioning on the values of the count when counting
the last k −1 votes.
Conditioning on Sn − k + 1 , the number of votes that candidate A had after counting the
first n − k +1 votes is

n − k + 1 + Sn − k + 1
2
,
and the number of votes that candidate B had is

n − k + 1 − Sn − k + 1
2
.
The ( n − k +1)th vote in the count is a random vote from among these first n − k + 1
votes. Also, Sn − k is equal to Sn − k + 1 +1 if the ( n − k +1)th vote was for candidate B
and equal to Sn − k + 1 −1 if that vote was for candidate A. Thus, for k ≥1,

E [ Sn − k | Sn − k + 1 ]=( Sn − k + 1 +1)
n − k + 1 − Sn − k + 1
2( n − k +1)
+( Sn − k + 1 −1)
n − k + 1 + Sn − k + 1
2( n − k +1)
= Sn − k + 1
n − k
n − k + 1
.
martingales
Therefore,

E [ Xk | X 0 ,..., Xk − 1 ]= E
[
Sn − k
n − k
| Sn ,..., Sn − k + 1
]
=
Sn − k + 1
n − k + 1
= Xk − 1 ,
showing that the sequence X 0 , X 1 ,..., Xn − 1 is a martingale.
Define T to be the minimum k such that Xk =0 if such a k exists, and T = n − 1
otherwise. Then T is a bounded stopping time, satisfying the requirements of the mar-
tingale stopping theorem, and

E [ XT ]= E [ X 0 ]=
E [ Sn ]
n
=
a − b
a + b
We now consider two cases.

Case 1: Candidate A leads throughout the count. In this case, all Sn − k (and therefore
all Xk ) are positive for 0≤ k ≤ n −1, T = n −1, and

XT = Xn − 1 = S 1 = 1.
That S 1 =1 follows because candidate A must receive the first vote in the count to
be ahead throughout the count.
Case 2: Candidate A does not lead throughout the count. In that case we claim that for
some k < n −1, Xk =0. Candidate A clearly has more votes at the end. If candidate
B ever leads, then there must be some intermediate point k where Sk (and therefore
Xk ) is 0. In this case, T = k < n −1 and XT =0.

Observe that

E [ XT ]=
a − b
a + b
= 1 ·Pr(Case 1)+ 0 ·Pr(Case 2),
and thus the probability of Case 1, in which candidate A leads throughout the count, is
( a − b )/( a + b ).

13.3 Wald’s Equation
An important corollary of the martingale stopping theorem is known as Wald’s equa-
tion. Wald’s equation deals with the expectation of the sum of independent random
variables in the case where the number of random variables being summed is itself a
random variable.

Theorem 13.3 [ Wald’s Equation ] : Let X 1 , X 2 ,... be nonnegative, independent, iden-
tically distributed random variables with distribution X. Let T be a stopping time for

13.3wald’s equation
this sequence. If T and X have bounded expectation, then

E
[ T
∑
i = 1
Xi
]
= E [ T ]· E [ X ].
In fact, Wald’s equation holds more generally; there are different proofs of the equality
that do not require the random variables X 1 , X 2 ,...to be nonnegative.

Proof: For i ≥1, let

Zi =
∑ i
j = 1
( Xj − E [ X ]).
The sequence Z 1 , Z 2 ,...is a martingale with respect to X 1 , X 2 ,...,and E [ Z 1 ]=0.
Now, E [ T ]<∞and

E [| Zi + 1 − Zi || X 1 ,..., Xi ]= E [| Xi + 1 − E [ X ]|]≤ 2 E [ X ].
Hence we can apply the martingale stopping theorem to compute

E [ ZT ]= E [ Z 1 ]= 0.
We now find

E [ ZT ]= E
⎡
⎣
∑ T
j = 1
( Xj − E [ X ])
⎤
⎦
= E
⎡
⎣
⎛
⎝
∑ T
j = 1
Xj
⎞
⎠− T E [ X ]
⎤
⎦
= E
⎡
⎣
∑ T
j = 1
Xj
⎤
⎦− E [ T ]· E [ X ]
= 0 ,
which gives the result.  /theta

In the case of a sequence of independent random variables, we have an equivalent,
simpler definition of stopping time that is easier to apply.

Definition 13.3: Let Z 0 , Z 1 ,... be a sequence of independent random variables. A
nonnegative, integer-valued random variable T is a stopping time for the sequence if
the event T = n is independent of Zn + 1 , Zn + 2 ,....

As a simple example, consider a gambling game in which a player first rolls one stan-
dard die. If the outcome of the roll is X then she rolls X new standard dice and her gain
Z is the sum of the outcomes of the X dice. What is the expected gain of this game?

martingales
For 1≤ i ≤ X , let Yi be the outcome of the i th die in the second round. Then
E [ Z ]= E
[ X
∑
i = 1
Yi
]
.
By Definition13.3, X is a stopping time, and hence by Wald’s equality we obtain

E [ Z ]= E [ X ]· E [ Yi ]=
(
7
2
) 2
=
49
4
.
Wald’s equation can arise in the analysis of Las Vegas algorithms, which always
give the right answer but have variable running times, as we saw for the randomized
algorithm for the median described in Section3.5. In a Las Vegas algorithm we often
repeatedly perform some randomized subroutine that may or may not return the right
answer. We then use some deterministic checking subroutine to determine whether or
not the answer is correct; if it is correct then the Las Vegas algorithm terminates with
the correct answer, and otherwise the randomized subroutine is run again. If N is the
number of trials until a correct answer is found and if Xi is the running time for the
two subroutines on the i th trial, then – as long as the Xi are independent and identically
distributed with distribution X – Wald’s equation gives that the expected running time
for the algorithm is

E
[ N
∑
i = 1
Xi
]
= E [ N ]· E [ X ].
An example of this approach is given in Exercise13.13.
As another example, consider a set of n servers communicating through a shared
channel. Time is divided into discrete slots. At each time slot, any server that needs
to send a packet can transmit it through the channel. If exactly one packet is sent at
that time, the transmission is successfully completed. If more than one packet is sent,
then none are successful (and the senders detect the failure). Packets are stored in the
server’s buffer until they are successfully transmitted. Servers follow the following sim-
ple protocol: at each time slot, if the server’s buffer is not empty then with probability
1 / n it attempts to send the first packet in its buffer. Assume that servers have an infinite
sequence of packets in their buffers. What is the expected number of time slots used
until each server successfully sends at least one packet?
Let N be the number of packets successfully sent until each server has successfully
sent at least one packet. Let ti be the time slot in which the i th successfully transmitted
packet is sent, starting from time t 0 =0, and let ri = ti − ti − 1. Then T , the number of
time slots until each server successfully sends at least one packet, is given by

T =
∑ N
i = 1
ri.
You may check that N is independent of the ri , and N is bounded in expectation; hence
N is a stopping time for the sequence of ri.

13.4 Tail Inequalities for Martingales
The probability that a packet is successfully sent in a given time slot is
p =
(
n
1
)(
1
n
)(
1 −
1
n
) n − 1
≈e−^1.
The ri each have a geometric distribution with parameter p ,so E [ ri ]= 1 / p ≈e.
Given that a packet was successfully sent at a given time slot, the sender of that
packet is uniformly distributed among the n servers, independent of previous steps.
Using our analysis of the expectation of the coupon collector’s problem from Chap-
ter 2 , we deduce that E [ N ]= nH ( n )= n ln n + O ( n ). We now use Wald’s equality to
compute

E [ T ]= E
[ N
∑
i = 1
ri
]
= E [ N ]· E [ ri ]
=
nH ( n )
p
,
which is about e n ln n.

13.4. Tail Inequalities for Martingales
Perhaps the most useful property of martingales for the analysis of algorithms is that
Chernoff-like tail inequalities can apply, even when the underlying random variables
are not independent. The main results in this area are Azuma’s inequality and Hoeff-
ding’s inequality. They are quite similar, so they are often together referred to as the
Azuma–Hoeffding inequality.

Theorem 13.4 [ Azuma–Hoeffding Inequality ] : Let X 0 ,..., Xnbe a martingale such
that

| Xk − Xk − 1 |≤ ck.
Then, for all t ≥ 1 and any λ> 0 ,

Pr(| Xt − X 0 |≥λ)≤2e−λ
(^2) /(2∑ tk = 1 c (^2) k )
.
Proof: The proof follows the same format as that for Chernoff bounds (Section4.2).
We first derive an upper bound for E [eα( Xt − X^0 )]. Toward that end, we define
Yi = Xi − Xi − 1 , i = 1 ,..., t.
Note that| Yi |≤ ci and, since X 0 , X 1 ,...is a martingale,
E [ Yi | X 0 , X 1 ,..., Xi − 1 ]= E [ Xi − Xi − 1 | X 0 , X 1 ,..., Xi − 1 ]
= E [ Xi | X 0 , X 1 ,..., Xi − 1 ]− Xi − 1 = 0.
Now consider
E [eα Yi | X 0 , X 1 ,..., Xi − 1 ].

martingales
Writing

Yi =− ci
1 − Yi / ci
2
+ ci
1 + Yi / ci
2
and using the convexity of eα Yi , we have that

eα Yi ≤
1 − Yi / ci
2
e−α ci +
1 + Yi / ci
2
eα ci
=
eα ci +e−α ci
2
+
Yi
2 ci
(eα ci −e−α ci ).
Since E [ Yi | X 0 , X 1 ,..., Xi − 1 ]=0, we have
E [eα Yi | X 0 , X 1 ,..., Xi − 1 ]≤ E
[
eα ci +e−α ci
2
+
Yi
2 ci
(eα ci −e−α ci )
∣
∣ X 0 , X 1 ,..., Xi − 1
]
=
eα ci +e−α ci
2
≤e(α ci )^2 /^2.
Here we have used the Taylor series expansion of e x to find

eα ci +e−α ci
2
≤e(α ci )
(^2) / 2
,
in a manner similar to the proof of Theorem4.7. It follows that

E
[
eα( Xt − X^0 )
]
= E
[ t
∏
i = 1
eα Yi
]
= E
[ t − 1
∏
i = 1
eα Yi
]
E [eα Yt | X 0 , X 1 ,..., Xt − 1 ]
≤ E
[ t − 1
∏
i = 1
eα Yi
]
e(α ct )
(^2) / 2
≤eα
2 ∑ tk = 1 c (^2) k / 2
.
Hence,
Pr( Xt − X 0 ≥λ)=Pr(eα( Xt − X^0 )≥eαλ)
≤
E [eα( Xt − X^0 )]
eαλ
≤eα
2 ∑ tk = 1 c (^2) k / 2 −αλ
≤e−λ
(^2) /(2∑ tk = 1 c (^2) k )
,
where the last inequality comes from choosingα=λ/
∑ t
k = 1 c
2
k. A similar argument
gives the bound for Pr( Xt − X 0 ≤−λ), as can be seen for example by replacing Xi
everywhere by− Xi , giving the theorem.  /theta

13.5 Applications of the Azuma–Hoeffding Inequality
The following corollary is often easier to apply.

Corollary 13.5: Let X 0 , X 1 ,... be a martingale such that, for all k ≥ 1 ,

| Xk − Xk − 1 |≤ c.
Then, for all t ≥ 0 and λ> 0 ,

Pr
(
| Xt − X 0 |≥λ c
√
t
)
≤2e−λ
(^2) / 2
We now present a more general form of the Azuma–Hoeffding inequality that yields
slightly tighter bounds in our applications.
Theorem 13.6 [ Azuma–Hoeffding Inequality ] : Let X 0 ,..., Xnbe a martingale such
that
Bk ≤ Xk − Xk − 1 ≤ Bk + dk
for some constants dkand for some random variables Bkthat may be functions of
X 0 , X 1 ,..., Xk − 1_. Then, for all t_ ≥ 0 and any λ> 0 ,
Pr(| Xt − X 0 |≥λ)≤2e−^2 λ
(^2) /(∑ tk = 1 d (^2) k )
This version of the inequality generalizes the requirement of a bound on| Xk − Xk − 1 |.
The key is the gap dk between the lower and upper bounds for Xk − Xk − 1. Notice that,
when we have the bound| Xk − Xk − 1 |≤ ck , this result is equivalent to Theorem13.4
using Bk =− ck with a gap dk = 2 ck. The proof is similar to that for Theorem13.4and
is left as Exercise13.7.

13.5. Applications of the Azuma–Hoeffding Inequality
13.5.1 General Formalization
Before giving several applications of the Azuma–Hoeffding inequality, we describe a
useful general technique. Let us say that a function

f ( X ̄)= f ( X 1 , X 2 ,..., Xn )
satisfies a Lipschitz condition with bound c if, for any i and for any set of values
x 1 ,..., xn and yi ,

| f ( x 1 , x 2 ,..., xi − 1 , xi , xi + 1 ,..., xn )− f ( x 1 , x 2 ,..., xi − 1 , yi , xi + 1 ,..., xn )|≤ c.
That is, changing the value of any single coordinate can change the function value by
at most c.
Let

Z 0 = E [ f ( X 1 , X 2 ,..., Xn )]
and

Zk = E [ f ( X 1 , X 2 ,..., Xn )| X 1 , X 2 ,..., Xk ].
martingales
The sequence Z 0 , Z 1 ,...is a Doob martingale, and if the Xk are independent ran-
dom variables then we claim that there exist random variables Bk , depending only on
Z 0 ,..., Zk − 1 , with Bk ≤ Zk − Zk − 1 ≤ Bk + c. The gap between the lower and upper
bounds on Zk − Zk − 1 is then at most c , so the Azuma–Hoeffding inequality of Theo-
rem13.6applies. This variation on the Azuma-Hoeffding inequality is often referred
to as McDiarmid’s inequality, which we now formalize.

Theorem 13.7 [ McDiarmid’s Inequality ] : Let f be a function on n variables that
satisfies the above Lipschitz condition with bound c. Let X 1 ,..., Xnbe independent
random variables such that f ( X 1 ,..., Xn ) is in the domain of f. Then

Pr(| f ( X 1 ,..., Xn )− E [ f ( X 1 ,..., Xn )]≥λ)≤2e−^2 λ
(^2) /( nc (^2) )
.
Proof: We prove this for the case of discrete random variables (although the result
holds more generally). To ease the notation, we use Sk as shorthand for X 1 , X 2 ,..., Xk ,
so that we write
E [ f ( X ̄)| Sk ]
for
E [ f ( X ̄)| X 1 , X 2 ,..., Xk ].
Also, let us abuse notation and define
fk ( X ̄, x )= f ( X 1 ,..., Xk − 1 , x , Xk + 1 ,..., Xn ).
That is, fk ( X ̄, x )is f ( X ̄) with the value x in the k th coordinate. We shall likewise write
fk ( ̄ z , x )= f ( z 1 ,..., zk − 1 , x , zk + 1 ,..., zn ).
Given this notation, we have
Zk − Zk − 1 = E [ f ( X ̄)| Sk ]− E [ f ( X ̄)| Sk − 1 ].
Hence Zk − Zk − 1 is bounded above by
sup
x
E [ f ( X ̄)| Sk − 1 , Xk = x ]− E [ f ( X ̄)| Sk − 1 ]
and bounded below by
inf y E [ f ( X ̄)| Sk − 1 , Xk = y ]− E [ f ( X ̄)| Sk − 1 ].
(If we are dealing with random variables that can take on only a finite number of values,
we could use max and min in place of sup and inf.) Therefore, letting
Bk =inf y E [ f ( X ̄)| Sk − 1 , Xk = y ]− E [ f ( X ̄)| Sk − 1 ],
if we can bound
sup
x
E [ f ( X ̄)| Sk − 1 , Xk = x ]−inf y E [ f ( X ̄)| Sk − 1 , Xk = y ]≤ c

13.5applications of the azuma–hoeffding inequality
then we will have appropriately bounded the gap Zk − Zk − 1 .Now

sup
x
E [ f ( X ̄)| Sk − 1 , Xk = x ]−inf
y
E [ f ( X ̄)| Sk − 1 , Xk = y ]
=sup
x , y
(
E [ f ( X ̄)| Sk − 1 , Xk = x ]− E [ f ( X ̄)| Sk − 1 , Xk = y ]
)
=sup
x , y
E [ fk ( X ̄, x )− fk ( X ̄, y )| Sk − 1 ].
Because the Xi are independent, the probability of any specific set of values for
Xk + 1 through Xn does not depend on the values of X 1 ,..., Xk. Hence, for any values
x , y , z 1 ,..., zk − 1 we have that

E [ fk ( X ̄, x )− fk ( X ̄, y )| X 1 = z 1 ,..., Xk − 1 = zk − 1 ]
is equal to
∑

zk + 1 ,..., zn
Pr(( Xk + 1 = zk + 1 )∩···∩( Xn = zn ))·( fk ( ̄ z , x )− fk ( ̄ z , y )).
But

fk ( ̄ z , x )− fk ( ̄ z , y )≤ c ,
and hence

E [ fk ( X ̄, x )− fk ( X ̄, y )| Sk − 1 ]≤ c ,
giving the required bound, so that we may apply Theorem12.6to conclude the
proof.  /theta

The requirement that the Xi be independent random variables is essential to applying
this general framework. Finding a counterexample when the Xi are not independent is
left as Exercise13.21.

13.5.2 Application: Pattern Matching
In many scenarios, including examining DNA structure, a goal is to find interesting
patterns in a sequence of characters. In this context, the phrase “interesting patterns”
often refers to strings that occur more often than one would expect if the characters were
simply generated randomly. This notion of “interesting” is reasonable if the number of
occurrences of a string is concentrated around its expectation in the random model.
We show concentration using the Azuma–Hoeffding inequality for a simple random
model.
Let X =( X 1 ,..., Xn ) be a sequence of characters chosen independently and uni-
formly at random from an alphabet, where s =||. Let B =( b 1 ,..., bk )beafixed
string of k characters from. Let F be the number of occurrences of the fixed string B
in the random string X. Clearly,

E [ F ]=( n − k +1)
(
1
s
) k
.
martingales
We use a Doob martingale and the Azuma–Hoeffding inequality to show that, if k
is relatively small with respect to n , then the number of occurrences of B in X is highly
concentrated around its mean.
Let
Z 0 = E [ F ],

and for 1≤ i ≤ n let

Zi = E [ F | X 1 ,..., Xi ].
The sequence Z 0 ,..., Zn is a Doob martingale, and

Zn = F.
Since each character in the string X can participate in no more than k possible
matches, for any 0≤ i ≤ n we have

| Zi + 1 − Zi |≤ k.
In other words, the value of Xi + 1 can affect the value of F by at most k in either direction,
since Xi + 1 participates in no more than k possible matches. Hence the difference

| E [ F | X 1 ,..., Xi + 1 ]− E [ F | X 1 ,..., Xi ]|=| Zi + 1 − Zi |
must be at most k. Applying Theorem13.4yields

Pr(| F − E [ F ]|≥ε)≤2e−ε
(^2) / 2 nk 2
,
or (from Corollary13.5)
Pr

(
| F − E [ F ]|≥λ k
√
n
)
≤2e−λ
(^2) / 2
We can obtain slightly better bounds by applying the general framework of Theo-
rem13.6. Let F = f ( X 1 , X 2 ,..., Xn ). Then, by our preceding argument, changing the
value of any single Xi can change the value of F by at most k , and hence the function
satisfies the Lipschitz condition with bound k. Theorem13.6then applies to give
Pr(| F − E [ F ]|≥ε)≤2e−^2 ε
(^2) / nk 2
,
improving the value in the exponent by a factor of 4.

13.5.3 Application: Balls and Bins
Suppose that we are throwing m balls independently and uniformly at random into n
bins. Let Xi be the random variable representing the bin into which the i th ball falls.
Let F be the number of empty bins after the m balls are thrown. Then the sequence

Zi = E [ F | X 1 ,..., Xi ]
is a Doob martingale. We claim that F = f ( X 1 , X 2 ,..., Xm ) satisfies the Lipschitz con-
dition with bound 1. Consider how F changes from the placement of the i th ball. If the
i th ball lands in a bin on its own, then changing Xi so that the i th ball lands in a bin with
some other ball will increase F by 1. Similarly, if the i th ball lands in a bin with other

13.6 Exercises
balls, then changing Xi so that the i th ball lands in an otherwise empty bin decreases F
by 1. In all other cases, changing Xi leaves F the same. We therefore obtain

Pr(| F − E [ F ]|≥ε)≤2e−^2 ε
(^2) / m
by the Azuma–Hoeffding inequality of Theorem13.6. We could also apply Theo-
rem13.4with| Zi + 1 − Zi |≤1, but this gives a slightly weaker result. Here
E [ F ]= n

(
1 −
1
n
) m
,
but we could obtain the concentration result without knowing E [ F ].
This result can be improved by taking more care in bounding the gap between the
bounds on Zi + 1 − Zi. This is considered in Exercise13.20.

13.5.4 Application: Chromatic Number
Given a random graph G in Gn , p , the chromatic number χ( G ) is the minimum number
of colors needed in order to color all vertices of the graph so that no adjacent vertices
have the same color. We use the vertex exposure martingale defined in Section13.1to
obtain a concentration result forχ( G ).
Let Gi be the random subgraph of G induced by the set of vertices 1,..., i , let
Z 0 = E [χ( G )], and let

Zi = E [χ( G )| G 1 ,..., Gi ].
Since a vertex uses no more than one new color, again we have that the gap between Zi
and Zi − 1 is at most 1, so we can apply the general framework of the Azuma–Hoeffding
inequality from Theorem13.6. We conclude that

Pr
(
|χ( G )− E [χ( G )]|≥λ
√
n
)
≤2e−^2 λ
2
.
This result holds even without knowing E [χ( G )].

13.6. Exercises
Exercise 13.1: Show that, if Z 0 , Z 1 ,..., Zn is a martingale with respect to
X 0 , X 1 ,..., Xn , then it is also a martingale with respect to itself.

Exercise 13.2: ∑ Let X 0 , X 1 ,...be a sequence of random variables, and let Si =
i
j = 1 Xj. Show that if S^0 , S^1 ,...is a martingale with respect to X^0 , X^1 ,..., then for
all i = j , E [ XiXj ]=0.

Exercise 13.3: Let X 0 =0 and for j ≥0 let Xj + 1 be chosen uniformly over the real
interval [ Xj ,1]. Show that, for k ≥0, the sequence

Yk = 2 k (1− Xk )
is a martingale.

martingales
Exercise 13.4: Let X 1 , X 2 ,...be independent and identically distributed random vari-
ables with expectation 0 and varianceσ^2 <∞. Let

Zn =
( n
∑
i = 1
Xi
) 2
− n σ^2.
Show that Z 1 , Z 2 ,...is a martingale.

Exercise 13.5: Consider the gambler’s ruin problem, where a player plays a sequence
of independent games, either winning one dollar with probability 1/2 or losing one
dollar with probability 1/2. The player continues until either losing n 1 dollars or win-
ning n 2 dollars. Let Xn be 1 if the player wins the n th game and−1 otherwise. Let
Zn =

(∑ n
i = 1 Xi
) 2
− n.
(a) Show that Z 1 , Z 2 ,...is a martingale.
(b) Let T be the stopping time when the player finishes playing. Determine E [ ZT ].
(c) Calculate E [ T ]. ( Hint: You can use what you already know about the probability
that the player wins.)

Exercise 13.6: Consider the gambler’s ruin problem, where now the independent
games are such that the player either wins one dollar with probability p < 1 /2 or loses
one dollar with probability 1− p. As in Exercise13.5, the player continues until either
losing n 1 dollars or winning n 2 dollars. Let Xn be 1 if the player wins the n th game and
−1 otherwise, and let Zn be the player’s total winnings after n games.

(a) Show that
An =
(
1 − p
p
) Zn
is a martingale with mean 1.
(b) Determine the probability that the player wins n 2 dollars before losing n 1 dollars.
(c) Show that
Bn = Zn −(2 p −1) n

is a martingale with mean 0.
(d) Let T be the stopping time when the player finishes playing. Determine E [ ZT ], and
use it to determine E [ T ]. ( Hint: You can use what you already know about the
probability that the player wins.)

Exercise 13.7: Prove Theorem13.6.[ Hint: Use Lemma4.13.]

Exercise 13.8: In the bin-packing problem, we are given items with sizes
a 1 , a 2 ,..., an with 0≤ ai ≤1for1≤ i ≤ n. The goal is to pack them into the mini-
mum number of bins, with each bin being able to hold any collection of items whose
total sizes sum to at most 1. Suppose that each of the ai is chosen independently accord-
ing to some distribution (which might be different for each i ). Let P be the number of

13.6exercises
bins required in the best packing of the resulting items. Prove that

Pr(| P − E [ P ]|≥λ)≤2e−^2 λ
(^2) / n
.
Exercise 13.9: Consider an n -cube with N = 2 n nodes. Let S be a nonempty set of
vertices on the cube, and let x be a vertex chosen uniformly at random among all vertices
of the cube. Let D ( x , S ) be the minimum number of coordinates in which x and y differ
over all points y ∈ S. Give a bound on
Pr(| D ( x , S )− E [ D ( x , S )]|>λ).
Exercise 13.10: In Chapter 4 we developed a tail bound for the sum of{ 0 , 1 }random
variables. We can use martingales to generalize this result for the sum of any random
variables whose range lies in [0,1]. Let X 1 , X 2 ,..., Xn be independent random vari-
ables such that Pr(0≤ Xi ≤1)=1. If Sn =
∑ n
i = 1 Xi , show that
Pr(| Sn − E [ Sn ]|≥λ)≤2e−^2 λ
(^2) / n
.
Exercise 13.11: A parking-lot attendant has mixed up n keys for n cars. The n car
owners arrive together. The attendant gives each owner a key according to a permutation
chosen uniformly at random from all permutations. If an owner receives the key to his
car, he takes it and leaves; otherwise, he returns the key to the attendant. The attendant
now repeats the process with the remaining keys and car owners. This continues until
all owners receive the keys to their cars.
Let R be the number of rounds until all car owners receive the keys to their cars. We
want to compute E [ R ]. Let Xi be the number of owners who receive their car keys in
the i th round. Prove that
Yi =
∑ i
j = 1
( Xj − E [ Xj | X 1 ,..., Xj − 1 ])
is a martingale. Use the martingale stopping theorem to compute E [ R ].
Exercise 13.12: Alice and Bob play each other in a checkers tournament, where the
first player to win four games wins the match. The players are evenly matched, so the
probability that each player wins each game is 1/2, independent of all other games.
The number of minutes for each game is uniformly distributed over the integers in the
range [30,60], again independent of other games. What is the expected time they spend
playing the match?
Exercise 13.13: Consider the following extremely inefficient algorithm for sorting
n numbers in increasing order. Start by choosing one of the n numbers uniformly at
random, and placing it first. Then choose one of the remaining n −1 numbers uniformly
at random, and place it second. If the second number is smaller than the first, start
over again from the beginning. Otherwise, next choose one of the remaining n − 2
numbers uniformly at random, place it third, and so on. The algorithm starts over from
the beginning whenever it finds that the k th item placed is smaller than the ( k −1)th

martingales
item. Determine the expected number of times the algorithm tries to place a number,
assuming that the input consists of n distinct numbers.

Exercise 13.14: Suppose that you are arranging a chain of n dominos so that, once
you are done, you can have them all fall sequentially in a pleasing manner by knocking
down the lead domino. Each time you try to place a domino in the chain, there is some
chance that it falls, taking down all of the other dominos you have already carefully
placed. In that case, you must start all over again from the very first domino.

(a) Let us call each time you try to place a domino a trial. Each trial succeeds with
probability p. Using Wald’s equation, find the expected number of trials necessary
before your arrangement is ready. Calculate this number of trials for n =100 and
p = 0 .1.
(b) Suppose instead that you can break your arrangement into k components, each
of size n / k , in such a way so that once a component is complete, it will not fall
when you place further dominos. For example: if you have 10 components of size
10, then once the first component of 10 dominos are placed successfully they will
not fall; misplacing a domino later might take down another component, but the
first will remain ready. Find the expected number of trials necessary before your
arrangement is ready in this case. Calculate the number of trials for n =100, k =
10, and p = 0 .1, and compare with your answer from part (a).

Exercise 13.15: (a) Let X 1 , X 2 ,...be a sequence of independent exponential random
variables, each with mean 1. Given a positive real number k , let N be defined by

N =min
{
n :
∑ n
i = 1
Xi > k
}
.
That is, N is the smallest number for which the sum of the first N of the Xi is larger than
k. Use Wald’s inequality to determine E [ N ].
(b) Let X 1 , X 2 ,...be a sequence of independent uniform random variables on the
interval (0,1). Given a positive real number k with 0< k <1, let N be defined by

N =min
{
n :
∏ n
i = 1
Xi < k
}
.
That is, N is the smallest number for which the product of the first N of the Xi is smaller
than k. Determine E [ N ]. ( Hint: You may find Exercise8.9helpful.)

Exercise 13.16: A subsequence of a string s is any string that can be obtained by delet-
ing characters from s. Consider two strings x and y of length n , where each character
in each string is independently a 0 with probability 1/2 and a 1 with probability 1/2.
We consider the longest common subsequence of the two strings.

(a) Show that the expected length of the longest common subsequence is greater than
c 1 n and less than c 2 n for constants c 1 > 1 /2 and c 2 <1 when n is sufficiently large.
13.6exercises
(Any constants c 1 and c 2 will do; as a challenge, you may attempt to find the best
constants c 1 and c 2 that you can.)
(b) Use a martingale inequality to show that the length of the longest common subse-
quence is highly concentrated around its mean.

Exercise 13.17: Given a bag with r red balls and g green balls, suppose that we uni-
formly sample n balls from the bin without replacement. Set up an appropriate martin-
gale and use it to show that the number of red balls in the sample is tightly concentrated
around nr /( r + g ).

Exercise 13.18: We showed in Chapter 5 that the fraction of entries that are 0 in a
Bloom filter is concentrated around

p ′=
(
1 −
1
n
) km
,
where m is the number of data items, k is the number of hash functions, and n is the
size of the Bloom filter in bits. Derive a similar concentration result using a martingale
inequality.

Exercise 13.19: Consider a random graph from Gn , N , where N = cn for some constant
c >0. Let X be the number of isolated vertices (i.e., vertices of degree 0).

(a) Determine E [ X ].
(b) Show that

Pr
(
| X − E [ X ]|≥ 2 λ
√
cn
)
≤2e−λ
(^2) / 2
.
( Hint: Use a martingale that reveals the locations of the edges in the graph, one at
a time.)
Exercise 13.20: We improve our bound from the Azuma–Hoeffding inequality for
the problem where m balls are thrown into n bins. We let F be the number of empty
bins after the m balls are thrown and Xi the bin in which the i th ball lands. We define
Z 0 = E [ F ] and Zi = E [ F | X 1 ,..., Xi ].
(a) Let Ai denote the number of bins that are empty after the i th ball is thrown. Show
that in this case
Zi − 1 = Ai − 1

(
1 −
1
n
) m − i + 1
.
(b) Show that, if the i th ball lands in a bin that is empty when it is thrown, then

Zi =( Ai − 1 −1)
(
1 −
1
n
) m − i
.
(c) Show that, if the i th ball lands in a bin that is not empty when it is thrown, then
Zi = Ai − 1
(
1 −
1
n
) m − i
.
martingales
(d) Show that the Azuma–Hoeffding inequality of Theorem13.6applies with di =
(1− 1 / n ) m − i.
(e) Using part (d), prove that
Pr(| F − E [ F ]|≥λ)≤2e−λ

(^2) (2 n −1)/( n (^2) −( E [ F ]) (^2) )
.
Exercise 13.21: Let f ( X 1 , X 2 ,..., Xn ) satisfy the Lipschitz condition so that, for any
i and any values x 1 ,..., xn and yi ,
| f ( x 1 , x 2 ,..., xi − 1 , xi , xi + 1 ,..., xn )− f ( x 1 , x 2 ,..., xi − 1 , yi , xi + 1 ,..., xn )|≤ c.
We set
Z 0 = E [ f ( X 1 , X 2 ,..., Xn )]
and
Zi = E [ f ( X 1 , X 2 ,..., Xn )| X 1 , X 2 ,..., Xi ].
Give an example to show that, if the Xi are not independent, then it is possible that
| Zi − Zi − 1 |> c.

chapter fourteen

Sample Complexity,

VC Dimension, and

Rademacher Complexity

Sampling is a powerful technique at the core of statistical data analysis and machine
learning. Using a finite, often small, set of observations, we attempt to estimate prop-
erties of an entire sample space. How good are estimates obtained from a sample? Any
rigorous application of sampling requires an understanding of the sample complexity
of the problem – the minimum size sample needed to obtain the required results. In this
chapter we focus on the sample complexity of two important applications of sampling:
range detection and probability estimation. Here a range is just a subset of the underly-
ing space. Our goal is to use one set of samples to detect a set of ranges or estimate the
probabilities of a set of ranges, where the set of possible ranges is large, in fact possibly
infinite. For detection, we mean that we want the sample to intersect with each range
in the set, while for probability estimation, we want the fraction of points in the sample
that intersect with each range in the set to approximate the probability associated with
that range.
As an example, consider a sample x 1 ,..., xm of m independent observations from
an unknown distributionD, where the values for our samples are inR. Given an
interval [ a , b ], if the probability of the interval is at least /theta, i.e., Pr( x ∈[ a , b ])≥ /theta,
then the probability that a sample of size m =^1  /thetaln^1 δ intersects (or, in this context,
detects) the interval [ a , b ] is at least 1−(1− /theta) m ≥ 1 −δ. Given a set of k intervals,
each of which has probability at least /theta, we can apply a union bound to show that the
probability that a sample of size m ′=^1  /thetaln k δintersects each of the k intervals is at least
1 − k (1− /theta) m ′≥ 1 −δ.
In many applications we need a sample that intersects with every interval that has
probability at least /theta, and there can be an infinite number of such intervals. What sample
size guarantees that? We cannot use a simple union bound to answer this question, as
our above analysis does not make sense when k is infinite. However, if there are many
such intervals, there can be significant overlap between them. For example, consider
samples chosen uniformly over [0,1] with /theta= 1 /10; there are infinitely many intervals
[ a , b ] of length at least 1/10, but the largest number of disjoint intervals of size at least
1 /10 is ten. A sample point may intersect with many intervals, and thus a small sample
may be sufficient.

sample complexity, vc dimension, and rademacher complexity
Indeed, the technique we will develop in this chapter will show that for any distribu-
tionD, a sample of size m(^1  /thetaln^1 δ), with probability at least 1−δ, intersects all intervals
of probability at least /theta. Similarly, we will show that a sample of size m( /theta^12 ln^1 δ), with
probability at least 1−δ, simultaneously estimates the probabilities of all intervals,
where each probability is estimated within an additive error bounded by /theta.
The above example shows that the set of intervals on a line corresponds to a set of
ranges that is easy to sample. In this chapter we develop general methods for evaluating
the sample complexity of sets of ranges. We will see an example of sets of ranges with
significantly larger sample complexity than the intervals example, and even sets of
ranges with infinite sample complexity for either detection or probability estimation.
We also present applications of the theory to rigorous machine learning and data mining
analysis.

14.1 The Learning Setting
The study of sample complexity was motivated by statistical machine learning. To moti-
vate our discussion of these concepts, we show how the task of learning a binary clas-
sification can be framed as either a detection or a probability estimation problem.
As a starting example, suppose that we know that a publisher uses a certain rule when
determining whether to review or reject a book based on the submitted manuscript. The
rule is a conjunction over certain Boolean variables (or their negations); for example,
there could be a Boolean variable for whether the manuscript is over 100 pages, for
whether the topic was of wide interest, for whether the author had suitable experience,
and so on. As outsiders, we might not know the rule, and the question is whether we
can learn the rule after seeing enough examples.
A second example involves learning the range of temperatures in which some elec-
tronic equipment is functioning correctly. We test the equipment at various tempera-
tures: some are too low and some are too high, but in between there is an interval of
temperatures in which the equipment is functioning correctly. The question is to deter-
mine an appropriate range of temperatures where the equipment functions.
Here is a general model for this sort of problem; we formalize these definitions later.
We have a universe U of objects that we wish to classify, and let c : U →{− 1 , 1 }be the
correct, unknown classification. Usually c ( x )=1 corresponds to x being a “positive”
example, and c ( x )=−1 corresponds to x being a “negative” example. The correct
classification also can be thought of as the subset of the universe corresponding to the
positive examples.
The learning algorithm receives a training set ( x 1 , c ( x 1 )),...,( xm , c ( xm )), where
xi ∈ U is chosen according to an unknown distributionD, and c ( xi ) is the correct clas-
sification of xi. The algorithm also receives a collectionCof hypotheses, or possi-
ble classifications, to choose from. This collection of hypotheses can be referred to as
the concept class. The output of the algorithm is a classification h ∈C. In the context
of binary classification, every h ∈Cis also a function h : U →{− 1 , 1 }. Equivalently,
each hypothesis is itself a subset of the universe, corresponding to the elements x with

14.2 VC Dimension
h ( x )=1. The correctness of the chosen classification is evaluated with respect to its
error in classifying new objects chosen according to the distributionD.
In our first example,Cis the collection of all possible conjunctions of subsets of
the Boolean variables or their negations. That is, each h ∈Ccorresponds to a Boolean
formula given by a conjunction of variables; h ( x ) is 1 if the Boolean expression evalu-
ates to true on x , and−1 if it evaluates to false. In the second example,Cis the set of
all intervals inR, so that for each h ∈C, h ( x )=1if x is a point in the corresponding
interval and h ( x )=−1 otherwise.
Assume first that the correct classification c is included in the collectionCof possible
classifications. For any other h ∈Clet

( c , h )={ x ∈ U | c ( x )= h ( x )}
be the set of objects that are not classified correctly by classification h. The proba-
bility of a set( c , h ) is the probability that the distributionDgenerates an object
in( c , h ). If our training set intersects with every set( c , h ) that has probability at
least /theta, then the learning algorithm can eliminate any classification h ∈Cthat has error
at least /thetaon input fromD. Thus, a sample (training set) that with probability 1−δ
detects (or intersects with) all sets{( c , h )|PrD(( c , h ))≥ /theta, h ∈ C }guarantees that
such an algorithm outputs with probability 1−δa classification that errs with proba-
bility bounded by /theta.
A more realistic scenario is that no classification inCis perfectly correct. In that case,
we require the algorithm to return a classification inCwith an error probability that is
no more than /thetalarger (with respect toD) than any classification inC. If our training set
approximates all sets{( c , h )| h ∈C}to within an additive error /theta/2, then the learning
algorithm has sufficient information to eliminate any h ∈Cwith error which is at least
 /thetalarger than the error of the best hypothesis inC.
Finally, we note a major difference between the two examples above. Since the num-
ber of possible conjunctions over a bounded number of variables or their complements
is bounded, the set of possible classifications in the first example is finite, and we can
use standard techniques (union bound and Chernoff bound) to bound the size of the
required sample (training set), though the bound may be loose. In the second example,
the size of the concept class is not bounded and we need more advanced techniques
to obtain a bound on the sample complexity. We present here two major techniques to
evaluate the sample complexity, VC dimension and Rademacher complexity.

14.2. VC Dimension
We begin with the formal definitions, using the setting of intervals on a line to help
explain them, and then consider other examples.
The Vapnik–Chervonenkis (VC) dimension is defined on range spaces.

Definition 14.1: A range space is a pair ( X ,R) where:

1. X is a (finite or infinite) set of points ;
2. R is a family of subsets of X, called ranges_._

sample complexity, vc dimension, and rademacher complexity
24 6
Figure 14.1: LetRbe the collection of all closed intervals inR. Any 2 points can be shattered, but
there is no interval that separates{ 2 , 6 }from{ 4 }. The VC dimension of (R,R) is therefore 2.

If for example X =Ris the set of real numbers, thenRcould be the family of all
closed intervals [ a , b ]inR.
Given a set S ⊆ X , one can obtain a subset of S by intersecting it with a range R ∈R.
The projection ofRon S corresponds to the collection of all subsets that can be obtained
in this way.

Definition 14.2: Let ( X ,R) be a range space and let S ⊆ X.The projection of R on
Sis

R S ={ R ∩ S | R ∈R}.
For example, let X =RandRbe the set of all closed intervals. Consider S ={ 2 , 4 }.
The intersection of S with the interval [0,1] gives the empty set; the intersection of S
with the interval [1,3] is{ 2 }; the intersection of S with the interval [3,5] is{ 4 }; and
the intersection of S with the interval [1,5] is{ 2 , 4 }. Hence the projection ofRon S is
the set of all possible subsets of S in this case, and indeed the same is true for any set
of two distinct points.
Consider now a set S ={ 2 , 4 , 6 }. You should convince yourself that the projection
ofRon S includes seven of the eight subsets of S , but not{ 2 , 6 }. This is because an
interval containing 2 and 6 must also contain 4. More generally, the projection ofRon
any set S of three distinct points would contain only seven of the eight possible subsets
of S.
We measure the complexity of a range space ( X ,R) by considering the largest subset
S of X such that all subsets of S are contained in the projection ofRon S.

Definition 14.3: Let ( X ,R) be a range space. A set S ⊆ Xis shattered by R if
|R S |= 2 | S | .The Vapnik–Chervonenkis (VC) dimension of a range space ( X ,R) is
the maximum cardinality of a set S ⊆ X that is shattered by R_. If there are arbitrarily
large finite sets that are shattered by_ R , then the VC dimension is infinite.

We have shown that any set of two points is shattered by closed intervals on the real
number line, but that any set of three points is not. Of course, that argument also shows
that no larger set of points is shattered by closed intervals. Therefore, the VC dimension
of that range space is 2. Our example shows that a range space with an infinite set
of points and an infinite number of ranges can have a bounded VC dimension. (See
Figure14.1.)
An important subtlety in the definition is that the VC dimension of a range space is d
if there is some set of cardinality d that is shattered byR. It does not imply that all sets
of cardinality d are shattered byR. On the other hand, to show that the VC dimension

14.2vc dimension
Figure 14.2: LetRbe the collection of all half-space partitions onR^2. Any three points can be
shattered, but there is no half-space partition that separates the two white points from the two black
points. Thus, the VC dimension of (R^2 ,R)is3.

is not d +1 or larger, one must show that all sets of cardinality larger than d are not
shattered byR.

14.2.1 Additional Examples of VC Dimension
We consider some other simple examples of VC dimension.

Linear half-spaces
Let X =R^2 and letRbe the set of all half-spaces defined by a linear partition of the
plane. That is, we consider all possible lines ax + by = c in the plane, andRconsists
of all half-spaces ax + by ≥ c. The VC dimenstion in this case is at least 3, since any
set of three points that do not lie on a line can be shattered. On the other hand, no set
of four points can be shattered. To see this, we need to consider several cases. First, if
any three points lie on a line they cannot be shattered, as we cannot separate the middle
point from the other two by any half-space. Hence we may assume no three points lie
on a line; this is often referred to as the points being in “general position”. Second, if
one point lies within the convex hull defined by the other three points, no half-space
can separate that point from the other three. Finally, if the four points define a convex
hull, then there is no half-space that separates two non-neighboring points from the
other two. (See Figure14.2.)
While harder to visualize, if X =R d andRcorresponds to all half-spaces in d
dimensions, the VC dimension is d +1. (See Exercise14.7.)

Convex sets
Let X =R^2 and letRbe the family of all closed convex sets on the plane. We show that
this range space has infinite VC dimension by showing that for every n there exists a set
of size n that can be shattered. Let Sn ={ x 1 ,..., xn }be a set of n points on the boundary
of a circle. Any subset Y ⊆ Sn , Y =∅defines a convex set that does not include any
point in Sn \ Y , and hence Y is included in the projection ofRon Sn. The empty set is
easily seen to be in the projection as well. Hence, for any number of points n , the set
Sn is shattered and the VC dimension is therefore infinite. (See Figure14.3.)

Monotone Boolean conjunctions
Let y 1 , y 2 ,..., yn be n Boolean variables, and let MCn be the collection of functions
defined by conjunctions of subsets of the non-negated variables yi. Let X ={ 0 , 1 } n

sample complexity, vc dimension, and rademacher complexity
Figure 14.3: LetRbe the set of all convex bodies inR^2. Any partition of the set of points on the
circle can be defined by a convex body. Therefore, the VC dimension of (R^2 ,R) is infinite.

correspond to all possible truth assignments of the n variables in the natural way. For
each function f ∈ MCn let Rf ={ a ̄∈ X : f ( ̄ a )= 1 }be the set of inputs that satisfy
f , and letR={ Rf | f ∈ MCn }. Consider the set S ⊆ X of n points:

(0, 1 , 1 ,...,1)
(1, 0 , 1 ,...,1)
(1, 1 , 0 ,...,1)
(1, 1 , 1 ,...,0).
We claim that each subset of S is equal to S ∩ Rf for some Rf. For example, the com-
plete set S corresponds to S ∩ Rf for the trivial function that is always 1, i.e., f ( ̄ a )=1.
More generally, the subset of S that has all points except those with a 0 in coordinates
i 1 , i 2 ,..., ij is equal to S ∩ Rf for f ( ̄ a )= yi 1 ∧ yi 2 ∧···∧ yij. This set can therefore be
shattered byRand the VC dimension is at least n. The VC dimension cannot be larger
than n since|R|=| MCn |= 2 n , and hence there can be at most 2 n distinct intersec-
tions of the form S ∩ Rf. If the VC dimension was larger than n , at least 2 n +^1 different
intersections would be needed.

14.2.2 Growth Function
The combinatorial significance of the concept of the VC dimension is that it gives
a bound on the number of different ranges in the projection of the range space on a
smaller set of points. In particular, when a range space with finite VC dimension d ≥ 2
is projected on a set of n points, the number of different ranges in the projection is
bounded by a polynomial in n with maximum degree d.
To prove this property we define the growth function

G( d , n )=
∑ d
i = 0
(
n
i
)
.
14.2vc dimension
For n = d ,wehaveG( d , n )= 2 d , and for n > d ≥2, we have

G( d , n )≤
∑ d
i = 0
ni
i!
≤ nd.
The growth function is related to the VC dimension through the following theorem.

Theorem 14.1 [ Sauer–Shelah ] : Let ( X ,R) be a range space with | X |= n and VC
dimension d. Then |R|≤G( d , n ).

Proof: We prove the claim by induction on d , and for each d by induction on n .As
the base case, the claim clearly holds for d =0or n =0, as in both of these cases
G ( d , n )=1, with the only possibleRbeing the family containing only the empty set.
Assume that the claim holds for d −1 and n −1, and for d and n −1. We may
therefore assume| X |= n >0. For some x ∈ X , consider two range spaces on X { x }:

R 1 ={ R \{ x }| R ∈R}
and

R 2 ={ R \{ x }| R ∪{ x }∈Rand R \{ x }∈R}.
We first observe that|R|=|R 1 |+|R 2 |. Indeed, each set R ∈Ris mapped to a set
R { x }∈R 1 , but if both R ∪{ x }and R { x }are inR, then both sets are mapped to the
same set R { x }∈R 1. By including that set again inR 2 ,wehave|R|=|R 1 |+|R 2 |.
Now ( X { x },R 1 ) is a range space on n −1 items, and its VC dimension is bounded
above by d , the VC dimension of ( X ,R). To see this, assume thatR 1 shatters a set S
of size d +1in X { x }. Then S is also shattered byR, as for any R ∈R 1 , there is a
corresponding R ′inRthat is either R or R ∪{ x }, and in either case the projection of
Ron S contains S ∩ R ′= S ∩ R. But thenRwould shatter the set S , contradicting the
assumption that ( X ,R) has VC dimension d.
Similarly, ( X { x },R 2 ) is a range space on n −1 items, and its VC dimension is
bounded above by d −1. To see this, assume thatR 2 shatters a set S of size d in X { x }.
Then consider the set S ∪{ x }inR. For any R ∈R 2 , both R and R ∪{ x }are inR, and
hence one can obtain both ( S ∪{ x })∩ R = S ∩ R and ( S ∪{ x })∩( R ∪{ x })= S ∪{ x }
in the projection ofRon S. But thenRwould shatter the set S ∪{ x }, contradicting the
assumption that ( X ,R) has VC dimension d.
Applying the induction hypothesis we get

|R|=|R 1 |+|R 2 |≤G( d , n −1)+G( d − 1 , n −1)
≤
∑ d
i = 0
(
n − 1
i
)
+
d ∑− 1
i = 0
(
n − 1
i
)
= 1 +
d ∑− 1
i = 0
((
n − 1
i + 1
)
+
(
n − 1
i
))
=
∑ d
i = 0
(
n
i
)
=G( d , n ).  /theta
sample complexity, vc dimension, and rademacher complexity
14.2.3 VC dimension component bounds
We can sometimes bound the VC dimension of a complex range space as a function of
the VC dimension of its simpler components.
The projection of a range space ( X ,R) on a set Y ⊆ X defines a range space ( Y ,R Y )
withR Y ={ R ∩ Y | R ∈R}.We have the following corollary of Theorem14.1.

Corollary 14.2: Let ( X ,R) be a range space with VC dimension d, and let Y ⊆ X.
Then

|R Y |≤G( d ,| Y |).
We also require the following technical lemma.
Lemma 14.3: If y ≥ x ln x ≥e , then ln^2 yy ≥ x.

Proof: For y = x ln x we have ln y =ln x +ln ln x ≤2ln x. Thus

2 y
ln y
≥
2 x ln x
2ln x
= x.
Differentiating f ( y )=ln 2 yy we find that f ( y ) is monotonically decreasing when y ≥

x ln x ≥e, and henceln^2 yy is monotonically increasing on the same interval, proving the
lemma.  /theta

We are now ready for the following theorem.
Theorem 14.4: Let ( X ,R^1 ),...,( X ,R k ) be k range spaces, each with VC dimen-
sion at most d. Let f :(R^1 ,...,R k )→ 2 Xbe a mapping of k-tuples ( r 1 ,..., rk )∈
(R^1 ,...,R k ) to subsets of X, and let

R f ={ f ( r 1 ,..., rk )| r 1 ∈R^1 ,..., rk ∈R k }.
The VC dimension of the range space ( X ,R f ) is O ( kd ln( kd )).

Proof: Let the VC dimension of ( X ,R f ) be at least t , so there is a set Y ⊆ X shattered
byR f with t =| Y |. Since the VC dimension of ( X ,R i ), 1≤ i ≤ k , is at most d ,by
Corollary14.2,|R iY |≤G( d , t )≤ td. Thus, the number of subsets in the projection of
R f on Y is bounded by

|R Yf |≤|R Y^1 |×···×|R kY |≤ tdk.
SinceR Yf shatters Y ,|R Yf |≥ 2 t. Hence tdk ≥ 2 t. Let us assume that y ≥ x ln x for
y = t and x =2( dk ln 2+1)and derive a contradiction. Applying Lemma14.3,

2 y
ln y
=
2 t
ln t
≥
2( dk +1)
ln 2
.
It follows that

t ≥( dk +1) log 2 t ,
so 2 t ≥ tdk +^1 > tkd. Hence if t ≥ x ln x , which is m( dk ln( dk )), we have a contradiction.
It follows that t must be O ( kd ln( kd )).  /theta

14.2vc dimension
The following stronger result below is proven in Exercise14.10.
Theorem 14.5: Let ( X ,R^1 ),...,( X ,R k ) be k range spaces each with VC dimen-
sions at most d. Let f :(R^1 ,...,R k )→ 2 Xbe a mapping of k-tuples ( r 1 ,..., rk )∈
(R^1 ,...,R k ) to subsets of X, and let

R f ={ f ( r 1 ,..., rk )| r 1 ∈R^1 ,..., rk ∈R k }.
The VC dimension of the range space ( X ,R f ) is O ( kd ln k ).

This yields the following corollary.
Corollary 14.6: Let ( X ,R^1 ) and ( X ,R^2 ) be two range spaces, each with VC dimen-
sion at most d. Let

R∪={ r 1 ∪ r 2 | r 1 ∈R^1 and r 2 ∈R^2 },
and

R∩={ r 1 ∩ r 2 | r 1 ∈R^1 and r 2 ∈R^2 }.
The VC dimensions of the range spaces ( X ,R∪) and ( X ,R∩) are O ( d ).

14.2.4.  /eta -nets and  /eta -samples
The applications of VC dimension to sampling, including to the types of learning prob-
lems mentioned at the beginning of the chapter, can be formulated in terms of objects
called /theta-nets and /theta-samples.
As a combinatorial object, an /theta-net for a subset A ⊆ X of a range space is a subset
N ⊆ A of points that intersects with all ranges in the range space that are not too small
with respect to A , in that the range contains an /theta-fraction of A. The object is called a
net because it “catches,” or intersects, every range of sufficient size.

Definition 14.4 [ combinatorial definition ] : Let ( X ,R) be a range space, and let
A ⊆ X be a finite subset of X. A set N ⊆ Aisa combinatorial /theta-net for A if N has a
nonempty intersection with every set R ∈R such that | R ∩ A |≥ /theta| A |.

However, /theta-nets can also be defined more generally with respect to a distributionD
on the point set X. The combinatorial definition above corresponds to a setting where
the distributionDis uniform over the set A. The more general form below is more
useful for many algorithmic applications. In what follows, recall that PrD( R ) for a set
R is the probability that a point chosen according toDis in R.

Definition 14.5: Let ( X ,R) be a range space, and let D be a probability distribution
on X. A set N ⊆ Xisan  /theta-net for X with respect to D if for any set R ∈R such that
PrD( R )≥ /theta , the set R contains at least one point from N, i.e.,

∀ R ∈R, PrD( R )≥ /theta⇒ R ∩ N =∅.
An /theta-sample (also called an /theta-approximation) provides even stronger guarantees
than an /theta-net. It not only intersects every suitably large range, but also ensures that
every range has roughly the right relative frequency within the sample.

sample complexity, vc dimension, and rademacher complexity
Definition 14.6: Let ( X ,R) be a range space, and let D be a probability distribution
on X. A set S ⊆ Xisan  /theta-sample for X with respect to D if for all sets R ∈R ,
∣∣
∣
∣PrD( R )−

| S ∩ R |
| S |
∣∣
∣
∣≤ /theta.
Again, by fixing the distributionDto be uniform over a finite set A ⊆ X , we obtain
the combinatorial version of this concept.

Definition 14.7 [ combinatorial definition ] : Let ( X ,R) be a range space, and let
A ⊆ X be a finite subset of X. A set N ⊆ Aisa combinatorial /theta-sample for A if for all
sets R ∈R ,
∣
∣∣
∣

| A ∩ R |
| A |
−
| N ∩ R |
| N |
∣
∣∣
∣≤ /theta.
In what follows, we may say /theta-net and /theta-sample in place of the more exact terms
combinatorial /theta-net and combinatorial /theta-sample when the meaning should be clear
from context.
Our goal is to obtain /theta-nets and /theta-samples through sampling. We say that a set S is a
sample of size m from a distributionDif the m elements of S were chosen independently
with distributionD.

Definition 14.8: A range space ( X ,R) has the uniform convergence property if for
every  /theta, δ > 0 there is a sample size m = m ( /theta, δ) such that for every distribution D
over X, if S is a random sample from D of size m then, with probability at least 1 −δ ,
Sisan  /theta -sample for X with respect to D_._

In the following sections we show that the minimum sample size that contains an
 /theta-net or an /theta-sample for a range space can be bounded in terms of the VC dimension
of the range space, independent of the numbers of its points or ranges. In particular,
we will show that a range space has the uniform convergence property if and only if its
VC dimension is finite. These results show that the VC dimension is a concrete, useful
measure of the complexity of a range space.

14.3. The  /eta -net Theorem
As a first step, we use a standard union bound argument to obtain bounds on the size
of a combinatorial /theta-net via the probabilistic method.

Theorem 14.7: Let ( X ,R) be a range space with VC dimension d ≥ 2 and let A ⊆ X
havesize | A |= n.Thenthereexistsacombinatorial  /theta -netN forAofsizeatmost  d ln /theta n .

Proof: Consider the projection of the range spaceRon A ; denote this byR′. By The-
orem14.1, the size ofR′is at mostG( d , n )≤ nd.
Suppose we take a sample of k = d ln /theta n points of A independently and uniformly
at random. For each set R ∈Rsuch that| R ∩ A |≥ /theta| A |, there is a corresponding set
R ′∈R′. The probability that our sample misses a given set R ′is (1− /theta) k , and there are

14.3 the  /eta -net theorem
at most nd possible sets R ′to consider. Applying a union bound, the probability that
the sample misses at least one such R ′is at most

nd (1− /theta) k < nd e− d ln n = 1.
Since the probability that a random sample of size k = d ln /theta n misses at least one set R ′
is strictly less than 1, by the probabilistic method there is a set of that size that misses
no set R ′∈R′, and is therefore an /theta-net for A.  /theta

We can, however, in general do much better than the bound of Theorem14.7.Our
goal is to show that with high probability we can obtain an /theta-net from a random sample
of elements where the size of the sample does not depend on n , aslongastheVCdimen-
sion is finite. This may appear somewhat surprising; while O (1/ /theta) points on average
are needed to hit any particular range, it is not clear how to hit all of them without some
dependence on n. Essentially, we are finding that the union bound of Theorem14.7is
too weak an approach in this setting, and that the VC dimension provides a means to
avoid it.
The following theorem, whose proof takes a somewhat unusual path that we some-
times refer to as “double sampling”, provides our main results on /theta-nets. The theorem
holds for our more general notion of /theta-nets, not just combinatorial /theta-nets.

Theorem 14.8: Let ( X ,R) be a range space with VC dimension d and let D be a
probability distribution on X. For any 0 <δ, /theta≤ 1 / 2 , there is an

m = O
(
d
 /theta
ln
d
 /theta
+
1
 /theta
ln
1
δ
)
such that a random sample from D of size greater than or equal to m is an  /theta -net for X
with probability at least 1 −δ.

In particular, Theorem14.8implies that there exists an /theta-net of size O ( d  /thetaln d  /theta).
Proof: Let M be a set of m independent samples from X according toD, and let E 1 be
the event that M is not an /theta-net for X with respect to the distributionD, i.e.,

E 1 ={∃ R ∈R|PrD( R )≥ /thetaand| R ∩ M |= 0 }.
We want to show that Pr( E 1 )≤δfor a suitable m. Notice that for any particular R ,
since PrD( R )≥ /theta, the expected size of| R ∩ M |would be at least /theta m , and hence it seems
natural that Pr( E 1 ) is small. However, as the union bound argument of Theorem14.7
is too weak to provide this strong a bound, we use an indirect means to bound Pr( E 1 ).
To do this, we choose a second set T of m independent samples from X according
toDand define E 2 to be the event that some range R with PrD( R )≥ /thetahas an empty
intersection with M but a reasonably large intersection with T :

E 2 ={∃ R ∈R|PrD( R )≥ /thetaand| R ∩ M |=0 and| R ∩ T |≥ /theta m / 2 }.
Since T is a random sample and PrD( R )≥ /theta, the event| R ∩ T |≥ /theta m /2 should occur
with nontrivial probability and therefore the events E 1 and E 2 should have similar prob-
ability. The following lemma formalizes this intuition:

sample complexity, vc dimension, and rademacher complexity
Lemma 14.9: For m ≥ 8 / /theta ,

Pr( E 2 )≤Pr( E 1 )≤2Pr( E 2 ).
Proof: As the event E 2 is included in the event E 1 ,wehavePr( E 2 )≤Pr( E 1 ). For
the second inequality, note that if event E 1 holds, there is some particular R ′so that
| R ′∩ M |=0 and PrD( R ′)≥ /theta. We use the definition of conditional probability to
obtain

Pr( E 2 )
Pr( E 1 )
=
Pr( E 1 ∩ E 2 )
Pr( E 1 )
=Pr( E 2 | E 1 )≥Pr(| T ∩ R ′|≥ /theta m /2).
Now for a fixed range R ′and a random sample T the random variable| T ∩ R ′|has
a binomial distribution B ( m ,PrD( R ′)). Since PrD( R ′)≥ /theta, by applying the Chernoff
bound (Theorem4.5), we have for m ≥ 8 / /theta,

Pr(| T ∩ R ′|< /theta m /2)≤e− /theta m /^8 < 1 / 2.
Thus,

Pr( E 2 )
Pr( E 1 )
=Pr( E 2 | E 1 )≥Pr(| T ∩ R ′|≥ /theta m /2)≥ 1 / 2 ,
giving Pr( E 1 )≤2Pr( E 2 ) as desired.  /theta

The lemma above gives us an approach to showing that Pr( E 1 ) is small. The intuition
is as follows: since M and T are both random samples of size m , it would be very
surprising to have| M ∩ R |=0but| T ∩ R |be large for some R. If we think of first
sampling the m items that form M and then sampling the m items that form T , we must
have somehow been very unlucky to have all the samples that intersect R come in the
second set of m samples, and none in the first.
Formally, we bound the probability of E 2 by the probability of a larger event E 2 ′:

E 2 ′={∃ R ∈R|| R ∩ M |=0 and| R ∩ T |≥ /theta m / 2 }.
The event E 2 ′excludes the condition that PrD( R )≥ /theta; in some sense, that has been
replaced by the condition on the size of| R ∩ T |. The event E 2 ′now depends only on
the elements in M ∪ T.

Lemma 14.10: It holds that

Pr( E 1 )≤2Pr( E 2 )≤2Pr( E 2 ′)≤2(2 m ) d 2 − /theta m /^2.
Proof: Since M and T are random samples, we can assume that we first choose a set
of 2 m elements and then partition it randomly into two equal size sets M and T.
For a fixed R ∈Rand k = /theta m /2, let

ER ={| R ∩ M |=0 and| R ∩ T |≥ k }.
To bound the probability of ER we note that this event implies that M ∪ T has at least k
elements of R , but all these elements were placed in T by the random partition. That is,

14.3 the  /eta -net theorem
of the

( 2 m
m
)
possible partitions of M ∪ T , we chose one of the
( 2 m − k
m
)
partitions where
no element of R is in M.
Hence

Pr( ER )≤Pr(| M ∩ R |= 0 || R ∩( M ∪ T )|≥ k )
=
( 2 m − k
m
)
( 2 m
m
)
=
(2 m − k )! m!
(2 m )!( m − k )!
=
m ( m −1)···( m − k +1)
(2 m )(2 m −1)···(2 m − k +1)
≤ 2 − /theta m /^2.
Our bound on Pr( ER ) does not depend on the choice of the set T ∪ M , only on its
random partition into T and M. By Theorem14.1the projection ofRon M ∪ T has no
more than (2 m ) d ranges. Thus,

Pr( E 2 ′)≤(2 m ) d 2 − /theta m /^2.  /theta
To complete the proof of Theorem14.8we show that for
m ≥
8 d
 /theta
ln
16 d
 /theta
+
4
 /theta
ln
2
δ
,
we have

Pr( E 1 )≤2Pr( E 2 ′)≤2(2 m ) d 2 − /theta m /^2 ≤δ.
Equivalently, we require

 /theta m / 2 ≥ln(2/δ)+ d ln(2 m ).
Clearly it holds that /theta m / 4 ≥ln(2/δ), since m >^4  /thetaln^2 δ.It therefore suffices to show
that /theta m / 4 ≥ d ln(2 m ) to complete the proof.
Applying Lemma14.3with y = 2 m ≥^16  /theta d ln^16  /theta d and x =^16  /theta d ,wehave

4 m
ln(2 m )
≥
16 d
 /theta
,
so
 /theta m
4

≥ d ln(2 m )
as required.  /theta

The above theorem gives a near tight bound, as shown by the following theorem (see
Exercise14.13for a proof).

Theorem 14.11: A random sample of a range space with VC dimension d that, with
probability at least 1 −δ ,isan  /theta -net must have size  m( d  /theta).

sample complexity, vc dimension, and rademacher complexity
14.4 Application: PAC Learning
Probably Approximately Correct (PAC) Learning provides a framework for mathemat-
ical analysis of computational learning from examples. PAC characterizes the complex-
ity of a learning problem in terms of the number of examples and computation needed to
provide answers that are approximately correct, in that they are approximately correct
with good probability, on as yet unseen examples. We use the model of PAC learning to
demonstrate an application of VC dimension to learning theory. However, we note that
the VC dimension technique applies to a broader setting of statistical machine learning.
We turn now to a formal definition of PAC learning. We assume a set of items X
and a probability distributionDdefined on X. We work here in the setting of binary
classifications, where a concept (or classification ) can be treated as a subset C ⊆ X ; all
items in C are said to have a positive classification and all items in X \ C are said to
have a negative classification. Equivalently, a classification can be treated as a function
c ( x ) that is 1 if x ∈ C and−1if x ∈/ C. We use both notions of a classification inter-
changeably, where the meaning is clear. The concept class Cis the set of all possible
classifications defined by the problem.
The learning algorithm calls a function Oracle that produces a pair ( x , c ( x )), where
x is distributed according toD, and c ( x )is1if x ∈ C and−1 otherwise. We assume that
successive calls to Oracle are independent. For clarity, we may write Oracle( C ,D)
to specify the concept and distribution under consideration. We also assume that the
classification problem is realizable , i.e. there is a classification h ∈Cthat conforms
with our input distribution. Formally,

∃ h ∈Csuch that PrD( h ( x )= c ( x ))= 0.
We now define what it means for a concept to be learnable.
Definition 14.9 [ PAC Learning ] : Aconceptclass C overinputsetX is PAC learnable^1
if there is an algorithm L, with access to a function Oracle( C ,D) , that satisfies the
following properties: for every correct concept C ∈C , every distribution D on X, and
every 0 < /theta,δ≤ 1 / 2 , the number of calls that the algorithm L makes to the function
Oracle( C ,D) is polynomial in  /theta−^1 and δ−^1 , and with probability at least 1 −δ the
algorithm L outputs a hypothesis h such that PrD( h ( x )= c ( x ))≤ /theta.

We first prove that any finite concept class is PAC learnable.
Theorem 14.12: Any finite concept class C can be PAC learned with m =
1
 /theta(ln|C|+ln

1
δ) samples.
Proof: Let c ∗∈Cbe the correct classification. A hypothesis h is said to be “bad” if
PrD( h ( x )= c ∗( x ))≥ /theta. The probability that any particular bad hypothesis is consistent

(^1) PAC learning is mainly concerned with the computational complexity of learning. In particular, a concept classC
is efficiently PAC learnable if the algorithm runs in time polynomial in the size of the problem, 1/ /thetaand 1/δ.Such
an algorithm uses at most polynomially many samples. Here we are only interested in the sample complexity
of the learning process; however, we note that the computational complexity of the learning algorithm is not
necessarily polynomial in the sample size.

14.4application: pac learning
with m random samples is bounded above by (1− /theta) m , and hence the probability that
any bad hypothesis is consistent with m random samples is bounded above by

|C|(1− /theta) m ≤δ.
The result follows.  /theta

We can also apply the PAC learning framework to infinite concept classes. Let us
consider learning an interval [ a , b ]∈R. The concept class here is the collection of all
closed intervals inR:

C={[ x , y ]| x ≤ y }∪∅.
Notice that we also include a trivial concept that corresponds to the empty interval.
Let c ∗∈Cbe the concept to be learned, and h be the hypothesis returned by our
algorithm. The training set is a collection of n points drawn from a distributionDon
R, where each point in the interval [ a , b ] is a positive example and each point outside
the interval is a negative example. If none of the sample points are positive examples,
then our algorithm returns the trivial hypothesis, where h ( x )=−1 everywhere. If any
of the sample points are positive examples, then let c and d respectively be the smallest
and largest values of positive examples. Our algorithm then returns the interval [ c , d ]
as its hypothesis. (If there is only one positive example, the algorithm will return an
interval of the form [ c , c ].) By design, our algorithm can only make an error on an
input x if x ∈[ a , b ]; our algorithm will not make an error outside this interval, because
it always returns−1 for points x ∈/[ a , b ].
We now determine the probability that our algorithm returns a bad hypothesis. Let
us first consider the case where PrD( x ∈[ a , b ])≤ /theta. Because our algorithm can only
return an incorrect answer on points in the interval [ a , b ], our algorithm always returns
a hypothesis with a probability of error at most /thetain this case, and hence never returns
a bad hypothesis.
Now let us consider when PrD( x ∈[ a , b ])> /theta. In this case, let a ′≥ a be the small-
est value such that PrD([ a , a ′])≥ /theta/2. Similarly, let b ′≤ b be the largest value such
that PrD([ b ′, b ])≥ /theta/2. Here a ′≤ b ′since PrD( x ∈[ a , b ])> /theta. For convenience, we
assume a ′< b ′; the case a ′= b ′can be handled similarly. (If a ′= b ′, then the point
a ′has nonzero probability of being selected, and we can divide up that probability
among the intervals [ a , a ′] and [ b ′, b ] so the probability of each is at least /theta/2.) For our
algorithm to return a bad hypothesis with error at least /theta, it must be the case that no
sample points fell either in the interval [ a , a ′] or the interval [ b ′, b ], or both. Otherwise,
our algorithm would return a range [ c , d ] that covers [ a ′, b ′], and correspondingly the
probability our hypothesis would be incorrect on a new input chosen fromDwould be
at most /theta.
The probability that a training set of n points does not have any examples from either
[ a , a ′]or[ b , b ′] is bounded above by

2
(
1 −
 /theta
2
) n
≤2e− /theta n /^2.
Hence choosing n ≥2ln(2/δ)/ /thetasamples guarantees that the probability of choosing a
bad hypothesis is bounded above byδ, and therefore this concept class is PAC learnable.

sample complexity, vc dimension, and rademacher complexity
While the above example of learning intervals demonstrates an infinite concept class
that is PAC learnable, the approach to this problem of considering intervals around the
maximum and minimum sampled points appears ad hoc. The idea behind this approach,
however, can be generalized. Observe that a concept classCover input set X defines
a range space ( X ,C). We show that the number of examples required to PAC learn a
concept class is the same as the number of samples needed to construct an /theta-net for a
range space of VC dimension equal to the VC dimension of the range space defined by
the concept class.

Theorem 14.13: Let C be a concept class that defines a range space with VC dimen-
sion d. For any 0 <δ, /theta≤ 1 / 2 , there is an

m = O
(
d
 /theta
ln
d
 /theta
+
1
 /theta
ln
1
δ
)
such that C is PAC learnable with m samples.

Proof: Let X be the ground set of inputs and assume that c ∈Cis the correct classi-
fication. For any c ′∈C, c ′= c let( c ′, c )={ x | c ( x )= c ′( x )}, where c ( x ) and c ′( x )
are the labeling functions for c and c ′respectively. Let( c )={( c ′, c )| c ′∈C}.
That is,( c ) is a collection of all the possible sets of points of disagreement with the
correct classification. The symmetric difference range space with respect toCand c is
( X ,( c )).We prove the following lemma about the symmetric difference range space.

Lemma 14.14: The VC dimension of ( X ,( c )) is equal to the VC dimension of ( X ,C).

Proof: For any set S ⊆ X we define a bijection from the projection of ( X ,C)on S ,
denoted byC S , to the projection of ( X ,( c )) on S , denoted by( c ) S. The bijec-
tion maps each element c ′∩ S ∈C S to( c ′∩ S , c ∩ S )∈( c ) S. To show this is a
bijection, we first consider two elements c ′, c ′′∈Cwith c ′∩ S = c ′′∩ S , and show
that( c ′∩ S , c ∩ S )=( c ′′∩ S , c ∩ S ). If c ′∩ S = c ′′∩ S , then there is an element
y ∈ S such that c ′( y )= c ′′( y ). Without loss of generality, assume that c ′( y )= c ( y )but
c ′′( y )= c ( y ). In that case y ∈( c ′∩ S , c )∩ S but y ∈( c ′′∩ S , c ∩ S ). Similarly, if
for two elements c ′, c ′′∈Cthere is an element y ∈ S such that( c ′∩ S , c ∩ S )=
( c ′′∩ S , c ∩ S ), then there is an element y ∈ S such that c ′( y )= c ′′( y ), so c ′∩ S =
c ′′∩ S , proving the bijection.
Thus, for any S ⊆ X ,|C S |=|( c ) S |, and S is shattered byCif and only if it is
shattered by( c ). The two range spaces therefore have the same VC dimension.  /theta

Since the range space ( X ,( c )) has a VC dimension d , by Theorem14.8there is an
m = O
(
d
 /theta
ln
d
 /theta
+
1
 /theta
ln
1
δ
)
so that any sample of size m or larger is, with probability at least 1−δ,an /theta-net for
that range space, and therefore has a nonempty intersection with every set( c ′, c ) that
has probability at least /theta. Thus, with probability at least 1−δ, our training set allows
the algorithm to exclude any hypothesis with error probability at least /theta.  /theta

14.5the  /eta -sample theorem
We saw in Section14.2.1that the VC dimension of the collection of closed intervals
onRis 2. Applying Theorem14.13to the problem of learning an interval on the line
gives an alternative proof to the result we saw in Section14.4that this range space can
be learned with O (^1  /thetaln^1 δ) samples.

14.5. The  /eta -sample Theorem
Recall that an /theta-sample for a range space ( X ,R) maintains the relative probability
weight of all sets R ∈Rwithin a tolerance of /theta(Definition14.6), while an /theta-net just
includes at least one element from each range with total probability at least /theta. Surpris-
ingly, adding just another O (1/ /theta) factor to the sample size gives an /theta-sample, again
with probability at least 1−δ. The proof of this result uses the same “double sampling”
method as in the proof of the /theta-net theorem, albeit with a somewhat more complicated
argument.

Theorem 14.15: Let ( X ,R) be a range space with VC dimension d and let D be a
probability distribution on X. For any 0 < /theta,δ< 1 / 2 , there is an

m = O
(
d
 /theta^2
ln
d
 /theta
+
1
 /theta^2
ln
1
δ
)
such that a random sample from D of size greater than or equal to m is an  /theta -sample
for X with probability at least 1 −δ.

Proof: Let M be a set of m independent samples from X according toD, and let E 1 be
the event that M is not an /theta-sample for X with respect to the distributionD,i.e.

E 1 =
{
∃ R ∈R|
∣∣
∣
∣PrD( R )−
| M ∩ R |
| M |
∣∣
∣
∣> /theta
}
.
We want to show that Pr( E 1 )≤δfor a suitable m. We choose a second set T of m
independent samples from X according toD, and define E 2 to be the event that some
range R is not well approximated by M but is reasonably well approximated by T :

E 2 =
{
∃ R ∈R|
∣∣
∣∣| R ∩ M |
| M |
−PrD( R )
∣∣
∣∣> /thetaand
∣∣
∣∣| R ∩ T |
| T |
−PrD( R )
∣∣
∣∣≤ /theta
2
}
.
Lemma 14.16:

Pr( E 2 )≤Pr( E 1 )≤2Pr( E 2 ).
Proof: Clearly the event E 2 is included in the event E 1 , thus Pr( E 2 )≤Pr( E 1 ). For
the second inequality we again use conditional probability. If E 1 holds, there is some
particular R ′so that

∣∣| R ′∩ M |
| M | −PrD( R
′)∣∣> /theta. Therefore,
Pr( E 2 )
Pr( E 1 )
=
Pr( E 1 ∩ E 2 )
Pr( E 1 )
=Pr( E 2 | E 1 )≥Pr
(∣∣
∣∣| R
′∩ T |
| T |
−PrD( R ′)
∣∣
∣∣≤ /theta
2
)
.
Now for a fixed range R ′and a random sample T , the random variable| T ∩ R ′|has a
binomial distribution B ( m ,PrD( R ′)), and applying the Chernoff bound (Theorem4.5)

sample complexity, vc dimension, and rademacher complexity
we have

Pr(|| T ∩ R ′|− m PrD( R ′)|> /theta m /2)≤2e− /theta m /^12 < 1 / 2
for m ≥ 24 / /theta. We conclude

Pr( E 2 )
Pr( E 1 )
=Pr( E 2 | E 1 )≥Pr
(∣∣
∣∣| R
′∩ T |
| T |
−PrD( R ′)
∣∣
∣∣≤ /theta
2
)
≥ 1 / 2.  /theta
Next we bound the probability of E 2 by the probability of a larger event E 2 ′:

E 2 ′=
{
∃ R ∈R||| R ∩ T |−| R ∩ M || ≥
 /theta
2
m
}
.
To see that E 2 ⊆ E 2 ′, assume that a set R satisfies the conditions of E 2 ,i.e.

|| R ∩ M |− m PrD( R )|≥ /theta m ,
and

|| R ∩ T |− m PrD( R )|≤ /theta m / 2.
In that case

|| R ∩ M |− m PrD( R )|−|| R ∩ T |− m PrD( R )|≥ /theta m / 2 ,
and by the reverse triangle inequality^2

|| R ∩ T |−| R ∩ M || ≥ || R ∩ M |− m PrD( R )|−|| R ∩ T |− m PrD( R )|≥ /theta m / 2.
The event E 2 ′depends only on the elements in M ∪ T.

Lemma 14.17:

Pr( E 2 )≤Pr( E 2 ′)≤(2 m ) d e− /theta
(^2) m / 8
.
Proof: Since M and T are random samples, we can assume that we first choose a
random sample of 2 m elements Z = z 1 ,..., z 2 m and then partition it randomly into two
sets of size m each. Since Z is a random sample, any partition that is independent of the
actual values of the elements generates two random samples. We will use the following
partition: for each pair of sampled items z 2 i − 1 and z 2 i , i = 1 ,..., m , with probability
1 /2 (independent of other choices) we place z 2 i − 1 in T and z 2 i in M , otherwise we place
z 2 i − 1 in M and z 2 i in T.
For a fixed R ∈R, let ER be the event{|| R ∩ T |−| R ∩ M || ≥ /theta 2 m }.To bound the
probability of ER we consider the contribution of the assignment of each pair z 2 i − 1 , z 2 i
to the value of|| R ∩ T |−| R ∩ M ||. If the two items are both in R or the two items are
both not in R , the contribution of the pair is 0. If one item is in R and the other is not
in R then the contribution of the pair is 1 with probability 1/2 and−1 with probability
(^2) The reverse triangle inequality is simply| x − y |≥|| x |−| y ||, which follows easily from the triangle inequality.

14.5the  /eta -sample theorem
1 /2. There are no more than m such pairs, so from the Chernoff bound in Theorem4.7
we can conclude

Pr( ER )≤e− /theta
(^2) m / 8
.
By Theorem14.1the projection ofRon Z has no more than (2 m ) d ranges. Thus,
by the union bound we have
Pr( E 2 ′)≤(2 m ) d e− /theta
(^2) m / 8

.  /theta

To complete the proof of Theorem14.15we show that for
m ≥
32 d
 /theta^2
ln
64 d
 /theta^2
+
16
 /theta^2
ln
2
δ
we have

Pr( E 1 )≤2Pr( E 2 ′)≤2(2 m ) d e− /theta^2 m /^8 ≤δ.
We remark that this value of m satisfies

m = O
(
d
 /theta^2
ln
d
 /theta
+
1
 /theta^2
ln
1
δ
)
as given in the statement of the theorem; although our explicit bound has a ln^64  /theta 2 d , that
term is O (ln d  /theta). Equivalently, we require

 /theta^2 m / 8 ≥ln(2/δ)+ d ln(2 m ).
Clearly it holds that /theta^2 m / 16 ≥ln(2/δ), since m >^16  /theta 2 ln^2 δ.It therefore suffices to
show that /theta^2 m / 16 ≥ d ln(2 m ) to complete the proof.
Applying Lemma14.3with y = 2 m ≥^64  /theta 2 d ln^64  /theta 2 d and x =^64  /theta 2 d ,wehave
4 m
ln(2 m )

≥
64 d
 /theta^2
,
so

 /theta^2 m
16
≥ d ln(2 m )
as required.  /theta

Since an /theta-sample is also an /theta-net, the lower bound on the sample complexity of
 /theta-nets in Theorem14.11holds for /theta-samples. Together with the upper bound of Theo-
rem14.15, this gives:

Theorem 14.18: A range space has the uniform convergence property if and only if
its VC dimension is finite.

14.5.1 Application: Agnostic Learning
In our discussion of PAC learning in Section14.4, we assumed that the algorithm is
given a concept classCthat includes the correct classification c. That is, there is a

sample complexity, vc dimension, and rademacher complexity
classification that is correct on all items in X , and in particular conforms with all exam-
ples in the training set. This assumption does not hold in most applications. First, the
training set may have some errors. Second, we may not know any concept class that
is guaranteed to include the correct classification and is also simple to represent and
compute. In this section we extend our discussion of PAC learning to the case in which
the concept class does not necessarily include a perfectly correct classification, which
is referred to as the unrealizable case or agnostic learning. Since the concept class may
not have a correct or even close to correct classification, the goal of the the algorithm
in this case is to select a classification c ′∈Cwith an error that is no more than /thetalarger
than that of any other classification inC. Formally, let c be the correct classification
(which may not be inC). We require the output classification c ′to satisfy the following
inequality:

PrD( c ′( x )= c ( x ))≤inf h ∈CPrD( h ( x )= c ( x ))+ /theta.
Recall from Section14.4that the symmetric difference range space with respect to
the concept classCand the correct classification c is ( X ,( c )). If the examples in the
training set define an /theta/2-sample for that range space then the algorithm has sufficiently
many examples to estimate the error probability of each c ′∈Cto within an additive
error /theta/2, and thus can select a classification that satisfies the above requirement.^3
Applying Theorem14.15, agnostic learning of a concept class with VC dimension d
requires O

(
min
(
| X |, /theta d 2 ln /theta d 2 + /theta^12 ln^1 δ
))
samples.
Finally, we state a general characterization of concept classes that are agnostic PAC
learnable.

Theorem 14.19: The following three conditions are equivalent:

1. A concept class C over a domain X is agnostic PAC learnable.
2. The range space ( X ,C) has the uniform convergence property.
3. The range space ( X ,C) has a finite VC dimension.

14.5.2 Application: Data Mining
Data mining involves extracting useful information from raw data. In some cases, such
as anomaly detection, one is interested in rare events. Finding such rare events may
require a complete analysis of the entire data set that is expensive in both computational
time and memory requirements. In other cases, however, the goal of data mining is to
detect major patterns or trends in data and ignore random fluctuations. In such settings,
analyzing a properly selected sample of the data instead of the entire data set can give
an excellent approximation at a fraction of the cost. The crucial question here is how
large the sample should be to give a reliable estimate. We give here two examples where
using /theta-samples can provide an answer to this question.

(^3) Recall that we are only concerned here with the sampling complexity of the problem. Depending on the particular
concept class, the computation cost may not be practically feasible.

14.5the  /eta -sample theorem
Example: Estimating dense neighborhoods
Assume that we are given a large set of n points in the plane and we need to answer a
sequence of queries of the form “what fraction of the points are at distance at most r
from point ( x , y )?”, for arbitrary values of ( x , y ) and r. Estimates of this kind are used
by businesses to determine where to locate new stores or other resources. For example,
if points represent home locations for customers, query locations ( x , y ) might represent
possible locations for a bank to place an automated teller machine, in which case quick
estimates of how many customers are near the location would be useful for planning
purposes.
We can answer each query by scanning the entire set of n points. Alternatively, we
can define a range space (R^2 ,R), whereRincludes, for each pair ( x , y )∈R^2 and
r ∈R+, the set of all the points inside the disk of radius r centered at ( x , y ). Since the
VC dimension of the set of all disks on the plane is 3 (see Exercise14.6), we can sample
a random set of m = O

( 1
 /theta^2 ln
1
 /theta+
1
 /theta^2 ln
1
δ
)
points and give fast approximate answers to
all the queries by scanning only the sample.
Generating a random sample may require an initial scan of all the n points, but we
need to execute it only once. The /theta-sample theorem guarantees that with probability
at least 1−δ, the answers to all of the queries are within /thetaof the correct value. Fur-
thermore, since the /theta-sample estimates all possible disks, we could also use it for other
purposes, such as approximately identifying the k densest disks.

Example: Mining frequent itemsets
Consider a supermarket that wants to design discounts for customers based on buying
a collection of items. In this case, the supermarket is interested not only in what are
the most frequent items purchased, but also in what sets of items are most frequently
bought together. This problem arises in many settings, and is commonly referred to
as the problem of mining frequent itemsets. Formally, we can describe the problem
as follows: we are given a set of itemsIand a collection of transactionsT, where
a transaction is a subset ofI. We are interested in sets of items that appear in many
transactions, where what is meant by many transactions can depend on the setting. We
might use a threshold, or a percentage of transactions.
Mining frequent itemsets is challenging to accomplish efficiently, both because the
number of customer transactions is usually large, and because it takes significant mem-
ory to store all possible frequent itemsets. Even if one limits the problem to itemsets
of size up to k , there are

(|I|
k
)
possible itemsets, which grows large even for small k.
All known exact solutions to this problem require either several passes over the data or
significant storage or both to store candidate frequent itemsets and their counts. On the
other hand, solving the problem on a relatively small sample can give effective results
much more efficiently.
A natural goal would be to make sure we find all sufficiently frequent itemsets
and discard all sufficiently infrequent itemsets. There might be some itemsets that are
ambiguously in between the thresholds we set for frequent and infrequent itemsets,
and that could therefore be characterized either way. Suppose that we want to cor-
rectly characterize all sets with frequency greater thanθas frequent and all sets with

sample complexity, vc dimension, and rademacher complexity
frequency less thanθ− /thetaas infrequent; sets with frequency [θ− /theta, θ] would be in the
ambiguous range. How many transactions do we need to sample?
Our goal is to approximate the true frequency of each set within an additive error
of /theta/2. Then we can treat all sets with frequency at leastθ− /theta/2 as frequent itemsets
and all sets with frequency less thanθ− /theta/2 as infrequent itemsets, ensuring that we
correctly characterize sets with frequency greater thanθand sets with frequency less
thanθ− /theta.
If all transactions have size at most n, then there are O (|I| n) different itemsets that
could be frequent. Applying a Chernoff bound and a union bound would require a
sample of size m

(θ
 /theta^2
(
 nln|I|+ln^1 δ
))
.In practice, n<<|I|. In such a case an /theta-sample
can give a significantly better bound. (Although, strictly speaking, here we need an
( /theta/2)-sample.)
For each subset s ⊆I, let T ( s )={ t ∈T and s ⊆ t }denote the collection of all
transactions in the data set that include s. LetR={ T ( s )| s ⊆I}, and consider the
range space (T,R). We would like to bound the VC dimension of this range space
by a parameter that can be evaluated in one pass over the data (say when the data is
first loaded to the system). We first observe that the VC dimension is bounded by n,
the maximum size of any transaction in the data set. Indeed, a transaction of size q has
2 q subsets and is therefore included in no more than 2 q ranges. Since no transaction
can belong to more than 2 nranges, no set of more than ntransactions can be shattered.
Thus, by Theorem14.15, with probability at least 1−δ, a sample of size

O
(
 n
 /theta^2
ln
 n
 /theta
+
1
 /theta^2
ln
1
δ
)
(14.1)
can guarantee that all itemsets are accurately determined to within /theta/2 of their true
proportion with probability at least 1−δ, and thus is sufficient for identifying all the
frequent itemsets. A better bound is proven in Exercise14.12.

14.6 Rademacher Complexity
Rademacher complexity is an alternative approach for computing sample complex-
ity. Unlike the VC-dimension based bounds, which were distribution independent, the
Rademacher complexity bounds depend on the training set distribution, and thus can
give better bounds for specific input distributions. Furthermore, the Rademacher com-
plexity can, in principle, be estimated from the training set, allowing for strong bounds
derived from a sample itself. Another advantage of Rademacher complexity is that it
can be applied to the estimation of any function, not just 0–1 classification functions.
(There are, to be clear, generalizations of VC dimensions to non-binary function.)
To motivate the definition of Rademacher averages, let us start with the binary
classification setting we used in section14.1and then generalize. We have a training
set ( x 1 , c ( x 1 )),...,( xm , c ( xm )) where xi ∈ U and c ( xi )∈{− 1 , 1 }, and a set of possible
hypotheses h ∈Cwhere each h is a function from the universe U to{− 1 , 1 }. The
training error of a hypothesis on the training set is the fraction of samples where the

14.6rademacher complexity
hypothesis disagrees with the given classification. Formally,

err ˆ( h )=
1
m
|{ i : h ( xi )= c ( xi ), 1 ≤ i ≤ m }|.
Now we make use of the fact that, because h ( xi ) and c ( xi ) take on values in{− 1 , 1 },
1 − c ( xi ) h ( xi )
2
=
{
0if c ( xi )= h ( xi ),
1if c ( xi )= h ( xi ).
Hence we can write

err ˆ ( h )=
1
m
∑ m
i = 1
1 − c ( xi ) h ( xi )
2
=
1
2
−
1
2 m
∑ m
i = 1
c ( xi ) h ( xi ).
The expression m^1
∑ m
i = 1 c ( xi ) h ( xi ) represents the correlation between c and h ;if c and
h always agree, the value of the expression is 1, and if they alway disagree, the value is
−1. The hypothesis that minimizes the training error is the hypothesis that maximizes
the correlation.
Now, given a collection of sample points xi ,1≤ i ≤ m , we consider how well our
class of possible hypothesesCcan align with all possible classifications of these sample
points. To consider all possible classifications, we use the Rademacher variables : m
independent random variables,σ=(σ 1 ,...,σ m ), with Pr(σ i =−1)=Pr(σ i =1)=
1 /2. The hypothesis that aligns best with fixed values of the Rademacher variablesσ
is then the one that maximizes the value

1
m
∑ m
i = 1
σ ih ( xi ),
and our training error is

1
2
−max
h ∈C
1
2 m
∑ m
i = 1
σ ih ( xi ).
To consider all possible sample points, we consider the expectation over all possible
outcomes forσ,or

E σmax
h ∈C
1
m
∑ m
i = 1
σ ih ( xi ). (14.2)
This expression corresponds intuitively to how expressive our class of hypothesesC
is. For example, ifCconsisted of just a single hypothesis h , the expectation would be
0, as h ( xi )=σ i with probability 1/2 for any randomly chosenσ. On the other hand,
ifCshatters the set{ x 1 , x 2 ,..., xm }, then the expectation would be 1, as there would
be some h ∈Cso that h ( xi )=σ i for all i for each possible randomly chosenσ. In this
particular setting, the expectation is always between 0 and 1, and intuitively a higher
number corresponds to a more expressive set of hypotheses.
To move to a more general definition of Rademacher averages, instead of thinking of
sets of hypotheses, we consider a set of real-valued functionsF, where the inputs to the

sample complexity, vc dimension, and rademacher complexity
function are defined according to a probability space with distributionD. Hence, for
f ∈F, when we refer to E [ f ], this would correspond to E [ f ( Z )] where Z is a random
variable with distributionD. We generalize the expectation (14.2) as follows.

Definition 14.10: The empirical Rademacher average of a set of functions F with
respect to a sample S ={ z 1 ,..., zm } , is defined as

R ̃ m (F, S )= E σ
[
sup
f ∈F
1
m
∑ m
i = 1
σ if ( zi )
]
,
where the expectation is taken over the distribution of the Rademacher variables σ=
(σ 1 ,...,σ m ).

We remark that we use sup instead of max since we are dealing with a family of
real-valued functions, so the maximum technically may not exist.
For a fixed assignment of values to the Rademacher variables the value of
sup f ∈F m^1

∑ m
i = 1 σ if ( zi ) represents the best correlation between any function inFand
the vector (σ 1 ,...,σ m ), generalizing the correlation for binary classifications. The
empirical Rademacher average therefore measures how well one can correlate random
partitions of the sample with some function in the setF, which provides a measure
of how expressive the set is. We therefore use the terms empirical Rademacher aver-
age and empirical Rademacher complexity interchangeably (both terms are used in the
literature).
Now let us look at the empirical Rademacher average in a different way. For large
m , an average m^1

∑ m
i = 1 f ( zi ) over a random sample S ={ z^1 ,..., zm }, should provide a
good approximation to E [ f ]. Multiplying by the Rademacher variables, the expression
1
m

∑ m
i = 1 σ if ( zi ) corresponds to splitting the sample S into two subsamples, correspond-
ing to the values of i whereσ i =1 and the values of i whereσ i =−1. If S is a random
sample then the expression is similar to the difference between the average of the two
random subsamples, and hence the expectation

E σ
[
1
m
∑ m
i = 1
σ if ( zi )
]
,
should be small. Finally, the empirical Rademacher complexity

R ̃ m (F, S )= E σ
[
sup
f ∈F
1
m
∑ m
i = 1
σ if ( zi )
]
considers the supremum of this expectation over all functions inF. Intuitively, if the
empirical Rademacher average with respect to a sample of size m is small, then we
expect m to be sufficiently large for a sample to provide a good estimate for all functions
inF. We formulate and prove this intuition in Theorem14.20.
To remove the dependency on a particular sample we can take an expectation over
the distribution of all samples S of size m , where the samples are taken from the distri-
butionD.

14.6rademacher complexity
Definition 14.11: The Rademacher average of F is defined as

Rm (F)= E S [ R ̃ m (F, S )]= E S E σ
[
sup
f ∈F
1
m
∑ m
i = 1
σ if ( zi )
]
,
where the expectation over S corresponds to samples of size m from a given
distribution D_._

We similarly use the terms Rademacher average and Rademacher complexity inter-
changeably.

14.6.1 Rademacher Complexity and Sample Error
A key property of the Rademacher complexity of a set of functionsFis that it bounds
the expected maximum error in estimating the mean of any function f ∈Fusing a
sample.
Let E D[ f ( z )] be the true mean of f with respect to distributionD. The estimate of
E D[ f ( z )] using the sample S ={ z 1 ,..., zm }is m^1

∑ m
i = 1 f ( zi ). The expected maximum
error, averaged over all samples of size m fromD, is given by

E S
[
sup
f ∈F
(
E D[ f ( z )]−
1
m
∑ m
i = 1
f ( zi )
)]
.
The following theorem bounds this error in terms of the Rademacher complexity ofF.

Theorem 14.20:

E S
[
sup
f ∈F
(
E D[ f ( z )]−
1
m
∑ m
i = 1
f ( zi )
)]
≤ 2 Rm (F).
Proof: Pick a second sample S ′={ z 1 ′,..., zm ′}.

E S
[
sup
f ∈F
(
E D[ f ( z )]−
1
m
∑ m
i = 1
f ( zi )
)]
= E S
[
sup
f ∈F
(
E S ′
1
m
∑ m
i = 1
f ( zi ′)−
1
m
∑ m
i = 1
f ( zi )
)]
≤ E S , S ′
[
sup
f ∈F
(
1
m
∑ m
i = 1
f ( zi ′)−
1
m
∑ m
i = 1
f ( zi )
)]
= E S , S ′,σ
[
sup
f ∈F
(
1
m
∑ m
i = 1
σ i ( f ( zi )− f ( zi ′))
)]
≤ E S ,σ
[
sup
f ∈F
1
m
∑ m
i = 1
σ if ( zi )
]
+ E S ′,σ
[
sup
f ∈F
1
m
∑ m
i = 1
σ if ( zi ′)
]
= 2 Rm (F).
sample complexity, vc dimension, and rademacher complexity
The first equality holds because the expectation from the sample S ′is the expectation
of f. The first inequality, in which the order of the expectation with respect to S ′with the
operation sup f ∈Fis interchanged, follows from Jensen’s inequaliy (Theorem2.4), and
the fact that supremum is a convex function. For the second equality, we use the fact that
multiplying f ( zi )− f ( zi ′) by a Rademacher variableσ i does not change the expectation
of the sum. Ifσ i =1 there is clearly no change, and ifσ i =−1 this is equivalent to
switching zi and zi ′between the two samples, which does not change the expectation.
For the second inequality, we use thatσ i and−σ i have the same distribution, so we can
change the sign to simplify the expression.  /theta

Next we show that for bounded functions the Rademacher complexity is well
approximated by the empirical Rademacher complexity, and the estimation error is well
approximated by twice the Rademacher complexity, thereby obtaining a probabilistic
bound on the estimation error of any bounded function inFfrom a sample.

Theorem 14.21: Let F be a set of functions such that for any f ∈F and for any
two values x and y in the domain of f, | f ( x )− f ( y )|≤ c for some constant c. Let
Rm (F) be the Rademacher complexity, andR ̃ m (F, S ) the empirical Rademacher com-
plexity of the set F , with respect to a random sample S ={ z 1 ,..., zm } of size m from a
distribution D_._

(1) For any  /theta∈(0,1) ,

Pr(| R ̃ m (F, S )− Rm (F)|)≥ /theta)≤2e−^2 m  /theta
(^2) / c 2
.
(2) For all f ∈F and  /theta∈(0,1) ,
Pr

(
E D[ f ( z )]−
1
m
∑ m
i = 1
f ( zi )≥ 2 R ̃ m (F, S )+ 3  /theta
)
≤2e−^2 m  /theta
(^2) / c 2
.
Proof: To prove the first part of the theorem we observe that R ̃ m (F, S ) is a function of
m random variables, z 1 ,..., zm , and any change in one of these variables can change the
value of R ̃ m (F, S ) by no more than c / m. Since E S [ R ̃ m (F, S )]= Rm (F) we can apply
Theorem13.7to obtain
Pr(| R ̃ m (F, S )− Rm (F|)≥ /theta)≤2e−^2 m  /theta
(^2) / c 2
.
To prove the second part, we observe that E D[ f ( z )]− m^1
∑ m
i = 1 f ( zi ) is a function of
z 1 ,..., zm , and a change in one of the zi changes the value of that function by no more
than c / m. Applying a one-sided form of Theorem13.7we have
Pr

((
E D[ f ( z )]−
1
m
∑ m
i = 1
f ( zi )
)
− E S
[
E D[ f ( z )]−
1
m
∑ m
i = 1
f ( zi )
]
≥ /theta
)
≤e−^2 m  /theta
(^2) / c 2
.
We now apply the bound in Theorem14.20,

E S
[
E D[ f ( z )]−
1
m
∑ m
i = 1
f ( zi )
]
≤ 2 Rm (F),
14.6rademacher complexity
to obtain,

Pr
(
E D[ f ( z )]−
1
m
∑ m
i = 1
f ( zi )≥ 2 Rm (F)+ /theta
)
≤e−^2 m  /theta
(^2) / c 2

. (14.3)

From the first part of the theorem we know that Rm (F)≤ R ̃ m (F, S )+ /thetawith probabil-
ity at least 1−e−^2 m  /theta

(^2) / c 2

. Combining this with Eqn. (14.3), we have the second part of
the theorem,

Pr
(
E D[ f ( z )]−
1
m
∑ m
i = 1
f ( zi )≥ 2 R ̃ m (F, S )+ 3  /theta
)
≤2e−^2 m  /theta
(^2) / c 2

.  /theta

14.6.2. Estimating the Rademacher Complexity
While the Rademacher complexity can, in principle, be computed from a sample, in
practice it is often hard to compute the expected supremum over a large (or even infinite)
set of functions. Massart’s theorem provides a bound that is often easy to compute for
finite sets of functions.

Theorem 14.22 [ Massart’s theorem ] : Assume that |F| is finite. Let S ={ z 1 ,..., zm }
be a sample, and let

B =max
f ∈F
( m
∑
i = 1
f^2 ( zi )
)^12
then

R ̃ m (F, S )≤ B
√
2ln|F|
m
.
Proof: For any s >0,

e smR ̃ m (F, S )=e s E σ[sup f ∈F
∑ m
i = 1 σ if ( zi )],
where the expectation is taken over the assignments of the Rademacher variablesσ=
(σ 1 ,...,σ m ).
By Jensen’s inequality (Theorem2.4),

e s E σ[sup f ∈F
∑ m
i = 1 σ if ( zi )]≤ E σ
[
e s sup f ∈F
∑ m
i = 1 σ if ( zi )
]
= E σ
[
sup
f ∈F
(
e
∑ m
i = 1 s σ if ( zi )
)]
≤
∑
f ∈F
E σ
[(
e
∑ m
i = 1 s σ if ( zi )
)]
=
∑
f ∈F
E σ
[ m
∏
i = 1
e s σ if ( zi )
]
=
∑
f ∈F
∏ m
i = 1
E σ
[
e s σ if ( zi )
]
.
sample complexity, vc dimension, and rademacher complexity
Here the first line follows from Jensen’s inequality, and the second line is just a
rearrangement of terms. The third line bounds the supremum by a summation, which is
possible since all the terms are positive. The fourth line changes the sum in the exponent
to a product, and the last line arises from the independence of the sample values.
Since E [σ if ( zi )]=0 and− f ( zi )≤σ if ( zi )≤ f ( zi ), we can apply Hoeffding’s
Lemma (Lemma4.13) to obtain

E
[
e s σ if ( zi )
]
≤e s
(^2) (2 f ( zi )) (^2) / 8
=e s
(^2) f ( zi ) (^2) / 2
Thus,
e smR ̃ m (F, S )=e s E [sup f ∈F
∑ m
i = 1 σ if ( zi )]
≤

∑
f ∈F
∏ m
i = 1
e s
(^2) f ( zi ) (^2) / 2

=
∑
f ∈F
e s
(^2) / 2 ∑ mi = 1 f ( zi ) 2
≤|F|e( s
(^2) B (^2) )/ 2
Hence, for any s >0,
R ̃ m (F, S )≤^1
m

(
ln|F|
s
+
sB^2
2
)
Setting s =

√2ln|F|
B yields
R ̃ m (F, S )≤ B
√
2ln|F|
m
.  /theta
14.6.3 Application: Agnostic Learning of a Binary Classification
LetCbe a binary concept class defined on a domain X , and letDbe a probability
distribution on X. For each x ∈ X let c ( x ) be the correct classification of x. For each
hypothesis h ∈ C we define a function fh ( x )by

fh ( x )=
{
1if h ( x )= c ( x )
−1 otherwise.
LetF={ fh | h ∈C}. Our goal is to find h ′∈Csuch that with probability at least 1−δ

E [ fh ′]≥sup
fh ∈F
E [ fh ]− /theta.
Let S be a sample of size m. We apply Theorem14.22to bound the empirical
Rademacher averageFwith respect to S. Since the functions inFtake on only the
values−1 and 1,

B =max
f ∈F
( m
∑
i = 1
f^2 ( zi )
) (^12)
√
m ,
14.7 Exercises
and for a finiteF

R ̃ m (F, S )≤
√
2ln|F|
m
.
Next we express this bound in terms of the VC dimension of the concept classC.
Each function fh ∈Fcorresponds to a hypothesis h ∈C. Let d be the VC dimension
ofC. The projection of the range space ( X ,C) on a sample of size m has no more than
md different sets, as we know from Theorem14.1. Thus, the set of different functions
we need to consider is bounded by md , and hence

R ̃ m (F, S )≤
√
2 d ln m
m
.
The bound on R ̃ m (F, S ) in conjunction with Theorem14.21can be used to obtain an
alternative bound on the sample complexity of agnostic learning, similar to the bound
found in Section14.5.1. The details are considered in Exercise14.15. However, for
specific distributions, the projection of ( X ,C) on the training set can be significantly
smaller, yielding a smaller Rademacher complexity and smaller sample complexity.

14.7. Exercises
Exercise 14.1: Consider a range space ( X ,C) where X ={ 1 , 2 ,..., n }andCis the
set of all subsets of X of size k for some k < n. What is the VC dimension ofC?

Exercise 14.2: Consider a range space (R^2 ,C) of all axis-aligned rectangles inR^2.
That is, c ∈Cif for some x 0 < x 1 and y 0 < y 1 , c ={( x , y )∈R^2 | x 0 ≤ x ≤ x 1 and y 0 ≤
y ≤ y 1 }.

(a) Show that the VC dimension of (R^2 ,C) is equal to 4. You should show both a set
of four points that can be shattered, and show that no larger set can be shattered.
(b) Construct and analyze a PAC learning algorithm for the concept class of all axis-
aligned rectangles inR^2.

Exercise 14.3: Consider a range space (R^2 ,C) of all axis-aligned squares inR^2 .Show
that the VC dimension of (R^2 ,C) is equal to 3.

Exercise 14.4: Consider a range space (R^2 ,C) of all squares (that need not be axis-
aligned) inR^2. Show that the VC dimension of (R^2 ,C) is equal to 5.

Exercise 14.5: Consider a range space (R^3 ,C) of all axis-aligned rectangular boxes
inR^3. Find the VC dimension of (R^3 ,C); you should show both the largest number of
points that can be shattered, and show that no larger set can be shattered.

Exercise 14.6: Prove that the VC dimension of the collection of all closed disks on
the plane is 3.

sample complexity, vc dimension, and rademacher complexity
Exercise 14.7: Prove that the VC dimension of the range space (R d ,R), whereR
is the set of all half-spaces inR d , is at least d +1, by showing that the set consisting
of the origin (0, 0 ,...,0) and the d unit points (1, 0 , 0 ,...,0), (0, 1 , 0 ,...,0),...,
(0, 0 ,...,1) is shattered byR.

Exercise 14.8: Let S =( X , R ) and S ′=( X , R ′) be two range spaces. Prove that if
R ′⊆ R then the VC dimension of S ′is no larger than the VC dimension of S.

Exercise 14.9: Show that for n ≥ 2 d and d ≥1 the growth function satisfies

G( d , n )=
∑ d
i = 0
(
n
i
)
≤ 2
(
n e
d
) d
.
Exercise 14.10: Use the bound of Exercise14.9to improve the result of Theorem14.4
to show the VC dimension of the range space ( X ,R f )is O ( kd ln k ).

Exercise 14.11: Use the bound of Exercise14.9to improve the result of Theorem14.8
to show that there is an

m = O
(
d
 /theta
ln
1
 /theta
+
1
 /theta
ln
1
δ
)
such that a random sample fromDof size greater than or equal to m suffices to obtain
the required /theta-net with probability at least 1−δ.( Hint : Use Lemma14.3with x =
O (^1  /theta) and y =^2 dm .)

Exercise 14.12: (a) Improve the result in Eqn. (14.1) by showing that the VC dimen-
sion of the frequent-itemsets range space is bounded by the maximum number q such
that the data set has q different transactions all of size at least q.
(b) Show how to compute an upper bound on the number q defined in (a) in one pass
over the data.

Exercise 14.13: Prove Theorem14.11using the following hints. Let ( X , R ) be a range
space with VC dimension d. Let Y ={ y 1 ,..., yd }⊆ X be a set of d elements that is
shattered by R. Define a probability distributionDon R as follows: Pr( y 1 )= 1 − 16  /theta,
Pr( y 2 )=Pr( y 3 )= ··· = Pr ( yd )= 16  /theta/( d −1), and all other elements have probabil-
ity 0. Consider a sample of size m =( d −1)/(64 /theta). Show that with probability at least
1 /2 the sample does not include at least half of the elements in{ y 2 ,..., yd }. Conclude
that with probabilityδ≥ 1 /2 the output classification has error at least /theta.

Exercise 14.14: Given a set of functionsFand constants a , b ∈R, consider the set
of functions

F a , b ={ af + b | f ∈F}.
14.7exercises
Let Rm () and R ̃ m () denote the Rademacher complexity and the empirical Rademacher
complexity, respectively. Prove that

(a) R ̃ m (F a , b , S )=| a | R ̃ m (F, S ),
(b) Rm (F a , b )=| a | Rm (F).

Exercise 14.15: We apply Theorem14.21to compute a bound on the sample com-
plexity of agnostic learning a binary classification. Assume a concept class with VC
dimension d and a sample size m.

(a) Find a sample size m 1 such that the Empirical Rademacher Average of the corre-
sponding set of functions is at most /theta/4.
(b) Use Theorem14.21to find a sample size m such that with probability at least 1−δ
the expectation of all the functions are estimated within error /theta.
(c) Compare your bound to the result obtained in Section14.5.1.

chapter fifteen

Pairwise Independence and

Universal Hash Functions

In this chapter we introduce and apply a limited notion of independence, known as
k -wise independence, focusing in particular on the important case of pairwise indepen-
dence. Applying limited dependence can allow us to reduce the amount of randomness
used by a randomized algorithm, in some cases enabling us to convert a randomized
algorithm into an efficient deterministic one. Limited dependence is also used in the
design of universal and strongly universal families of hash functions, giving space- and
time-efficient data structures. We consider why universal hash functions are effective
in practice and show how they lead to simple perfect hash schemes. Finally, we apply
these ideas to the design of effective and practical approximation algorithms for finding
frequent objects in data streams, generalizing the Bloom filter data structure introduced
in Chapter 5.

15.1 Pairwise Independence
Recall that in Chapter 2 we defined a set of events E 1 , E 2 ,..., En to be mutually inde-
pendent if, for any subset I ⊆[1, n ],

Pr
(
⋂
i ∈ I
Ei
)
=
∏
i ∈ I
Pr( Ei ).
Similarly, we defined a set of random variables X 1 , X 2 ,..., Xn to be mutually indepen-
dent if, for any subset I ⊆[1, n ] and any values xi , i ∈ I ,

Pr
(
⋂
i ∈ I
( Xi = xi )
)
=
∏
i ∈ I
Pr( Xi = xi ).
Mutual independence is often too much to ask for. Here, we examine a more limited
notion of independence that proves useful in many contexts: k -wise independence.

15.1pairwise independence
Definition 15.1:

1. A set of events E 1 , E 2 ,..., Enis k- wise independent if, for any subset I ⊆[1, n ] with
| I |≤ k,

Pr
(
⋂
i ∈ I
Ei
)
=
∏
i ∈ I
Pr( Ei ).
2. A set of random variables X 1 , X 2 ,..., Xnis k-wise independent if, for any subset
I ⊆[1, n ] with | I |≤ k and for any values xi,i ∈ I,

Pr
(
⋂
i ∈ I
( Xi = xi )
)
=
∏
i ∈ I
Pr( Xi = xi ).
3. The random variables X 1 , X 2 ,..., Xnare said to be pairwise independent if they are
2-wise independent. That is, for any pair i , j and any values a , b,

Pr(( Xi = a )∩( Xj = b ))=Pr( Xi = a )Pr( Xj = b ).
15.1.1 Example: A Construction of Pairwise Independent Bits
A random bit is uniform if it assumes the values 0 and 1 with equal probability. Here we
show how to derive m = 2 b −1 uniform pairwise independent bits from b independent,
uniform random bits X 1 ,..., Xb.
Enumerate the 2 b −1 nonempty subsets of{ 1 , 2 ,..., b }in some order, and let Sj be
the j th subset in this ordering. Set

Yj =
⊕
i ∈ Sj
Xi ,
where

⊕
is the exclusive-or operation. Equivalently, we could write this as
Yj =
∑
i ∈ Sj
Xi mod 2.
Lemma 15.1: The Yjare pairwise independent uniform bits.

Proof: We first show that, for any nonempty set Sj , the random bit

Yj =
⊕
i ∈ Sj
Xi
is uniform. This follows easily using the principle of deferred decisions (see Sec-
tion1.3). Let z be the largest element of S. Then

Yj =
⎛
⎝
⊕
i ∈ Sj −{ z }
Xi
⎞
⎠⊕ Xz.
pairwise independence and universal hash functions
Suppose we reveal the values for Xi for all i ∈ Sj −{ z }. Then it is clear that the value
of Xz determines the value of Yj and that Yj will take on the values 0 and 1 with equal
probability.
Now consider any two variables Yk and Y nwith their corresponding sets Sk and S n.
Without loss of generality, let z be an element of S nthat is not in Sk and consider, for
any values c , d ∈{ 0 , 1 },

Pr( Y n= d | Yk = c ).
We claim, again by the principle of deferred decisions, that this probability is 1/2. For
suppose that we reveal the values for Xi for all i in ( Sk ∪ S n)−{ z }. Even though this
determines the value of Yk , the value of Xz will determine Y n. The conditioning on the
value of Yk therefore does not change that Y nis equally likely to be 0 or 1. Hence

Pr(( Yk = c )∩( Y n= d ))=Pr( Y n= d | Yk = c )Pr( Yk = c )
= 1 / 4.
Since this holds for any values of c , d ∈{ 0 , 1 }, we have proven pairwise
independence.  /theta

15.1.2 Application: Derandomizing an Algorithm for Large Cuts
In Chapter 6 , we examined a simple randomized algorithm for finding a large cut in an
undirected graph G =( V , E ): the algorithm places each vertex on one side of the cut
independently with probability 1/2. The expected value of a cut generated this way is
m /2, where m is the number of edges in the graph. We also showed (in Section6.3)
that this algorithm could be derandomized effectively using conditional expectations.
Here we present another way to derandomize this algorithm, using pairwise inde-
pendence. This argument exemplifies the approach of derandomization using k -wise
independence.
Suppose that we have a collection Y 1 , Y 2 ,..., Yn of pairwise independent bits, where
n =| V |is the number of vertices in the graph. We define our cut by putting all vertices
i with Yi =0 on one side of the cut and all vertices i with Yi =1 on the other side
of the cut. We show that, in this case, the expected number of edges that crosses the
cut remains m /2. That is, we do not require complete independence to analyze the
expectation; pairwise independence suffices.
Recall the argument of Section6.2.1: number the edges from 1 to m , and let Zi = 1
if the i th edge crosses the cut and Zi =0 otherwise. Then Z =

∑ m
i = 1 Zi is the number
of edges crossing the cut, and

E [ Z ]= E
[ m
∑
i = 1
Zi
]
=
∑ m
i = 1
E [ Zi ].
Let a and b be the two vertices adjacent to the i th edge. Then

Pr( Zi =1)=Pr( Ya = Yb )= 1 / 2 ,
15.1pairwise independence
where we have used the pairwise independence of Ya and Yb. Hence E [ Zi ]= 1 /2, and
it follows that E [ Z ]= m /2.
Now let our n pairwise independent bits Y 1 ,..., Yn be generated from b inde-
pendent, uniform random bits X 1 , X 2 ,..., Xb in the manner of Lemma15.1(here
b =log 2 ( n +1)). Then E [ Z ]= m /2 for the resulting cut, where the sample space
is just all the possible choices for the initial b random bits. By the probabilistic method
(specifically, Lemma6.2), there is some setting of the b bits that gives a cut with value
at least m /2. We can try all possible 2 b settings for the bits to find such a cut. Since
2 b is O ( n ) and since, for each cut, the number of crossing edges can easily be calcu-
lated in O ( m ) time, it follows that we can find a cut with at least m /2 crossing edges
deterministically in O ( mn ) time.
Although this approach does not appear to be as efficient as the derandomization
of Section6.3, one redeeming feature of the scheme is that it is trivial to parallelize.
If we have sufficiently many processors available, then each of the m( n ) possibilities
for the random bits X 1 , X 2 ,..., Xb can be assigned to a single processor, with each
possibility giving a cut. The parallelization reduces the running time by a factor of m( n )
using O ( n ) processors. In fact, using O ( mn ) processors, we can assign a processor for
each combination of a specific edge with a specific sequence of random bits and then
determine, in constant time, whether the edge crosses the cut for that setting of the
random bits. After that, only O (log n ) time is necessary to collect the results and find
the large cut.
15.1.3. Example: Constructing Pairwise Independent Values Modulo a Prime
We consider another construction that provides pairwise independent values
Y 0 , Y 1 ,..., Yp − 1 that are uniform over the values{ 0 , 1 ,..., p − 1 }for a prime p.
Our construction requires only two independent, uniform values X 1 and X 2 over
{ 0 , 1 ,..., p − 1 }, from which we derive

Yi = X 1 + iX 2 mod p for i = 0 ,..., p − 1.
Lemma 15.2: The variables Y 0 , Y 1 ,..., Yp − 1 are pairwise independent uniform ran-
dom variables over { 0 , 1 ,..., p − 1 }.
Proof: It is clear that each Yi is uniform over{ 0 , 1 ,..., p − 1 }, again by applying the
principle of deferred decisions. Given X 2 , the p distinct possible values for X 1 give p
distinct possible values for Yi modulo p , each of which is equally likely.
Now consider any two variables Yi and Yj. We wish to show that, for any a , b ∈
{ 0 , 1 ,..., p − 1 },
Pr(( Yi = a )∩( Yj = b ))=
1
p^2
,
which implies pairwise independence. The event Yi = a and Yj = b is equivalent to
X 1 + iX 2 = a mod p and X 1 + jX 2 = b mod p.
pairwise independence and universal hash functions
This is a system of two equations and two unknowns with just one solution:

X 2 =
b − a
j − i
mod p and X 1 = a −
i ( b − a )
j − i
mod p.
Since X 1 and X 2 are independent and uniform over{ 0 , 1 ,..., p − 1 }, the result
follows.  /theta

This proof can be extended to the following useful result: given 2 n independent, uni-
form random bits, one can construct up to 2 n pairwise independent and uniform strings
of n bits. The extension requires knowledge of finite fields, so we only sketch the result
here. The setup and proof are exactly the same as for Lemma15.2except that, instead
of working modulo p , we perform all arithmetic in a fixed finite field with 2 n elements
(such as the field GF (2 n ) of all polynomials with coefficients in GF (2) modulo some
irreducible polynomial of degree n ). That is, we assume a fixed one-to-one mapping f
from strings of n bits, which can also be thought of as numbers in{ 0 , 1 ,..., 2 n − 1 },
to field elements. We let

Yi = f −^1 ( f ( X 1 )+ f ( i )· f ( X 2 )),
where X 1 and X 2 are chosen independently and uniformly over{ 0 , 1 ,..., 2 n − 1 }, i runs
over the values{ 0 , 1 ,..., 2 n − 1 }, and the addition and multiplication are performed
over the field. The Yi are then pairwise independent.

15.2 Chebyshev’s Inequality for Pairwise Independent Variables
Pairwise independence is much weaker than mutual independence. For example, we
can use Chernoff bounds to evaluate the tail distribution of a sum of independent ran-
dom variables, but we cannot directly apply a Chernoff bound if the Xi are only pairwise
independent. However, pairwise independence is strong enough to allow for easy calcu-
lation of the variance of the sum, which allows for a useful application of Chebyshev’s
inequality.

Theorem 15.3: Let X =

∑ n
i = 1 Xi, where the Xiare pairwise independent random vari-
ables. Then

Var [ X ]=
∑ n
i = 1
Var [ Xi ].
Proof: We saw in Chapter 3 that

Var
[ n
∑
i = 1
Xi
]
=
∑ n
i = 1
Var [ Xi ]+ 2
∑
i < j
Cov ( Xi , Xj ),
where

Cov ( Xi , Xj )= E [( Xi − E [ Xi ])( Xj − E [ Xj ])]= E [ XiXj ]− E [ Xi ] E [ Xj ].
15.2chebyshev’s inequality for pairwise independent variables
Since Xi , X 2 ,..., Xn are pairwise independent, it is clear (by the same argument as
in Theorem3.3) that for any i = j we have

E [ XiXj ]− E [ Xi ] E [ Xj ]= 0.
Therefore,

Var [ X ]=
∑ n
i = 1
Var [ Xi ].
 /theta
Applying Chebyshev’s inequality to the sum of pairwise independent variables yields
the following.

Corollary 15.4: Let X =

∑ n
i = 1 Xi, where the Xiare pairwise independent random
variables. Then

Pr(| X − E [ X ]|≥ a )≤
Var [ X ]
a^2
=
∑ n
i = 1 Var [ Xi ]
a^2
15.2.1 Application: Sampling Using Fewer Random Bits
We apply Chebyshev’s inequality for pairwise independent random variables to obtain
a good approximation through sampling. This uses less randomness than the natural
approach based on Chernoff bounds.
Suppose that we have a function f :{ 0 , 1 } n →[0,1] mapping n -bit vectors into real
numbers. Let f ̄=

(∑
x ∈{ 0 , 1 } nf ( x )
)
/ 2 n be the average value of f. We want to compute
a1−δconfidence interval for f ̄.That is, we wish to find an interval [ f ̃−ε, f ̃+ε]
such that

Pr( f ̄∈[ f ̃−ε, f ̃+ε])≥ 1 −δ.
As a concrete example, suppose that we have an integrable function g :[0,1]→
[0,1] and that the derivative of g exists with| g ′( x )|≤ C for some fixed constant C over
the entire interval (0,1). We are interested in

∫ 1
x = 0 g ( x ) dx. There may be no direct way
to compute this integral exactly, but through sampling we can obtain a good estimate. If
X is a uniform random variable on [0,1], then E [ g ( X )]=

∫ 1
x = 0 g ( x ) dx by the definition
of the expectation of a continuous random variable. By taking the average of multiple
independent samples, we can approximate the integral. If our source of randomness
generates only random bits instead of random real numbers, then we might approximate
the integral as follows. For a string of bits x ∈{ 0 , 1 } n , we may interpret x as a real
number ̃ x ∈[0,1] by considering it as a decimal in binary; for example, 11001 would
correspond to 0. 11001 = 25 /32. Let f ( x ) denote the value of the function g at the
decimal value ̃ x. Then, for any integer i with 0≤ i ≤ 2 n −1, for y ∈[ i / 2 n ,( i +1)/ 2 n )
we have

f
(
i
2 n
)
−
C
2 n
≤ g ( y )≤ f
(
i
2 n
)
+
C
2 n
.
pairwise independence and universal hash functions
It follows that

1
2 n
∑
x ∈{ 0 , 1 } n
(
f ( x )−
C
2 n
)
≤
∫ 1
x = 0
g ( x ) dx ≤
1
2 n
∑
x ∈{ 0 , 1 } n
(
f ( x )+
C
2 n
)
.
By taking n sufficiently large, we can guarantee that f ̄=

(∑
x ∈{ 0 , 1 } nf ( x )
)
/ 2 n differs
from the integral of g by at most a constantγ. In this case, a confidence interval
[ f ̃−ε, f ̃+ε]for f ̄would yield a confidence interval [ ̃ g −ε−γ, g ̃+ε+γ] for the
integral of g.
We could handle the problem of finding a confidence interval for the average value
f ̄by using independent samples and applying a Chernoff bound. That is, suppose
that we sample uniformly with replacement random points in{ 0 , 1 } n , evaluate f at
all of these points, and take the average of our samples. This is similar to the para-
meter estimation of Section4.2.3. Theorem15.5is an immediate consequence of the
following Chernoff bound, which can be derived using Exercises4.13and4.19.If
Z 1 , Z 2 ,..., Zm are independent, identically distributed, real-valued random variables
with meanμthat take on one of a finite possible set of values in the range [0,1],
then

Pr
(∣
∣∣
∣
∑ m
i = 1
Zi − m μ
∣∣
∣
∣≥ε m
)
≤2e−^2 m ε
2
.
Theorem 15.5: Let f :{ 0 , 1 } n →[0,1] andf ̄=

(∑
x ∈{ 0 , 1 } nf ( x )
)
/ 2 n.LetX 1 ,..., Xm
be chosen independently and uniformly at random from { 0 , 1 } n.Ifm >ln(2/δ)/ 2 ε^2 ,
then

Pr
(∣
∣∣
∣
1
m
∑ m
i = 1
f ( Xi )− f ̄
∣
∣∣
∣≥ε
)
≤δ.
Although the exact choice of m depends on the Chernoff bound used, in general
this straightforward approach requires m(ln(1/δ)/ε^2 ) samples to achieve the desired
bounds.
A possible problem with this approach is that it requires a large number of random
bits to be available. Each sample of f requires n independent bits, so applying Theo-
rem15.5means that we need at least m( n ln(1/δ)/ε^2 ) independent, uniform random
bits to obtain an approximation that has additive error at mostεwith probability at least
1 −δ.
A related problem arises when we need to record how the samples were obtained, so
that the work can be reproduced and verified at a later time. In this case, we also need
to store the random bits used for archival purposes. In this case, using fewer random
bits would lessen the storage requirements.
We can use pairwise independent samples to obtain a similar approximation
using less randomness. Let X 1 ,..., Xm be pairwise independent points chosen from
{ 0 , 1 } n , and let Y =

(∑ m
i = 1 f ( Xi )
)
/ m. Then E [ Y ]= f ̄, and we can apply Chebyshev’s
15.3 Universal Families of Hash Functions
inequality to obtain

Pr(| Y − f ̄|≥ε)≤
Var [ Y ]
ε^2
=
Var
[(∑ m
i = 1 f ( Xi )
)
/ m
]
ε^2
=
∑ m
i = 1 Var [ f ( Xi )]
m^2 ε^2
≤
m
m^2 ε^2
=
1
m ε^2
,
since Var [ f ( Xi )]≤ E [( f ( Xi ))^2 ]≤1. We therefore find Pr(| Y − f ̄|≥ε)≤δwhen m =
1 /δε^2. (In fact, one can prove that Var [ f ( Xi )]≤ 1 /4, giving a slightly better bound; this
is left as Exercise 15.4.)
Using pairwise independent samples requires more samples: /eta(1/δε^2 ) instead of the
 /eta(ln(1/δ)/ε^2 ) samples when they are independent. But recall from Section15.1.3that
we can obtain up to 2 n pairwise independent samples with just 2 n uniform independent
bits. Hence, as long as 1/δε^2 < 2 n , just 2 n random bits suffice; this is much less than
the number required when using completely independent samples. Usuallyεandδare
fixed constants independent of n , and this type of estimation is quite efficient in terms
of both the number of random bits used and the computational cost.

15.3. Universal Families of Hash Functions
Up to this point, when studying hash functions we modeled them as being completely
random in the sense that, for any collection of items x 1 , x 2 ,..., xk , the hash values
h ( x 1 ), h ( x 2 ),..., h ( xk ) were considered uniform and independent over the range of the
hash function. This was the framework we used to analyze hashing as a balls-and-bins
problem in Chapter 5. The assumption of a completely random hash function simplifies
the analysis for a theoretical study of hashing. In practice, however, completely random
hash functions are too expensive to compute and store, so the model does not fully
reflect reality.
Two approaches are commonly used to implement practical hash functions. In many
cases, heuristic or ad hoc functions designed to appear random are used. Although these
functions may work suitably for some applications, they generally do not have any
associated provable guarantees, making their use potentially risky. Another approach
is to use hash functions for which there are some provable guarantees. We trade away
the strong statements one can make about completely random hash functions for weaker
statements with hash functions that are efficient to store and compute.
We consider one of the computationally simplest classes of hash functions that pro-
vide useful provable performance guarantees: universal families of hash functions.
These functions are widely used in practice.

Definition 15.2: Let U be a universe with | U |≥ n and let V ={ 0 , 1 ,..., n − 1 } .A
family of hash functions H from U to V is said to be k- universal if, for any elements

pairwise independence and universal hash functions
x 1 , x 2 ,..., xkand for a hash function h chosen uniformly at random from H , we have

Pr( h ( x 1 )= h ( x 2 )= ··· = h ( xk ))≤
1
nk −^1
.
A family of hash functions H from U to V is said to be strongly k- universal if, for
any elements x 1 , x 2 ,..., xk, any values y 1 , y 2 ,..., yk ∈{ 0 , 1 ,..., n − 1 } , and a hash
function h chosen uniformly at random from H , we have

Pr(( h ( x 1 )= y 1 )∩( h ( x 2 )= y 2 )∩···∩( h ( xk )= yk ))=
1
nk
.
We will primarily be interested in 2-universal and strongly 2-universal families of hash
functions. When we choose a hash function from a family of 2-universal hash functions,
the probability that any two elements x 1 and x 2 have the same hash value is at most 1/ n.
In this respect, a hash function chosen from a 2-universal family acts like a random hash
function. It does not follow, however, that for 2-universal families the probability of any
three values x 1 , x 2 , and x 3 having the same hash value is at most 1/ n^2 , as would be the
case if the hash values of x 1 , x 2 , and x 3 were mutually independent.
When a family is strongly 2-universal and we choose a hash function from that fam-
ily, the values h ( x 1 ) and h ( x 2 ) are pairwise independent, since the probability that they
take on any specific pair of values is 1/ n^2. Because of this, hash functions chosen from
a strongly 2-universal family are also known as pairwise independent hash functions.
More generally, if a family is strongly k -universal and we choose a hash function from
that family, then the values h ( x 1 ), h ( x 2 ),..., h ( xk )are k -wise independent. Notice that
a strongly k -universal hash function is also k -universal.
To gain some insight into the behavior of universal families of hash functions, let
us revisit a problem we considered in the balls-and-bins framework of Chapter 5 .We
saw in Section5.2that, when n items are hashed into n bins by a completely random
hash function, the maximum load is /eta(log n /log log n ) with high probability. We now
consider what bounds can be obtained on the maximum load when n items are hashed
into n bins using a hash function chosen from a 2-universal family.
First, consider the more general case where we have m items labeled x 1 , x 2 ,..., xm.
For 1∑ ≤ i < j ≤ m , let Xij =1 if items xi and xj land in the same bin. Let X =

1 ≤ i < j ≤ mXij be the total number of collisions between pairs of items. By the linearity
of expectations,

E [ X ]= E
⎡
⎣
∑
1 ≤ i < j ≤ m
Xij
⎤
⎦=
∑
1 ≤ i < j ≤ m
E [ Xij ].
Since our hash function is chosen from a 2-universal family, it follows that

E [ Xij ]=Pr( h ( xi )= h ( xj ))≤
1
n
and hence

E [ X ]≤
(
m
2
)
1
n
<
m^2
2 n
. (15.1)
15.3universal families of hash functions
Markov’s inequality then yields

Pr
(
X ≥
m^2
n
)
≤Pr( X ≥ 2 E [ X ])≤
1
2
.
If we now suppose that the maximum number of items in a bin is Y , then the number
of collisions X must be at least

( Y
2
)
. Therefore,

Pr
((
Y
2
)
≥
m^2
n
)
≤Pr
(
X ≥
m^2
n
)
≤
1
2
,
which implies that

Pr
(
Y − 1 ≥ m
√
2 / n
)
≤
1
2
In particular, in the case where m = n , the maximum load is at most 1+

√
2 n with
probability at least 1/2.
This result is much weaker than the one for perfectly random hash functions, but
it is extremely general in that it holds for any 2-universal family of hash functions.
The result will prove useful for designing perfect hash functions, as we describe in
Section15.3.3.

15.3.1 Example: A 2-Universal Family of Hash Functions
Let the universe U be the set{ 0 , 1 , 2 ,..., m − 1 }and let the range of our hash func-
tion be V ={ 0 , 1 , 2 ,..., n − 1 }, with m ≥ n. Consider the family of hash functions
obtained by choosing a prime p ≥ m , letting

ha , b ( x )=(( ax + b )mod p )mod n
and then taking the family

H={ ha , b | 1 ≤ a ≤ p − 1 , 0 ≤ b ≤ p − 1 }.
Notice that a cannot here take on the value 0.

Lemma 15.6: H is 2-universal.

Proof: We count the number of functions inHfor which two distinct elements x 1 and
x 2 from U collide.
First we note that, for any x 1 = x 2 ,

ax 1 + b = ax 2 + b mod p.
This follows because ax 1 + b = ax 2 + b mod p implies that a ( x 1 − x 2 )=0mod p , yet
here both a and ( x 1 − x 2 ) are nonzero modulo p.
In fact, for every pair of values ( u ,v) such that u =vand 0≤ u ,v≤ p −1, there
exists exactly one pair of values ( a , b ) for which ax 1 + b = u mod p and ax 2 + b =v

pairwise independence and universal hash functions
mod p. This pair of equations has two unknowns, and its unique solution is given by:

a =
v− u
x 2 − x 1
mod p ,
b = u − ax 1 mod p.
Since there is exactly one hash function for each pair ( a , b ), it follows that there is
exactly one hash function inHfor which

ax 1 + b = u mod p and ax 2 + b =vmod p.
Therefore, in order to bound the probability that ha , b ( x 1 )= ha , b ( x 2 ) when ha , b is
chosen uniformly at random fromH, it suffices to count the number of pairs ( u ,v),
0 ≤ u ,v≤ p −1, for which u =vbut u =vmod n. For each choice of u there are
at most p / n −1 possible appropriate values for v , giving at most p ( p / n −1)≤
p ( p −1)/ n pairs. Each pair corresponds to one of p ( p −1) hash functions, so

Pr( ha , b ( x 1 )= ha , b ( x 2 ))≤
p ( p −1)/ n
p ( p −1)
=
1
n
,
proving thatHis 2-universal.  /theta

15.3.2 Example: A Strongly 2-Universal Family of Hash Functions
We can apply ideas similar to those used to construct the 2-universal family of hash
functions in Lemma15.6to construct strongly 2-universal families of hash functions.
To start, suppose that both our universe U and the range V of the hash function are
{ 0 , 1 , 2 ,..., p − 1 }for some prime p. Now let

ha , b ( x )=( ax + b )mod p ,
and consider the family

H={ ha , b | 0 ≤ a , b ≤ p − 1 }.
Notice that here a can take on the value 0, in contrast with the family of hash functions
used in Lemma15.6.

Lemma 15.7: H is strongly 2-universal.

Proof: This is entirely similar to the proof of Lemma15.2. For any two elements x 1
and x 2 in U and any two values y 1 and y 2 in V , we need to show that

Pr(( ha , b ( x 1 )= y 1 )∩( ha , b ( x 2 )= y 2 ))=
1
p^2
.
The condition that both ha , b ( x 1 )= y 1 and ha , b ( x 2 )= y 2 yields two equations mod-
ulo p with two unknowns, the values for a and b : ax 1 + b = y 1 mod p and ax 2 + b =
y 2 mod p. This system of two equations and two unknowns has just one solution:

a =
y 2 − y 1
x 2 − x 1
mod p ,
b = y 1 − ax 1 mod p.
15.3universal families of hash functions
Hence only one choice of the pair ( a , b ) out of the p^2 possibilities results in x 1 and x 2
hashing to y 1 and y 2 , proving that
Pr(( ha , b ( x 1 )= y 1 )∩( ha , b ( x 2 )= y 2 ))=
1
p^2
,
as required.  /theta
Although this gives a strongly 2-universal hash family, the restriction that the universe
U and the range V be the same makes the result almost useless; usually we want to
hash a large universe into a much smaller range. We can extend the construction in a
natural way that allows much larger universes. Let V ={ 0 , 1 , 2 ,..., p − 1 }, but now let
U ={ 0 , 1 , 2 ,..., pk − 1 }for some integer k and prime p. We can interpret an element
u in the universe U as a vector ̄ u =( u 0 , u 1 ,..., uk − 1 ), where 0≤ ui ≤ p −1for0≤
i ≤ k −1 and where

∑ k − 1
i = 0 uip
i = u. In fact, this gives a one-to-one mapping between
vectors of this form and elements of U.
For any vector ̄ a =( a 0 , a 1 ,..., ak − 1 ) with 0≤ ai ≤ p −1, 0≤ i ≤ k −1, and for
any value b with 0≤ b ≤ p −1, let
ha ̄, b ( u )=
( k − 1
∑
i = 0
aiui + b
)
mod p ,
and consider the family
H={ ha ̄, b | 0 ≤ ai , b ≤ p −1 for all 0≤ i ≤ k − 1 }.
Lemma 15.8: H is strongly 2 -universal.
Proof: We follow the proof of Lemma15.7. For any two elements u 1 and u 2 with
corresponding vectors ̄ ui =( ui , 0 , ui , 1 ,..., ui , k − 1 ) and for any two values y 1 and y 2 in
V , we need to show that
Pr(( ha ̄, b ( u 1 )= y 1 )∩( ha ̄, b ( u 2 )= y 2 ))=
1
p^2
.
Since u 1 and u 2 are different, they must differ in at least one coordinate. Without loss
of generality let u 1 , 0 = u 2 , 0. For any given values of a 1 , a 2 ,..., ak − 1 , the condition that
ha ̄, b ( u 1 )= y 1 and ha ̄, b ( u 2 )= y 2 is equivalent to:
a 0 u 1 , 0 + b =
⎛
⎝ y 1 −
∑ k −^1
j = 1
aju 1 , j
⎞
⎠mod p
a 0 u 2 , 0 + b =
⎛
⎝ y 1 −
∑ k −^1
j = 1
aju 2 , j
⎞
⎠mod p.
For any given values of a 1 , a 2 ,..., ak − 1 , this gives a system with two equations and two
unknowns (namely, a 0 and b ), which – as in Lemma15.8– has exactly one solution.
Hence, for every a 1 , a 2 ,..., ak − 1 , only one choice of the pair ( a 0 , b ) out of the p^2
pairwise independence and universal hash functions
possibilities results in u 1 and u 2 hashing to y 1 and y 2 , proving that

Pr(( ha ̄, b ( u 1 )= y 1 )∩( ha ̄, b ( u 2 )= y 2 ))=
1
p^2
,
as required.  /theta

Although we have described both the 2-universal and the strongly 2-universal hash fam-
ilies in terms of arithmetic modulo a prime number, we could extend these techniques
to work over general finite fields – in particular, fields with 2 n elements represented by
sequences of n bits. The extension requires knowledge of finite fields, so we just sketch
the result here. The setup and proof are exactly the same as for Lemma15.8except that,
instead of working modulo p , we perform all arithmetic in a fixed finite field with 2 n
elements. We assume a fixed one-to-one mapping f from strings of n bits, which can
also be thought of as numbers in{ 0 , 1 ,..., 2 n − 1 }, to field elements. We let

ha ̄, b ( u )= f −^1
( k − 1
∑
i = 0
f ( ai )· f ( ui )+ f ( b )
)
,
where the ai and b are chosen independently and uniformly over{ 0 , 1 ,..., 2 n − 1 }
and where the addition and multiplication are performed over the field. This gives a
strongly 2-universal hash function with a range of size 2 n.

15.3.3 Application: Perfect Hashing
Perfect hashing is an efficient data structure for storing a static dictionary. In a static
dictionary, items are permanently stored in a table. Once the items are stored, the table
is used only for search operations: a search for an item gives the location of the item in
the table or returns that the item is not in the table.
Suppose that a set S of m items is hashed into a table of n bins, using a hash function
from a 2-universal family and chain hashing. In chain hashing (see Section5.5.1), items
hashed to the same bin are kept in a linked list. The number of operations for looking
up an item x is proportional to the number of items in x ’s bin. We have the following
simple bound.

Lemma 15.9: Assume that m elements are hashed into an n-bin chain hashing table
by using a hash function h chosen uniformly at random from a 2-universal family. For
an arbitrary element x, let X be the number of items at the bin h ( x ). Then

E [ X ]≤
{
m / n if x ∈/ S ,
1 +( m −1)/ n if x ∈ S.
Proof: Let Xi =1 if the i th element of S (under some arbitrary ordering) is in the same
bin as x and 0 otherwise. Because the hash function is chosen from a 2-universal family,
it follows that

Pr( Xi =1)≤ 1 / n.
15.3universal families of hash functions
Then the first result follows from

E [ X ]= E
[ m
∑
i = 1
Xi
]
=
∑ m
i = 1
E [ Xi ]≤
m
n
,
where we have used the universality of the hash function to conclude that E [ Xi ]≤ 1 / n.
Similarly, if x is an element of S then (without loss of generality) let it be the first
element of S. Hence X 1 =1, and again

Pr( Xi =1)≤ 1 / n
when i =1. Therefore,

E [ X ]= E
[ m
∑
i = 1
Xi
]
= 1 +
∑ m
i = 2
E [ Xi ]≤ 1 +
m − 1
n
.
 /theta
Lemma15.9shows that the average performance of hashing when using a hash function
from a 2-universal family is good, since the time to look through a bin of any item is
bounded by a small number. For instance, if m = n then, when searching the hash table
for x , the expected number of items other than x that must be examined is at most 1.
However, this does not give us a bound on the worst-case time of a lookup. Some bin
may contain

√
n elements or more, and a search for one of these elements requires a
much longer lookup time.
This motivates the idea of perfecthashing. Given a set S , we would like to construct a
hash table that gives excellent worst-case performance. Specifically, by perfect hashing
we mean that only a constant number of operations are required to find an item in a hash
table (or to determine that it isn’t there).
We first show that perfect hashing is easy if we are given sufficient space for the
hash table and a suitable 2-universal family of hash functions.

Lemma 15.10: If h ∈H is chosen uniformly at random from a 2-universal family of
hash functions mapping the universe U to [0, n −1] then, for any set S ⊂ U of size m,
the probability of h being perfect is at least 1 / 2 when n ≥ m^2_._

Proof: Let s 1 , s 2 ,..., sm be the m items of S. Let Xij be 1 if the h ( si )= h ( sj ) and 0
otherwise. Let X =

∑
1 ≤ i < j ≤ mXij. Then, as we saw in Eqn. (15.1), the expected number
of collisions when using a 2-universal hash function is

E [ X ]= E
⎡
⎣
∑
1 ≤ i < j ≤ m
Xij
⎤
⎦=
∑
1 ≤ i < j ≤ m
E [ Xij ]≤
(
m
2
)
1
n
<
m^2
2 n
.
Markov’s inequality then yields

Pr
(
X ≥
m^2
n
)
≤Pr( X ≥ 2 E [ X ])≤
1
2
.
Hence, when n ≥ m^2 ,wefind X <1 with probability at least 1/2. This implies that a
randomly chosen hash function is perfect with probability at least 1/2.  /theta

pairwise independence and universal hash functions
To find a perfect hash function when n ≥ m^2 , we may simply try hash functions cho-
sen uniformly at random from the 2-universal family until we find one with no colli-
sions. This gives a Las Vegas algorithm. On average we need to try at most two hash
functions.
We would like to have perfect hashing without requiring space for m( m^2 ) bins to
store the set of m items. We can use a two-level scheme that accomplishes perfect
hashing using only O ( m ) bins. First, we hash the set into a hash table with m bins using
a hash function from a 2-universal family. Some of these bins will have collisions.
For each such bin, we provide a second hash function from an appropriate 2-universal
family and an entirely separate second hash table. If the bin has k >1 items in it then
we use k^2 bins in the secondary hash table. We have already shown in Lemma15.10
that with k^2 bins we can find a hash function from a 2-universal family that will give
no collisions. It remains to show that, by carefully choosing the first hash function, we
can guarantee that the total space used by the algorithm is only O ( m ).

Theorem 15.11: The two-level approach gives a perfect hashing scheme for m items
using O ( m ) bins.

Proof: As we showed in Lemma15.10, the number of collisions X in the first stage
satisfies

Pr
(
X ≥
m^2
n
)
≤Pr( X ≥ 2 E [ X ])≤
1
2
.
When n = m , this implies that the probability of having more than m collisions is at
most 1/2. Using the probabilistic method, there exists a choice of hash function from
the 2-universal family in the first stage that gives at most m collisions. In fact, such
a hash function can be found efficiently by trying hash functions chosen uniformly at
random from the 2-universal family, giving a Las Vegas algorithm. We may therefore
assume that we have found a hash function for the first stage that gives at most m
collisions.
Let ci be the number of items in the i th bin. Then there are

( ci
2
)
collisions between
items in the i th bin, so

∑ m
i = 1
(
ci
2
)
≤ m.
For each bin with ci >1 items, we find a second hash function that gives no collisions
using space c^2 i. Again, for each bin, this hash function can be found using a Las Vegas
algorithm. The total number of bins used is then bounded above by

m +
∑ m
i = 1
c^2 i ≤ m + 2
∑ m
i = 1
(
ci
2
)
+
∑ m
i = 1
ci ≤ m + 2 m + m = 4 m.
Hence, the total number of bins used is only O ( m ).  /theta

15.4 Application: Finding Heavy Hitters in Data Streams
15.4. Application: Finding Heavy Hitters in Data Streams
A router forwards packets through a network. At the end of the day, a natural question
for a network administrator to ask is whether the number of bytes traveling from a
source s to a destination d that have passed through the router is larger than a pre-
determined threshold value. We call such a source–destination pair a heavy hitter.
When designing an algorithm for finding heavy hitters, we must keep in mind the
restrictions of the router. Routers have very little memory and so cannot keep a count
for each possible pair s and d , since there are simply too many such pairs. Also, routers
must forward packets quickly, so the router must perform only a small number of
computational operations for each packet. We present a randomized data structure
that is appropriate even with these limitations. The data structure requires a thresh-
old q ; all source–destination pairs that are responsible for at least q total bytes are
considered heavy hitters. Usually q is some fixed percentage, such as 1%, of the total
expected daily traffic. At the end of the day, the data structure gives a list of possi-
ble heavy hitters. All true heavy hitters (responsible for at least q bytes) are listed, but
some other pairs may also appear in the list. Two other input constants,εandδ,are
used to control what extraneous pairs might be put in the list of heavy hitters. Sup-
pose that Q represents the total number of bytes over the course of the day. Our data
structure has the guarantee that any source–destination pair that constitutes less than
q −ε Q bytes of traffic is listed with probability at mostδ. In other words, all heavy
hitters are listed; all pairs that are sufficiently far from being a heavy hitter are listed
with probability at mostδ; pairs that are close to heavy hitters may or may not be
listed.
This router example is typical of many situations where one wants to keep a succinct
summary of a large data stream. In most data stream models, large amounts of data
arrive sequentially in small blocks, and each block must be processed before the next
block arrives. In the setting of network routers, each block is generally a packet. The
amount of data being handled is often so large and the time between arrivals is so
small that algorithms and data structures that use only a small amount of memory and
computation per block are required.
We can use a variation of a Bloom filter, discussed in Section5.5.3, to solve this
problem. Unlike our solution there, which assumed the availability of completely ran-
dom hash functions, here we obtain strong, provable bounds using only a family of
2-universal hash functions. This is important, because efficiency in the router setting
demands the use of only very simple hash functions that are easy to compute, yet at the
same time we want provable performance guarantees.
We refer to our data structure as a count-min filter. The count-min filter processes
a sequential stream of pairs X 1 , X 2 ,...of the form Xt =( it , ct ), where it is an item
and ct >0 is an integer count increment. In our routing setting, it would be the pair of
source–destination addresses of a packet and ct would be the number of bytes in the
packet. Let

Count( i , T )=
∑
t : it = i , 1 ≤ t ≤ T
ct.
pairwise independence and universal hash functions
That is, Count( i , T ) is the total count associated with an item i up to time T. In the
routing setting, Count( i , T ) would be the total number of bytes associated with packets
with an address pair i up to time T. The count-min filter keeps a running approximation
of Count( i , T ) for all items i and all times T in such a way that it can track heavy hitters.
A count-min filter consists of m counters. We assume henceforth that our coun-
ters have sufficiently many bits that we do not need to worry about overflow; in many
practical situations, 32-bit counters will suffice and are convenient for implementation.
A count-min filter uses k hash functions. We split the counters into k disjoint groups
G 1 , G 2 ,..., Gk of size m / k. For convenience, we assume in what follows that k divides
m evenly. We label the counters by Ca , j , where 1≤ a ≤ k and 0≤ j ≤ m / k −1, so that
Ca , j corresponds to the j th counter in the a th group. That is, we can think of our counters
as being organized in a 2-dimensional array, with m / k counters per row and k columns.
Our hash functions should map items from the universe into counters, so we have hash
functions Ha for 1≤ a ≤ k , where Ha : U →[0, m / k −1]. That is, each of the k hash
functions takes an item from the universe and maps it into a number [0, m / k −1].
Equivalently, we can think of each hash function as taking an item i and mapping it to
the counter Ca , Ha ( i ). The Ha should be chosen independently and uniformly at random
from a 2-universal hash family.
We use our counters to keep track of an approximation of Count( i , T ). Initially, all
the counters are set to 0. To process a pair ( it , ct ), we compute Ha ( it ) for each a with
1 ≤ a ≤ k and increment Ca , Ha ( it )by ct. Let Ca , j ( T ) be the value of the counter Ca , j after
processing X 1 through XT. We claim that, for any item, the smallest counter associated
with that item is an upper bound on its count, and with bounded probability the smallest
counter associated with that item is off by no more thanεtimes the total count of all the
pairs ( it , ct ) processed up to that point. Specifically, we have the following theorem.

Theorem 15.12: For any i in the universe U and for any sequence
( i 1 , c 1 ),...,( iT , cT ) ,
min
j = Ha ( i ), 1 ≤ a ≤ k

Ca , j ( T )≥Count( i , T ).
Furthermore, with probability 1 −( k / m ε) kover the choice of hash functions,

min
j = Ha ( i ), 1 ≤ a ≤ k
Ca , j ( T )≤Count( i , T )+ε
∑ T
t = 1
ct.
Proof: The first bound,

j = H min
a ( i ),^1 ≤ a ≤ k
Ca , j ( T )≥Count( i , T ),
is trivial. Each counter Ca , j with j = Ha ( i ) is incremented by ct when the pair ( i , ct )is
seen in the stream. It follows that the value of each such counter is at least Count( i , T )
at any time T.
For the second bound, consider any specific i and T. We first consider the specific
counter C 1 , H 1 ( i )and then use symmetry. We know that the value of this counter is at
least Count( i , T ) after the first T pairs. Let the random variable Z 1 be the amount the
counter is incremented owing to items other than i. Let Xt be a random variable that is

15.4 application: finding heavy hitters in data streams
1if it = i and H 1 ( it )= H 1 ( i ); Xt is 0 otherwise. Then

Z 1 =
∑
tH :1≤ t ≤ T , it = i
1 ( it )= H 1 ( i )
ct =
∑ T
t = 1
Xtct.
Because H 1 is chosen from a 2-universal family, for any it = i we have

Pr( H 1 ( it )= H 1 ( i ))≤
k
m
and hence

E [ Xt ]≤
k
m
.
It follows that

E [ Z 1 ]= E
[ T
∑
t = 1
Xtct
]
=
∑ T
t = 1
ct E [ Xt ]≤
k
m
∑ T
t = 1
ct.
By Markov’s inequality,
Pr
(
Z 1 ≥ε
∑ T
t = 1
ct
)
≤
k / m
ε
=
k
m ε
. (15.2)
Let Z 2 , Z 3 ,..., Zk be corresponding random variables for each of the other hash func-
tions. By symmetry, all of the Zi satisfy the probabilistic bound of Eqn. (15.2). More-
over, the Zi are independent, since the hash functions are chosen independently from
the family of hash functions. Hence

Pr
(
k
min
j = 1
Zj ≥ε
∑ T
t = 1
ct
)
=
∏ k
j = 1
Pr
(
Zj ≥ε
∑ T
t = 1
ct
)
(15.3)
≤
(
k
m ε
) k
. (15.4)

 /theta
It is easy to check using calculus that ( k / m ε) k is minimized when k = m ε/e, in which
case
(
k
m ε

) k
=e− m ε/e.
Of course, k needs to be chosen so that k and m / k are integers, but this does not sub-
stantially affect the probability bounds.
We can use a count-min filter to track heavy hitters in the routing setting as follows.
When a pair ( iT , cT ) arrives, we update the count-min filter. If the minimum hash value
associated with iT is at least the threshold q for heavy hitters, then we put the item
into a list of potential heavy hitters. We do not concern ourselves with the details of
performing operations on this list, but note that it can be organized to allow updates

pairwise independence and universal hash functions
and searches in time logarithmic in its size by using standard balanced search-tree data
structures; alternatively, it could be organized in a large array or a hash table.
Recall that we use Q to represent the total traffic at the end of the day.

Corollary 15.13: Suppose that we use a count-min filter with k =

⌈
ln^1 δ
⌉
hash func-
tions, m =

⌈
ln^1 δ
⌉
·
⌈e
ε
⌉
counters, and a threshold q. Then all heavy hitters are put on
the list, and any source–destination pair that corresponds to fewer than q −ε Q bytes
is put on the list with probability at most δ.

Proof: Since counts increase over time, we can simply consider the situation at the
end of the day. By Theorem15.12, the count-min filter will ensure that all true heavy
hitters are put on the list, since the smallest counter value for a true heavy hitter will
be at least q. Further, by Theorem15.12, the smallest counter value for any source–
destination pair that corresponds to fewer than q −ε Q bytes reaches q with probability
at most
(
k
m ε

) k
≤e−ln(1/δ)=δ.  /theta
The count-min filter is very efficient in terms of using only limited randomness in its
hash functions, only O

( 1
εln
1
δ
)
counters, and only O
(
ln^1 δ
)
computations to process
each item. (Additional computation and space might be required to handle the list of
potential heavy hitters, depending on its representation.)
Before ending our discussion of the count-min filter, we describe a simple improve-
ment known as conservative update that often works well in practice, although it is
difficult to analyze. When a pair ( it , ct ) arrives, our original count-min filter adds ct to
each counter Ca , j that the item i hashes to, thereby guaranteeing that

min
j = Ha ( i ), 1 ≤ a ≤ k
Ca , j ( T )≥Count( i , T )
holds for all i and T. In fact, this can often be guaranteed without adding ct to each
counter. Consider the state after the ( t −1)th pair has been processed. Suppose that,
inductively, up to that point we have, for all i ,

min
j = Ha ( i ), 1 ≤ a ≤ k
Ca , j ( t −1)≥Count( i , t −1).
Then, when ( it , ct ) arrives, we need to ensure that

Ca , j ( t )≥Count( it , t )
for all counters, where j = Ha ( it ), a ≤ 1 ≤ k .But

Count( it , t )=Count( it , t −1)+ ct ≤ min
j = Ha ( it ), 1 ≤ a ≤ k
Ca , j ( t −1)+ ct.
Hence we can look at the minimum counter valuevobtained from the k counters that
it hashes to, add ct to that value, and increase tov+ ct any counter that is smaller than
v+ ct. An example is given in Figure15.1. An item arrives with a count of 3; at the
time of arrival, the smallest counter associated with the item has value 4. It follows that
the count for this item is at most 7, so we can update all associated counters to ensure
they are all at least 7. In general, if all the counters it hashes to are equal, conservative

15.5 Exercises
Figure 15.1: An item comes in, and 3 is to be added to the count. The initial state is on the left;
the shaded counters need to be updated. Using conservative update, the minimum counter value 4
determines that all corresponding counters need to be pushed up to at least 4+ 3 =7. The resulting
state after the update is shown on the right.

update is equivalent to just adding ct to each counter. When the it are not all equal, the
conservative update improvement adds less to some of the counters, which will tend to
reduce the errors that the filter produces.

15.5. Exercises
Exercise 15.1: A fair coin is flipped n times. Let Xij , with 1≤ i < j ≤ n , be 1 if the
i th and j th flip landed on the same side; let Xij =0 otherwise. Show that the Xij are
pairwise independent but not independent.

Exercise 15.2: (a) Let X and Y be numbers that are chosen independently and uni-
formly at random from{ 0 , 1 ,..., n }. Let Z be their sum modulo n +1. Show that X ,
Y , and Z are pairwise independent but not independent.
(b) Extend this example to give a collection of random variables that are k -wise
independent but not ( k +1)-wise independent.

Exercise 15.3: For any family of hash functions from a finite set U to a finite set V ,
show that, when h is chosen at random from that family of hash functions, there exists
a pair of elements x and y such that

Pr( h ( x )= h ( y ))≥
1
| V |
−
1
| U |
.
This result should not depend on how the function h is chosen from the family.

Exercise 15.4: Show that, for any discrete random variable X that takes on values in
the range [0,1], Var [ X ]≤ 1 /4.

Exercise 15.5: Suppose we have a randomized algorithm Test for testing whether a
string appears in a language L that works as follows. Given an input x , the algorithm
Test chooses a random integer r uniformly from the set S ={ 0 , 1 ,..., p − 1 }for some
prime p .If x is in the language, then Test( x , r )=1 for at least half of the possible
values of r. A value of r such that Test( x , r )=1 is called a witness for x .If x is not in
the language, then Test( x , r )=0 always.

pairwise independence and universal hash functions
If we run the algorithm Test twice on an input x ∈ L by choosing two numbers r 1
and r 2 independently and uniformly from S and evaluating Test( x , r 1 ) and Test( x , r 2 ),
then we find a witness with probability at least 3/4. Argue that we can obtain a witness
with probability at least 1− 1 / t using the same amount of randomness by letting si =
r 1 i + r 2 mod p and evaluating Test( x , si ) for values 0≤ i ≤ t < p.

Exercise 15.6: Our analysis of Bucket sort in Section5.2.2assumed that n elements
were chosen independently and uniformly at random from the range [0, 2 k ). Suppose
instead that n elements are chosen uniformly from the range [0, 2 k ) in such a way that
they are only pairwise independent. Show that, under these conditions, Bucket sort still
requires linear expected time.

Exercise 15.7: (a) We have shown that the maximum load when n items are hashed
into n bins using a hash function chosen from a 2-universal family of hash functions is
at most 1+

√
2 n with probability at least 1/2. Generalize this argument to k -universal
hash functions. That is, find a value such that the probability that the maximum load is
larger than that value is at most 1/2.
(b) In Lemma5.1we showed that, under the standard balls-and-bins model, the
maximum load when n balls are thrown independently and uniformly at random into
n bins is at most 3 ln n /ln ln n with probability 1− 1 / n. Find the smallest value of k
such that the maximum load is at most 3 ln n /ln ln n with probability at least 1/2 when
choosing a hash function from a k -universal family.

Exercise 15.8: We can generalize the problem of finding a large cut to finding a large
k -cut. A k -cut is a partition of the vertices into k disjoint sets, and the value of a cut
is the weight of all edges crossing from one of the k sets to another. In Section15.1.2
we considered 2-cuts when all edges had the same weight 1, and we showed how to
derandomize the standard randomized algorithm using collections of n pairwise inde-
pendent bits. Explain how this derandomization could be generalized to obtain a poly-
nomial time algorithm for 3-cuts, and give the running time for your algorithm. ( Hint:
You may want to use a hash function of the type found in Section15.3.2.)

Exercise 15.9: Suppose we are given m vectors ̄v 1 ,v ̄ 2 ,...,v ̄ m ∈{ 0 , 1 } nsuch that any
k of the m vectors are linearly independent modulo 2. Let ̄v i =(v i , 1 ,v i , 2 ,...,v i , n). Let
u ̄be chosen uniformly at random from{ 0 , 1 } n, and let Xi =

∑ n
j = 1 v i , juj mod 2. Show
that the Xi are uniform, k -wise independent bits.

Exercise 15.10: We examine a specific way in which 2-universal hash functions differ
from completely random hash functions. Let S ={ 0 , 1 , 2 ,..., k }, and consider a hash
function h with range{ 0 , 1 , 2 ,..., p − 1 }for some prime p much larger than k. Con-
sider the values h (0), h (1),..., h ( k ). If h is a completely random hash function, then
the probability that h (0) is smaller than any of the other values is roughly 1/( k +1).
(There may be a tie for the smallest value, so the probability that any h ( i ) is the unique
smallest value is slightly less than 1/( k +1).) Now consider a hash function h chosen

15.5exercises
uniformly from the family
H={ ha , b | 0 ≤ a , b ≤ p − 1 }
of Section15.3.2. Estimate the probability that h (0) is smaller than h (1),..., h ( k )by
randomly choosing 10,000 hash functions from h and computing h ( x ) for all x ∈ S.
Run this experiment for k =32 and k =128, using primes p = 5 , 023 ,309 and p =
10 , 570 ,849. Is your estimate close to 1/( k +1)?
Exercise 15.11: In a multi-set, each element can appear multiple times. Suppose that
we have two multi-sets, S 1 and S 2 , consisting of positive integers. We want to test if
the two sets are the “same” – that is, if each item appears the same number of times in
each set. One way of doing this is to sort both sets and then compare the sets in sorted
order. This takes O ( n log n ) time if each multi-set contains n elements.
(a) Consider the following algorithm. Hash each element of S 1 into a hash table with
cn counters; the counters are initially 0, and the i th counter is incremented each
time the hash value of an element is i. Using another table of the same size and
using the same hash function, do the same for S 2. If the i th counter in the first table
matches the i th counter in the second table for all i , report that the sets are the same,
and otherwise report that the sets are different.
Analyze the running time and error probability of this algorithm, assuming that
the hash function is chosen from a 2-universal family. Explain how this algorithm
can be extended to a Monte Carlo algorithm, and analyze the trade-off between its
running time and its error probability.
(b) We can also design a Las Vegas algorithm for this problem. Now each entry in the
hash table corresponds to a linked list of counters. Each entry holds a list of the
number of occurrences of each element that hashes to that location; this list can be
kept in sorted order. Again, we create a hash table for S 1 and a hash table for S 2 ,
and we test after hashing if the resulting tables are equal.
Argue that this algorithm requires only linear expected time using only linear
space.

Exercise 15.12: In Section15.3.1we showed that the family
H={ ha , b | 1 ≤ a ≤ p − 1 , 0 ≤ b ≤ p − 1 }
is 2-universal when p ≥ n , where
ha , b ( x )=(( ax + b )mod p )mod n.
Consider now the hash functions
ha ( x )=( ax mod p )mod n
and the family
H′={ ha | 1 ≤ a ≤ p − 1 }.
pairwise independence and universal hash functions
Give an example to show thatH′is not 2-universal. Then prove thatH′is almost 2-
universal in the following sense: for any x , y ∈{ 0 , 1 , 2 ,..., p − 1 },if h is chosen uni-
formly at random fromH′then

Pr( h ( x )= h ( y ))≤
2
n
.
Exercise 15.13: In describing count-min filters, we assumed that the data stream con-
sisted of pairs of the form ( it , ct ), where it was an item and ct >0 an integer count
increment. Suppose that one were also allowed to decrement the count for an item, so
that the stream could include pairs of the form ( it , ct ) with ct <0. We could require
that the total count for an item i ,

Count( i , T )=
∑
t : it = i , 1 ≤ t ≤ T
ct ,
always be positive.
Explain how you could modify or otherwise use count-min filters to find heavy hit-
ters in this situation.

chapter sixteen

Power Laws and

Related Distributions

In this chapter, we explore some additional basic probability distributions that arise
in a number of computer science applications. One family of distributions we focus
on are called power law distributions. An interesting aspect of these distributions is
that, unlike many of the distributions we have seen, the variance of the distribution can
be extremely large – with some natural choices of parameters, the variance is infinite.
As a result, certain methods we usually rely on in probabilistic arguments, such as
concentration of the sum of random variables, may not apply.
Power laws and related distributions may initially appear surprising or unusual, but
in fact they are quite natural, and arise easily from a number of basic models. We
examine some of these models in the course of the chapter. Power laws may contrast
sharply with other distributions we have seen, such as Gaussian distributions, which
also appear quite frequently in real-world settings, but both types of distributions have
their uses and their place.
As some groundwork, suppose we want to consider the average height of women
in the United States. We could take a random sample of women, and we would expect
that a fairly small number of samples would quickly lead to a good estimate. (The U.S.
Census Bureau publishes data on height distribution; currently, the average woman’s
height is somewhere between 5’ 4” and 5’ 5”, although the range depends on the age
group you are considering.) This is because heights fall in a narrow range, with the
number of people of a certain height falling very quickly as you move away from
the average – very few women are more than 7 feet tall. On the other hand, suppose
we wanted to find the average number of times a word appears in all the books printed
in the United States in a year. Some common words, such as “the”, “of”, and “an”,
appear remarkably frequently, while most words would only appear at most a handful
of times. In fact, the distribution of words in literature has been studied in some detail,
and has been found to roughly follow a power law distribution. We consider later some
proposed arguments for why that might naturally be the case.
Many other phenomena share this property that the corresponding distribution is
not well concentrated around its mean, such as the sizes of cities, the strength of

power laws and related distributions
earthquakes, and the distribution of wealth among families. For many such examples,
a power law distribution provides a plausible model for the distribution.

16.1 Power Law Distributions: Basic Definitions and Properties
Before defining a power law distribution, it may help to give an example. A Pareto
distribution with parametersα>0 and minimum value m >0 satisfies

Pr( X ≥ x )=
(
x
m
)−α
.
Here the minimum value m appropriately satisfies Pr( X ≥ m )=1. The valueαis
sometimes called the tail index. Correspondingly, the density function for the Pareto
distribution is

f ( x )=α m α x −α−^1.
Let us try to examine the moments of this random variable. The mean E [ X ]isgiven
by

E [ X ]=
∫∞
x = m
xf ( x ) dx
=
∫∞
x = m
x (α m α x −α−^1 ) dx
=α m α
∫∞
x = m
x −α dx.
We already notice something unusual; the mean is not finite whenα≤1, as the integral
in the expression above diverges. Forα>1, we can complete the calculation to find
the mean is given by

E [ X ]=
α m
α− 1
.
If we look at the j th moment E [ Xj ], we have
E [ Xj ]=
∫∞
x = m
xjf ( x ) dx
=
∫∞
x = m
xj (α m α x −α−^1 ) dx
=α m α
∫∞
x = m
xj −^1 −α dx.
The j th moment is not finite whenα≤ j ;forα> j ,wehave

E [ Xj ]=
α mj
α− j
.
So, for example, whenα≤2, the second moment is infinite. Correspondingly, the vari-
ance is infinite when 1<α≤2; forα≤1 since both the first and second moments are
infinite the variance is not well-defined.

16.1power law distributions: basic definitions and properties
More generally, we say that a nonnegative random variable X is said to have a power
law distribution if

Pr( X ≥ x )∼ cx −α
for constants c >0 andα>0. Here f ( x )∼ g ( x ) represents that the limit of the ratio of
f ( x ) and g ( x ) converges to 1 as x grows large. Roughly speaking, a power law distribu-
tion asymptotically behaves like a Pareto distribution. It is worth noting that the term is
sometimes used slightly differently in other contexts. For example, sometimes people
use power law distribution interchangeably with Pareto distribution, and refer to what
we have called a power law distribution as an asymptotic power law distribution. Also,
sometimes people useα+1 in the definition where we have usedα. (This convention
yields that the density function, rather than the complementary cumulative distribution
function, has parameterα.) Finally, sometimes one allows the ratio to converge not to
1, but to some slowly growing function.
A power law is best visualized on what is called a log–log plot , where both axes
are presented using logarithmic scales. On a log–log plot the relationship y = axb is
shown by presenting ln y = b ln x +ln a , so that the polynomial relationship appears as
a straight line whose slope depends on the exponent b. (Here we use a natural logarithm
for our log–log plot, but we could use any base for the logarithm and still obtain a
straight line.) For a Pareto distribution with parametersα>0 and m >0, a log–log
plot of F ̄( x )=Pr( X ≥ x ) (which we recall is called the complementary cumulative
distribution function) therefore follows a straight line:

ln F ̄( x )=−αln x +αln m.
More generally, if X has a power law distribution, then in a log–log plot of the comple-
mentary cumulative distribution function, asymptotically the behavior will be a straight
line. This provides a simple empirical test for whether a random variable may behave
according to a power law given an appropriate sample; while a nearly straight line does
not guarantee a power law distribution, if the results are far from a straight line, a power
law is unlikely. (It is important to emphasize that the “straight-line” test on a log–log
plot is sometimes used to infer that a sample arises from a distribution that follows a
power law, but because many other distributions produce nearly linear outcomes on a
log–log plot, one must take more care to test for power laws.) On a log–log plot the
density function for the Pareto distribution also is a straight line:

ln f ( x )=(−α−1)ln x +αln m +lnα.
Similarly, asymptotically the density function for a power law will approach a straight
line.
Thus far we have focused on the mathematical definitions for continuous power
law distributions. But we could also consider discrete variations. For example, the zeta
distribution with parameter s >1 is defined for all positive integer values according to

Pr( X = k )=
k − s
ζ( s )
,
power laws and related distributions
where the Riemann zeta functionζ( s ) is given byζ( s )=

∑∞
j = 1 j
− s. The fact that
Pr( X = k ) is proportional to k − s is the natural discrete analogue for a power law
distribution.

16.2 Power Laws in Language
16.2.1 Zipf’s Law and Other Examples
It has long been observed that the distribution of word frequencies appears to follow
a power law. That is, the frequency of the k th most frequent word in a language is
roughly proportional to k − s for some exponent s , so the frequency distribution is well
modeled by a (discrete) power law. In terms of probability, this means that if you choose
a word uniformly at random from a collection of texts, the k th most frequent word will
be chosen with probability roughly proportional to k − s. Equivalently, the rank of the
frequency of a randomly chosen word is well modeled by a zeta distribution. For several
languages, the value of s is close to 1.
This empirical observation is commonly referred to as Zipf’s law, as the linguist
George Zipf popularized it in the first half of the twentieth century, although it appears
to have been noticed by others before Zipf. One sometimes also sees reference to the
Zipf–Mandelbrot law, which offers the generalization that the frequency of the k th most
frequent word in a language is roughly proportional to ( k + q )− s for some exponent s
and some constant q. Also, while one usually hears Zipf’s law in reference to languages,
sometimes Zipf’s law is used to refer to other natural occurences where the frequency
of ranked items of a collection of objects follows a power law distribution.
Is there a mathematical framework that might explain the power law behavior of
languages? Below, we consider multiple models that lead to power laws, and might
apply. (However, we encourage healthy skepticism regarding whether any particular
simple mathematical model offers a complete or compelling explanation for Zipf’s
law.)
Before continuing, it is worth noting that historically power laws have been observed
in a variety of fields in the natural and social sciences, not merely in languages. Perhaps
the earliest references appear in the work by Vilfredo Pareto, whose name we have
seen in the context of the Pareto distribution; he introduced this distribution around
1897 to describe the distribution of income. Over a century ago the German physicist
Felix Auerbach first suggested that city sizes appear to follow a power law distribution,
although Zipf is often credited with this finding as well. Alfred Lotka in the 1920s
found in examining the number of articles produced by chemists that the distribution
followed a power law; the finding that publication frequency follows a power law is
commonly called Lotka’s Law. The Richter scale for measuring the magnitude of an
earthquake is based on the Gutenberg–Richter law, which says that the relationship
between the frequency of earthquakes and their magnitude follows a power law. The
historical record shows that several natural occurrences of power laws have been known
for a long time; it is therefore not especially surprising that power laws would also
naturally occur in several places in computer science and engineering.

16.2power laws in language
16.2.2 Languages via Optimization
One argument for power laws arising in the context of word frequencies arises naturally
under a framework where the goal is to maximize the efficiency of the language in a
specific sense. Consider a language consisting of n words. Let the cost of using the k th
most frequent word of the language be Ck. For example, if we think of English text, the
cost of a word might be thought of as the number of letters plus the additional cost of
a space. This is the cost to write the word, and (at least roughly) might correspond to
the cost of speaking it.
If the alphabethas size d , then we have dj possible words of length j .We
would naturally assign more frequent words to shorter strings to reduce the cost.
With such an assignment, the words with frequency ranks 1+( dj −1)/( d −1) to
( dj +^1 −1)/( d −1) have j letters. As a simplification, the length of the k th most fre-
quent word would be approximately log dk , and hence a natural cost function has
Ck ∼log dk. For convenience, let us take Ck =log dk in what follows.
Now suppose that we had the power to design the frequencies of words in our
language in order to maximize the ratio between the average amount of information
obtained for each word written and the corresponding average cost. This seems like
a natural goal. We first need to quantify what we mean by information. The natural
choice to use here is entropy. (See Chapter 10 .) That is, we now think of each word
used as being selected randomly according to a probability distribution, and the proba-
bility that the k th most frequent word in the language is chosen is pk. Then the average
information given per word is the entropy H =−

∑ n
k = 1 pk log 2 pk , and the average cost
per word is C =
∑ n
k = 1 pkCk. The question is how would the pk be chosen to maximize
the ratio R = H / C. It is slightly easier to look at the equivalent problem of minimizing
the ratio A = C / H.
Taking derivatives, we find
dA
dpk
=
CkH + C log 2 (e pk )
H^2
Hence all the derivatives are 0 (and A is in fact minimized) when
pk = 2 − HCk / C /e= 2 − RCk /e.
Using Ck =log dk ,wefind
pk = 2 − R log dk /e= k − R log d^2 /e.
That is, regardless of what the optimal value R turns out to be, the corresponding pk
fall according to a power law distribution.
16.2.3 Monkeys Typing Randomly
The optimization argument appears quite compelling. However, another argument
shows that power law word frequency distributions can arise even without an underly-
ing optimization.
Consider the following experiment. A monkey types randomly on a keyboard with
d characters and a space bar. A space is hit with probability q ; all other characters are
power laws and related distributions
hit with equal probability (1− q )/ d. A space is used to separate words. We consider
the frequency distribution of words.
It is clear that as the monkey types, each word with j characters occurs with proba-
bility

qj =
(
1 − q
d
) j
q ,
and there are dj words of length j. (We allow the empty word of length 0 for con-
venience.) The words of longer length are less likely and hence occur lower in the
rank order of word frequency. In particular, the words with frequency ranks 1+( dj −
1)/( d −1) to ( dj +^1 −1)/( d −1) have j letters. Hence, the word with frequency rank
k = dj has length j =log dk and occurs with probability

pk =
(
1 − q
d
)log dk
q = k log d (1− q )−^1 q.
For k = dj , as in Section16.2.2it is reasonable to use as an approximation that the
length of the k th most frequent word is log dk , in which case pk ≈ k log d (1− q )−^1 q , and
the power law behavior is apparent.^1
The power law associated with word frequency, although it naturally arises from
optimization, does not seem to actually require it. This result serves as something of a
warning; there are multiple ways that power law distributions can arise. Indeed, we next
turn to one of the most frequently used models that leads to power law distributions,
preferential attachment.

16.3 Preferential Attachment
To describe preferential attachment, let us work with a very simple model of the World
Wide Web. The World Wide Web conists of web pages and directed hyperlinks from
one page to another. The World Wide Web can naturally be thought of as a graph, with
pages corresponding to vertices and hyperlinks corresponding to directed edges. The
graph grows and changes as pages and links are added to the Web.
Our model of the Web’s growth will be very basic; our goal is not detailed accu-
racy, but a high-level understanding of what might be happening. Let us start with two
pages, each linking to the other; the starting configuration does not make a substantial
difference, so this configuration is chosen for convenience. At each time step, a new
page appears, with just a single link. (One could try to be more accurate by having
multiple links or a distribution on links, but having a single link per page simplifies our

(^1) The attentive reader might note that technically the result above does not quite match a power law as we have
defined it; the appropriate limit does not tend to 1 but is bounded above and below by a constant, because instead
of a steady decrease in the frequency with the rank there are discrete jumps. That is, instead of taking a power
law to be defined by Pr( X ≥ x )∼ cx −α, we instead have a case here where Pr( X ≥ x )is /eta( x −α). This is a minor
point; small amounts of noise in the frequency of how individual letters are chosen would lead to a smoother
behavior. Also, in some settings, random variables where Pr( X ≥ x )is /eta( x −α) are referred to as power laws.

16.3preferential attachment
analysis and yields the important insights.) How should we model what page the new
link points to?
The idea behind preferential attachment is that new links will tend to attach to popu-
lar pages. In the case of the Web graph, new links tend to go to pages that already have
links. We can model this by thinking of the new page as copying a random link, with
some probability. Specifically, with probabilityγ<1, the link for the new page points
to a page chosen uniformly at random, but with probability 1−γ, the new page copies
a random link, so that the new page points to an existing page chosen proportionally to
the indegree of that page. We point out that our preferential attachment model of the
World Wide Web is a Markov chain, as we do not care about the history of how links
attached when a new link is added. We only care about the number of links directed
into each page.
Let us start with a not entirely rigorous argument that provides the intuition for how
this model behaves. Let Xj ( t ) (or just Xj where the meaning is clear) be the number
of pages with indegree j when there are t pages in the system. Then for j ≥1 the
probability that Xj increases is just

γ Xj − 1 ( t )/ t +(1−γ)( j −1) Xj − 1 ( t )/ t ;
the first term is the probability a new link is chosen at random and chooses a page
with indegree j −1, and the second term is the probability that a new link is chosen
proportionally to the indegrees and chooses a page with indegree j −1. Similarly, the
probability that Xj decreases is

γ Xj ( t )/ t +(1−γ) jXj ( t )/ t.
Let us write( Xj ( t )) for Xj ( t +1)− Xj ( t ). Then we can write

E [( Xj ( t ))| Xj − 1 ( t ), Xj ( t )]=
γ Xj − 1 ( t )
t
+
(1−γ)( j −1) Xj − 1 ( t )
t
−
γ Xj ( t )
t
−
(1−γ) jXj ( t )
t
. (16.1)
The case of X 0 must be treated specially, since each new page introduces a new
vertex of indegree 0. Hence

E [( X 0 ( t ))| X 0 ( t )]= 1 −
γ X 0 ( t )
t
. (16.2)
While not required for our analysis, it is worth observing that for any value k , the
vector of values ( X 0 ( t ), X 1 ( t ),..., Xk ( t )) is also a Markov chain.
Now as t grows large, we would suspect that pages of indegree j will constitute a
fraction cj of the total pages. Suppose that in the limit as t grows large Xj ( t )= cj · t ,
with high probability. What values of cj are consistent with this family of equations?
With these assumptions, we can solve for the appropriate cj , successively starting from
c 0. In this case, E [( X 0 ( t ))| X 0 ( t )]= c 0 , because on average X 0 must increase by c 0
per new page in order for X 0 ( t )togrowlike c 0 t. Hence, Eqn. (16.2) becomes

c 0 = 1 −γ c 0 ,
power laws and related distributions
so c 0 = 1 +^1 γ. More generally, we find using Eqn. (16.1) that

cj =γ cj − 1 +(1−γ)( j −1) cj − 1 −γ cj −(1−γ) jcj. (16.3)
This gives the following recurrence for cj.

cj = cj − 1
(γ+( j −1)(1−γ))
(1+γ+ j (1−γ))
. (16.4)
This is enough for us to find the values for cj explicitly. If we focus on the asymp-
totics, we find that for large j

cj
cj − 1
= 1 −
2 −γ
1 +γ+ j (1−γ)
∼ 1 −
(
2 −γ
1 −γ
)(
1
j
)
.
Asymptotically, for the above to hold we have cj ∼ j −

21 −−γγ
, giving a power law. To see
this, we observe that cj ∼ j −

21 −−γγ
implies
cj
cj − 1
∼
(
j − 1
j
)^21 −−γγ
∼ 1 −
(
2 −γ
1 −γ
)(
1
j
)
16.3.1 A Formal Version
We can formalize the above argument for preferential attachment by using martingales.
We first show that it is enough to find the expectations of the degrees for the preferen-
tial attachment process, as the number of vertices of each degree can be placed in the
framework of a Doob martingale, allowing us to utilize Azuma–Hoeffding tail bounds.
We then formalize how to compute the corresponding expectations.
Consider the preferential attachment process described previously, until there are
T pages overall. To be clear, we start with two pages, pointing at each other. We let
Zi refer to the random variables corresponding to the choices made when there are i
pages in the system to determine the page linked to by the ( i +1)st page. Let Xj , t be
a random variable corresponding to the number of pages of degree j when there are t
pages. Further, for t ≥2, let Yj , t = E [ Xj , T | Z 2 , Z 3 ,..., Zt − 1 ].

Lemma 16.1: Then

Pr(| Xj , T − E [ Xj , T ]|≥λ)≤2e−λ
(^2) /(8 T )
.
Proof: As mentioned, we show that the Yj , t form a Doob martingale with
| Yj , t − Yj , t + 1 |≤ 2 ;
then the result follows immediately from Theorem13.6. (We could replace the T by
T −2 in the denominator, as there are only T −2 time steps, but this notation is eas-
ier.) The sequence is clearly a Doob martingale, obtained by revealing the choices at
each step. To bound the difference| Yj , t − Yj , t + 1 |, we claim that the choice correspond-
ing to Zt affects the number of vertices with degree j at the end of the process by at
most 2. Specifically, if a vertexv 1 is chosen to receive a link at the t th step, and we
instead consider what would have happened if another vertexv 2 had been chosen, the

16.3preferential attachment
only vertex degrees affected are those ofv 1 andv 2. To see this, consider the evolu-
tion of graphs G 1 and G 2 arising after choosingv 1 orv 2 , respectively, at the t th step.
If the next step creates a link to a random vertex (the same random vertex in both
graphs, as the Zt + 1 would be the same), the degree of every vertex besidesv 1 andv 2
remains the same in G 1 and G 2. Similarly, consider if the next step creates a link by
copying a random link; that is, Zt + 1 says to copy the link created at the nth step of the
process for some n.If n= t , then again the degree of every vertex besidesv 1 andv 2
clearly remains the same in G 1 and G 2 , since the same vertex receives a new link. If
 n= t , thenv 1 obtains an extra link in G 1 andv 2 obtains an extra link in G 2 ;however,
this only affects the degrees of verticesv 1 andv 2 , so the bound on| Yj , t − Yj , t + 1 |still
holds.  /theta

It remains to determine the expectations E [ Xj , T ]. We start with E [ X 0 , t ].
Lemma 16.2: For t ≥ 2_._

1
1 +γ
−
2
t
≤
E [ X 0 , t ]
t
≤
1
1 +γ
.
Proof: It follows from Eqn. (16.2), Theorem2.7, and the linearity of expectations that

E [ X 0 , t + 1 ]= E [ X 0 , t ]+ 1 −
γ E [ X 0 , t ]
t
= 1 +
(
1 −
γ
t
)
E [ X 0 , t ]. (16.5)
Because we started with two pages pointing to each other for convenience, we have the
initial condition E [ X 0 , 2 ]=0.
For t ≥2, let

δ( t )=
1
1 +γ
t − E [ X 0 , t ].
From Eqn. (16.5), it is easy to calculateδ(2)= 1 +^2 γandδ(3)=^21 −+γγ. More generally,

δ( t +1)−δ( t )= E [ X 0 , t ]− E [ X 0 , t + 1 ]+
1
1 +γ
=− 1 +
γ E [ X 0 , t ]
t
+
1
1 +γ
=−
γδ( t )
t
.
It follows thatδ( t ) is decreasing in t , but is always greater than 0. The lemma follows,
sinceδ( t )<2.  /theta

More generally, we have the following:
Lemma 16.3: Let cjbe the constants given by Eqn. (16.3). For any constant j, there
is a constant Bjsuch that for t ≥ 2
∣∣
∣
∣

E [ Xj , t ]
t
− cj
∣∣
∣
∣≤
Bj
t
.
power laws and related distributions
Before beginning the proof, we outline the reasoning behind it. The idea is that if
E [ Xj , t ] is too far from cjt , at the next step there will be a push to reduce the difference.
If E [ Xj , t ] becomes too large, then it becomes more likely that at the next step a vertex
with degree j will gain a link to it and become a vertex of degree j +1, reducing the
difference between E [ Xj , t ] and cjt. Similarly, if E [ Xj , t ] becomes too small, then it is
less likely that a vertex with degree j will gain a link and the difference is similarly
reduced. The complication is that Xj , t + 1 also depends on E [ Xj − 1 , t ], which may itself be
deviating from cj − 1 t , and therefore may also serve to push E [ Xj , t ] from cjt. Inductively,
however, those deviations are small, and as such their effect is overcome by the initial
effect that pushes E [ Xj , t ]to cjt.

Proof: We know the statement is true for j =0, and we prove it by induction for larger
j , by also performing an induction on the time t .For j ≥0 and t ≥2, let
δ j ( t )= cjt − E [ Xj , t ],

where cj is given by Eqn. (16.3). From Eqn. (16.1), we have for j ≥ 1

E [ Xj , t + 1 ]= E [ Xj , t ]+
γ+(1−γ)( j −1)
t
E [ Xj − 1 , t ]−
γ+(1−γ) j
t
E [ Xj , t ].(16.6)
Using Eqn. (16.6), we find for j ≥ 1
δ j ( t +1)= cj ( t +1)− E [ Xj , t + 1 ]

= cjt + cj − E [ Xj , t ]−
γ+(1−γ)( j −1)
t
E [ Xj − 1 , t ]+
γ+(1−γ) j
t
E [ Xj , t ]
= cj +δ j ( t )−
γ+(1−γ)( j −1)
t
E [ Xj − 1 , t ]+
γ+(1−γ) j
t
E [ Xj , t ]
= cj +δ j ( t )−
γ+(1−γ)( j −1)
t
( cj − 1 t −δ j − 1 ( t ))
+
γ+(1−γ) j
t
( cjt −δ j ( t ))
=δ j ( t )+
γ+(1−γ)( j −1)
t
δ j − 1 ( t )−
γ+(1−γ) j
t
δ j ( t ).
Suppose inductively that|δ j − 1 ( t )|≤ Bj − 1. Now for t ≤γ+(1−γ) j we can find a
constant Bj so that|δ j − 1 ( t )|≤ Bj , since this is only over a constant number of steps.
Let us also suppose that Bj ≥ Bj − 1 ; if not, we could simply increase Bj to this value.
For t >γ+(1−γ) j , the right-hand side above has absolute value bounded by
∣
∣∣
∣

(
1 −
γ+(1−γ) j
t
)
δ j ( t )+
γ+(1−γ)( j −1)
t
δ j − 1 ( t )
∣
∣∣
∣.
As t >γ+(1−γ) j impliesγ+(1 t −γ) j <1, this expression is bounded above by
(
1 −

γ+(1−γ) j
t
)
Bj +
γ+(1−γ)( j −1)
t
Bj − 1 ≤ Bj
where here we have inductively assumedδ j ( t )≤ Bj , and the right-hand side then
follows fromγ+(1 t −γ) j ≥γ+(1−γ t )( j −1)and Bj ≥ Bj − 1.
As a result we have|δ j ( t +1)|≤ Bj and by induction the lemma follows.  /theta

16.4 Using the Power Law in Algorithm Analysis
To summarize, we have shown that under appropriate initial conditions for the pref-
erential attachment model for the World Wide Web with high probability the fraction
of pages with j other pages linking to that page converges to cj , where the cj follow a

power law given by cj ∼ j −

21 −−γγ
.Here1−γis the probability a link for a new page is
chosen by copying an existing link.
Although we have presented preferential attachment as a potential model for the
Web graph, our analysis applies generally to preferential attachment models. In fact,
the idea of preferential attachment arose much earlier than the World Wide Web; in
1925, Yule used a similar analysis to explain the distribution of species among gen-
era of plants, which had been shown empirically to satisfy a power law distribution.
Another development of how preferential attachment leads to a power law was given
by Simon in 1955. While Simon was a bit too early to provide a model for the graph
arising from the World Wide Web, he suggested several potential applications of this
type of preferential attachment model: distributions of word frequencies in documents,
distributions of numbers of papers published by scientists, distribution of cities by pop-
ulation, distribution of incomes, and distribution of species among genera.

16.4. Using the Power Law in Algorithm Analysis
In some cases, algorithms can be designed to take advantage of or analyzed taking
advantage of the fact that the object being studied is related to a power law. One pro-
totypical example involves listing or counting triangles in graphs. Counting triangles
often proves important in network analysis; the number of triangles is related to the
clustering coefficient of a graph, which measures the tendency of vertices in a graph
to cluster together. For example, in social networks, where vertices represent people
and edges represent friendships, the closing of a triangle represents two people with a
mutual friend becoming friends themselves.
Let us work in the simple setting of undirected graphs with no self-loops or mul-
tiple parallel edges between vertices, and with n vertices and m edges. Counting and
listing triangles can naturally be done in /eta( n^3 ) time by checking all possible triples of
vertices. Counting can be done faster via matrix multiplication; if A is the adjacency
matrix of the graph, then the diagonal of A^3 is related to the number of triangles in
the graph. Specifically, the i th diagonal entry is equal to twice the number of triangles
in which the i th vertex is contained. Hence one can divide the sum of the diagonal
entries by six to obtain the total number of triangles in the graph. Since matrix multi-
plication can be done in time o ( n^3 ), this approach is faster than checking all possible
triples.
For sparse graphs, we can do even better. We provide an algorithm that can count and
list triangles in O ( m^3 /^2 ) time, which is optimal in that there can be m( m^3 /^2 ) triangles in
a graph. We then show that the running time of the algorithm can be improved, under
the assumption that the degree sequence of the graph is governed by a power law.

Theorem 16.4: The Triangle Listing Algorithm runs in O ( m^3 /^2 ) time (assuming
m ≥ n).

power laws and related distributions
Triangle Listing Algorithm:
Input: An undirected graph G =( V , E ) with no self-loops or multiple edges
between vertices, in adjacency list form.
Output: A listing of all triangles in the graph.
Main routine:
1. Sort the vertices by degree, from smallest to largest; let d (v) be the degree ofv,
and N (v) be the set of neighbors ofv.
2. Let d ∗(v)bev’s position in the sorted order, with ties broken arbitrarily.
3. Create an array of n lists, where A [v] is a list associated withv. Initially all lists
are empty.
4. For each vertexv, in decreasing order of d ∗(v):
(a) For each u ∈ N (v) with d ∗( u )< d ∗(v):
i. for eachw∈ A [ u ]∩ A [v] output the triangle{ u ,v,w};
ii. addvto the list A [ u ].

Algorithm 16.1: Triangle Listing.
Proof: We first show that all triangles are listed exactly once. Consider a triangle
{ x , y , z }with d ∗( x )> d ∗( y )> d ∗( z ). Let us say a vertexvis processed when we reach
it as we go through the vertices in decreasing order of the d ∗values. Then vertex x
is processed before vertices y and z. The triangle is listed precisely when vertex y is
processed, since vertex x was added to A [ y ] and A [ z ] when x was processed, and when
y is processed we have that z ∈ N ( y ) and d ∗( z )< d ∗( y ), so the triangle will be output.
Moreover, this will be the only time this triangle is output, since when vertex x is pro-
cessed neither y nor z is in A [ x ], and when vertex z is processed both d ∗( x ) and d ∗( y )
are greater than d ∗( z ).
To bound the running time, we first see that calculating the vertex degrees is O ( m )
and the initial sorting step is O ( n log n ), and these are each O ( m^3 /^2 ). We claim that for
each of the O ( m ) edges, corresponding to the step “For each u ∈ N (v)”, we do at most
O (

√
m ) work to calculate the intersection of A [ u ] and A [v]. Because A [ u ] and A [v]are
in sorted order with vertices ordered by d ∗as they are added to the list, the intersection
can be computed in time proportional to the maximum list size. But for any vertex x ,
A [ x ] contains only vertices with degree at least as large as x ’s degree. If x ’s degree was
larger than 2

√
m , then all of x ’s neighbors in A [ x ] would also have degree larger than
2

√
m , which would yield at least (2
√
m )^2 / 2 = 2 m edges in the graph. (We divide by
2 as each edge might be counted twice.) This contradicts that there are only m edges
in the graph, so every list A [ x ] has size at most O (

√
m ), and the total running time is
bounded by O ( m^3 /^2 ).  /theta

Now let us consider this algorithm in the setting where the graph has a degree dis-
tribution that is governed by a power law. There are various ways one could define
the degree distribution being governed by a power law, but for our purposes here it
will suffice to say that we assume we have a graph where the number of vertices of

16.5 Other Related Distributions
degree at least j is at most cnj −αfor some constants c andα. Notice that if the num-
ber of vertices of degree exactly j is at most c 2 nj −βfor some constants c 2 andβ, then
this condition is satisfied, withα=β−1. Such an assumption could hold with high
probability for a random graph produced by, for example, a preferential attachment
model.
Theorem 16.5: The Triangle Listing Algorithm runs in O ( mn^1 /(1+α)) time if the num-
ber of vertices of degree at least j is at most cnj −α for some constants c and α.
Theorem16.5offers an improved bound over Theorem16.4whenα>1. Notice
that such power law graphs are sparse, so in this case m = O ( n ); hence the running
time could also be expressed as O ( n (2+α)/(1+α)).
Proof: We again bound the work to calculate the intersections of A [ u ] and A [v]. For
any vertex x ,| A [ x ]|≤ d ( x ), since only neighbors of x are on the list A [ x ]. Also, A [ x ]
only contains vertices with degree at least d ( x ). Hence| A [ x ]|≤min( d ( x ), cn ( d ( x ))−α).
Equalizing terms in the minimization, we find| A [ x ]|≤( cn )^1 /(1+α). The theorem
follows.  /theta
16.5. Other Related Distributions
While power law distributions can often provide a natural model, there are other dis-
tributions with similar behaviors that may provide better models in some situations.
Indeed, there can be controversy as to what is the best model in various situations, and
because the tail of a power law distribution corresponds to relatively rare events, the
choice of model can have significant implications regarding the importance of these
rare events. It could be important to have a good model, for example, of exactly how
rarely very strong earthquakes will occur. Here we examine some distributions that are
often suggested as alternatives to a power law distribution.
16.5.1 Lognormal Distributions
A non-negative random variable X has a lognormal distribution if the random variable
Y =ln X has a normal (or Gaussian) distribution. Recall from Chapter 9 that the normal
distribution Y is given by the density function

g ( y )=
1
√
2 πσ
e−( y −μ)
(^2) / 2 σ 2
whereμis the mean,σis the standard deviation (σ^2 is the variance), and the range is
−∞< y <∞. The density function f ( x ) for a lognormal random variable X is simply
g (ln x )/ x , over the range 0< x <∞. The x in the denominator arises because
f ( x ) dx = g ( y ) dy
for y =ln x , which gives dy = dx / x.

power laws and related distributions
The density function for a lognormal distribution X therefore satisfies
f ( x )=
1
√
2 πσ x
e−(ln x −μ)
(^2) / 2 σ 2
and the complementary cumulative distribution function for a lognormal distribution
is given by
Pr( X ≥ x )=

∫∞
z = x
1
√
2 πσ z
e−(ln z −μ)
(^2) / 2 σ 2
dz.
We say that X has parametersμandσ^2 when the associated normal distribution Y
has meanμand varianceσ^2 , where the meaning is clear. The lognormal distribution is
skewed, with mean eμ+
(^12) σ 2
, median eμ, and mode eμ−σ^2. (Proving these values is left as
Exercise16.13.) A lognormal distribution has finite mean and variance, in contrast with
power law distributions, which as we have seen can have infinite mean and variance
under some parameters.
An interesting property of lognormal distributions is that the product of independent
lognormal distributions is again lognormal. This follows from the fact that normal dis-
tributions have the property that the sum of two independent normal random variables
Y 1 and Y 2 withμ 1 andμ 2 and variancesσ 12 andσ 22 respectively is a normal random
variable with meanμ 1 +μ 2 and varianceσ 12 +σ 22.
Although it has finite moments, the lognormal distribution is extremely similar in
“shape” to a power law distribution, in the following sense: if X has a lognormal dis-
tribution, then in a log–log plot of the complementary cumulative distribution function
or the density function of X , the plot can appear to be close to linear for much of the
distribution, depending on the variance of the corresponding normal distribution. If the
variance is large, the log–log plot may appear linear for several orders of magnitude.
To see this, let us look at the logarithm of the density function, which is slightly
easier to work with than the complementary cumulative distribution function. We have
ln f ( x )=−ln x −ln

√
2 πσ−
(ln x −μ)^2
2 σ^2
. (16.7)
Ifσis large, then the quadratic term in Eqn. (16.7) will be small for a large range of x
values, so ln f ( x ) appears almost linear in ln x over a large range. Recall that checking
for linearity over a log–log plot is the first test (albeit an “eyeball” test) one uses to check
if a distribution is a power law; the lognormal distribution shows why the linearity test
is generally not enough to allow one to conclude that a sampled distribution follows a
power law.

16.5.2 Power Law with Exponential Cutoff
The power law with exponential cutoff refers to a distribution where the density func-
tion is of the form

f ( x )∼ x −αe−λ x
16.6 Exercises
for someα, λ >0. The idea behind using such a distribution is that, similar to a lognor-
mal distribution, it can roughly follow a power law distribution for much of the body
of the distribution whenλis small, but for sufficiently large values of x the exponential
term will dominate. The exponential cutoff can model power laws that eventually must
end because of resource limitations. For example, the distribution of wealth may be
better fit to a power law with an exponential cutoff rather than a power law; eventually,
there are limits to the money to be had, and as such the exponential cutoff may better
model the tail of the distribution.

16.6. Exercises
Exercise 16.1: (a) Pareto distributions are often said to be “scale invariant” in the
following sense. If X is a random variable that follows a Pareto distribution and has
density f ( x ), then the rescaled random variable having density g ( x )= f ( cx ) has density
proportional to f ( x ). Prove this statement.
(b) An implication of scale invariance is that if we measure our random variable in
different units, it remains a Pareto distribution. For example, if we think wealth follows
a Pareto distribution and we rescale to measure wealth in millions of dollars instead of
dollars, we still have a Pareto distribution. Show that, under such a rescaling (where
g ( x )= f ( cx )), a Pareto distribution remains a straight line on a log–log plot, and is just
shifted up or down.

Exercise 16.2: Suppose that the time to finish a project in hours is given by a Pareto
distribution with parameterα=2 and a minimum time of one hour. What is the
expected time to complete the project? Now suppose that the project is not completed
after three hours. If the time to complete the project is given by the initial Pareto dis-
tribution conditioned on the completion time being at least three hours, what is the
expected remaining time until the completion of the project? How does this compare
with the original expected time to complete the project?

Exercise 16.3: Consider a random variable X that has a Pareto distribution with
parametersα>0 and minimum value m. Determine for x ≥ y ≥ m the conditional
distribution

Pr( X ≥ x | X ≥ y ).
Exercise 16.4: Suppose that the time to finish a project in hours is given by a Pareto
distribution with parameterαand a minimum time of one hour. Pareto distributions
can have the property that the longer the project goes without completing, the longer
it is expected to take to complete. That is, if X is the time the project finishes, we are
concerned with

f ( y )= E [ X − y | X ≥ y ].
Show that f is an increasing function whenα>1.

power laws and related distributions
Exercise 16.5: In section9.6, we discussed maximizing the log-likelihood function
in order to find a maximum likelihood estimate of a parameter. Suppose we have n
samples from a Pareto distribution, x 1 , x 2 ,..., xn , where the minimum value is known
to be 1 but the parameterαis unknown.

(a) What is the log-likelihood function in terms of the parameterαand the sample
values x 1 , x 2 ,..., xn?
(b) What is the maximum likelihood estimator forα?

Exercise 16.6: Power law distributions are often described anecdotally by phrases
such as “20% of the population has 80% of the income.” If one assumes a Pareto dis-
tribution, this phrase determines a parameterα. What value ofαcorresponds to this
phrase? Your argument should explain why your result is independent of the minimum
value m.

Exercise 16.7: Consider the standard random walk X 0 , X 1 , X 2 ,...on the integers that
starts at 0 and moves from Xi to Xi +1 with probability 1/2 at each step and from Xi to
Xi −1 with probability 1/2 at each step. We are interested in the first return time to 0.
Note that this time must be even. Let ft be the probability that the first time the walk
returns to 0 is at time 2 t. Let ut be the probability the walk is at 0 at time 2 t.

(a) Prove that ut =
(
2 t
t
)
2 −^2 t.
(b) Consider the probability Pr( X 1 > 0 , X 2 > 0 ,..., X 2 t − 1 > 0 | X 2 t =0). Show that
this probability is 2 t^1 − 1 .( Hint : this can be done using the Ballot Theorem from
Section13.2.1.)
(c) Prove that ft = 2 tu − t 1.
(d) Using Stirling’s formula, show that ft follows a power law.

Exercise 16.8: Consider the monkey typing randomly experiment with an alphabet
of two letters that are hit with differing probabilities: “a” occurs with probability q ,
“b” occurs with probability q^2 , and a space occurs with probability 1− q − q^2 .(Here
q satisfies 1− q − q^2 >0.)

(a) Show that every word the monkey can type occurs with probability qj (1− q − q^2 )
for some integer j.
(b) Let us say a word has pseudo-rank j if it occurs with probability qj (1− q − q^2 ).
Show that the number of words with pseudo-rank j is the ( j +1)st Fibonacci num-
ber Fj + 1 (where here we start with F 0 =0 and F 1 =1).
(c) Use facts about the Fibonacci numbers, such as

∑ k
i = 1 Fk = Fk +^2 −1, and Fk ≈
φ k /
√
5 for large k whereφ=^1 +
√ 5
2 , to show that the frequency of the j th most
frequent word behaves (roughly) like a power law, following a similar approach to
that used to analyze the setting of the monkeys typing randomly experiment with
equal character probabilities.
Exercise 16.9: Write a program to simulate the monkeys typing randomly experiment
of Section16.2.3. Your simulation should consider the following two scenarios.

16.6exercises
 /thetaYou have an alphabet of 8 letters and space; the space is chosen with probability 0.2,
and the other letters are chosen with equal probability.
 /thetaYou have an alphabet of 8 letters and space; the space is chosen with probability 0.2,
and the probability for each of the other letters is chosen uniformly at random, with
the constraint that the sum of their probabilities is 0.8.
For each scenario, generate 1 million words, and track the frequency for each word
that appears. Recall that the empty word should be treated as a word, and you will have
many fewer than 1 million distinct words to track.

(a) In practice, you should be able to represent each word seen in an experiment using
at most 256 bits (at least most of the time). Explain why this is the case.
(b) Plot the distribution of word frequencies for each scenario on a log–log plot. The x -
axis should be the rank of the word in terms of its frequency, and the y -axis should
be the frequency. Do the two plots differ?
(c) Do your plots appear to follow a power law? Explain.

Exercise 16.10: Write a program to simulate the preferential attachement process,
starting with four pages linked together as a cycle on four vertices, adding pages each
with one outlink until there are 1 million pages, and usingγ= 0 .5 as the probability
a link is to a page chosen uniformly at random and 1−γ= 0 .5 as the probabiliy a
link is copied from existing links. Draw a plot of the degree distribution, showing the
number of vertices of each degree on a log–log plot. Also draw a plot showing the
complimentary cumulative degree distribution – that is, the number of vertices with
degree at least k for every value of k – on a log–log plot. Does the degree distribution
appear to follow a power law? Explain.

Exercise 16.11: Write a program to simulate the preferential attachement process as in
the prevous problem, but now start with four pages with each page pointing to the other
three, and add pages each with three outlinks until there are 1 million pages. Again
draw the plots of the degree distribution and the complimentary cumulative degree
distribution. How do these plots differ from those in the previous problem? Does the
degree distribution appear to follow a power law?

Exercise 16.12: Derive the expressions for the mean and variance of a power law
distribution with an exponential cutoff with parametersαandλ. (You may assumeα
is a positive integer.)

Exercise 16.13: Derive the expressions for the mean, median, and mode of a lognor-
mal distribution with parametersμandσ^2. Recall the mean is eμ+

(^12) σ 2
, the median is
eμ, and the mode is eμ−σ^2.
Exercise 16.14: Consider the count-min filter from Section15.4. We show that the
bounds on its performance can be improved if the distribution of item counts follows a
power law distribution. Suppose we have a collection of N items, where the total count
associated with the nth most frequent item is given by f n= c / n z for a constant z >1 and

power laws and related distributions
a value c. (You may assume all the f nare suitably rounded integers for convenience.) As
described in Section15.4, we assume we have k disjoint groups of counters, each with
m / k counters. We use the minimum counter Ca , j that an item hashes to as an estimate
for its count.

(a) Show that the tail of the total count for all items after removing the b ≥1 most
frequent items is bounded by
∑ N
i = b + 1
fi ≤
cb^1 − z
z − 1
≤ Fb^1 − z ,
where F =
∑ N
i = 1 fi.
(b) Consider now an element i with total count fi , and let us consider a single group
of counters. Show that the probability that i collides with any of the m /(3 k ) items
with the largest count (besides possibly itself) is at most 1/3.
(c) Show that, conditioned on the eventEthat i does not collide with any of the m /(3 k )
items with the largest count, the expected count for the counter Ca , j that i hashes
to is bounded by

E [ Ca , j |E]≤ fi + F
( m / 3 k )^1 − z
( m / k )
.
(d) Letγ= 3 ( m /^3 k )

1 − z
m / k. Prove that Ca , j ≤ fi +γ F with probability at least 1/3.
(e) Explain why the above implies that the count-min filter produces an estimate for
fi that is at most fi +γ F with probability at least 1−(2/3) k.
(f) Suppose we want to find all items with a count of at least q ; when an item is hashed
into the count-min filter, it is put on a list if its minimum counter is at least q.
Prove that we can construct a count-min filter with O (ln^1 δ) hash functions and
O (ln^1 δ /theta−^1 / z ) counters so that all items with count at least q are put on the list,
and any item that has a count of less than q − /theta F is put on the list with probability
at mostδ. (This improves the result of Corollary15.13for this type of skewed
distribution of item counts.)
chapter seventeen ∗

Balanced Allocations and

Cuckoo Hashing

In this chapter, we examine simple and powerful variants of the classic balls-and-bins
paradigm, where each ball may have a choice of a small number of bins where it can
be placed. In our first setting, often referred to as balanced allocations, the balls have
choices, and a choice of where a ball is to be placed must be made once and for all when
the ball enters the system. In our second setting, referred to as cuckoo hashing, balls
may move to another choice after their initial placement under some circumstances.

17.1 The Power of Two Choices
Suppose that we sequentially place n balls into n bins by putting each ball into a bin
chosen independently and uniformly at random. We studied this classic balls-and-bins
problem in Chapter 5. There we showed that, at the end of the process, the most balls
in any bin – the maximum load – is /eta(ln n /ln ln n ) with high probability.
In a variant of the process, each ball comes with d possible destination bins, each
chosen independently and uniformly at random, and is placed in the least full bin among
the d possible locations at the time of the placement. The original balls-and-bins process
corresponds to the case where d =1. Surprisingly, even when d =2, the behavior is
completely different: when the process terminates, the maximum load is ln ln n /ln 2+
O (1) with high probability. Thus, an apparently minor change in the random allocation
process results in an exponential decrease in the maximum load. We may then ask what
happens if each ball has three choices; perhaps the resulting load is then O (ln ln ln n ).
We shall consider the general case of d choices per ball and show that, when d ≥2, with
high probability the maximum load is ln ln n /ln d + /eta(1). Although having more than
two choices does reduce the maximum load, for any constant d the reduction changes
it by only a constant factor, so it remains /eta(ln ln n ) for a constant d.

17.1.1 The Upper Bound
Theorem 17.1: Suppose that n balls are sequentially placed into n bins in the fol-
lowing manner. For each ball, d ≥ 2 bins are chosen independently and uniformly at

balanced allocations and cuckoo hashing
random (with replacement). Each ball is placed in the least full of the d bins at the
time of the placement, with ties broken randomly. After all the balls are placed, the
maximum load of any bin is at most ln ln n /ln d + O (1) with probability 1 − o (1/ n ).

The proof is rather technical, so before beginning we informally sketch the main points.
In order to bound the maximum load, we need to approximately bound the number of
bins with i balls for all values of i. In fact, for any given i , instead of trying to bound the
number of bins with load exactly i , it will be easier to bound the number of bins with
load at least i. The argument proceeds via what is, for the most part, a straightforward
induction. We wish to find a sequence of valuesβ i such that the number of bins with
load at least i is bounded above byβ i with high probability.
Suppose that we knew that, over the entire course of the process, the number of bins
with load at least i was bounded above byβ i. Let us consider how we would determine
an appropriate inductive bound forβ i + 1 that holds with high probability. Define the
height of a ball to be one more than the number of balls already in the bin in which
the ball is placed. That is, if we think of balls as being stacked in the bin by order of
arrival, the height of a ball is its position in the stack. The number of balls of height at
least i +1 gives an upper bound for the number of bins with at least i +1 balls.
A ball will have height at least i +1 only if each of its d choices for a bin has load
at least i. If there are indeed at mostβ i bins with load at least i at all times, then the
probability that each choice yields a bin with load at least i is at mostβ i / n. Therefore,
the probability that a ball has height at least i +1isatmost(β i / n ) d. We can use a
Chernoff bound to conclude that, with high probability, the number of balls of height
at least i +1 will be at most 2 n (β i / n ) d. That is, if everything works as sketched, then

β i + 1
n
≤ 2
(
β i
n
) d
.
We examine this recursion carefully in the analysis and show thatβ j becomes O (ln n )
when j =ln ln n /ln d + O (1). At this point, we must be a bit more careful in our ana-
lysis because Chernoff bounds will no longer be sufficiently useful, but the result is
easy to finish from there.
The proof is technically challenging primarily because one must handle the condi-
tioning appropriately. In boundingβ i + 1 , we assumed that we had a bound onβ i. This
assumption must be treated as a conditioning in the formal argument, which requires
some care.
We shall use the following notation: the state at time t refers to the state of the system
immediately after the t th ball is placed. The variable h ( t ) denotes the height of the t th
ball, andν i ( t ) andμ i ( t ) refer (respectively) to the number of bins with load at least i
and the number of balls with height at least i at time t. We useν i andμ i forν i ( n ) and
μ i ( n ) when the meaning is clear. An obvious but important fact, of which we make
frequent use in the proof, is thatν i ( t )≤μ i ( t ), since every bin with load at least i must
contain at least one ball with height at least i.
Before beginning, we make note of two simple lemmas. First, we utilize a specific
Chernoff bound for binomial random variables, easily derived from Eqn. (4.2) by letting
δ=1.

17.1the power of two choices
Lemma 17.2:
Pr( B ( n , p )≥ 2 np )≤e− np /^3. (17.1)
The following lemma will help us cope with dependent random variables in the main
proof.
Lemma 17.3: Let X 1 , X 2 ,..., Xnbe a sequence of random variables in an arbitrary
domain, and letY 1 , Y 2 ,..., Ynbe a sequence of binary random variables with the prop-
erty that Yi = Yi ( X 1 ,..., Xi ) .If
Pr( Yi = 1 | X 1 ,..., Xi − 1 )≤ p ,
then
Pr
(∑ n
i = 1
Yi > k
)
≤Pr( B ( n , p )> k ).
Proof: If we consider the Yi one at a time, then each Yi is less likely to take on the
value 1 than an independent Bernoulli trial with success probability p , regardless of
the values of the Xi. The result then follows by a simple induction.  /theta
We now begin the main proof.
Proof of Theorem17.1: Following the earlier sketch, we shall construct valuesβ i such
that, with high probability,ν i ( n )≤β i for all i. Letβ 4 = n /4, and letβ i + 1 = 2 β id / nd −^1
for 4≤ i < i ∗, where i ∗is to be determined. We letE i be the event thatν i ( n )≤β i. Note
thatE 4 holds with probability 1; there cannot be more than n /4 bins with at least 4 balls
when there are only n balls. We now show that, with high probability, ifE i holds then
E i + 1 holds for 4≤ i < i ∗.
Fix a value of i in the given range. Let Yt be a binary random variable such that
Yt =1 if and only if h ( t )≥ i +1 andν i ( t −1)≤β i.
That is, Yt is 1 if the height of the t th ball is at least i +1 and if, at time t −1, there
are at mostβ i bins with load at least i. The requirement that Yt be 1 only if there are at
mostβ i bins with load at least i may seem a bit odd; however, it makes handling the
conditioning much easier.
Specifically, letω j represent the bins selected by the j th ball. Then
Pr( Yt = 1 |ω 1 ,...,ω t − 1 )≤
β id
nd
.
That is, given the choices made by the first t −1 balls, the probability that Yt is 1 is
bounded by (β i / n ) d. This is because, in order for Yt to be 1, there must be at mostβ i
bins with load at least i ;and when this condition holds, the d choices of bins for the
t th ball all have load at least i with probability (β i / n ) d. If we did not force Yt to be 0 if
there are more thanβ i bins with load at least i , then we would not be able to bound this
conditional probability in this way.

balanced allocations and cuckoo hashing
Let pi =β id / nd. Then, from Lemma17.3, we can conclude that
Pr
( n
∑
t = 1
Yt > k
)
≤Pr( B ( n , pi )> k ).
This holds independently of any of the eventsE i , owing to our careful definition of Yt.
(Had we not included the condition that Yt =1 only ifν i ( t −1)≤β i , the inequality
would not necessarily hold.)
Conditioned onE i ,wehave

∑ n
t = 1 Yt =μ i +^1. Sinceν i +^1 ≤μ i +^1 ,wehave
Pr(ν i + 1 > k |E i )≤Pr(μ i + 1 > k |E i )
=Pr
( n
∑
t = 1
Yt > k
∣∣
E i
)
≤
Pr
(∑ n
t = 1 Yt > k
)
Pr(E i )
≤
Pr( B ( n , pi )> k )
Pr(E i )
.
We bound the tail of the binomial distribution by using the Chernoff bound of
Lemma17.2. Letting k =β i + 1 = 2 npi in the previous equations yields

Pr(ν i + 1 >β i + 1 |E i )≤
Pr( B ( n , pi )> 2 npi )
Pr(E i )
≤
1
e pin /^3 Pr(E i )
,
which gives

Pr(¬E i + 1 |E i )≤
1
n^2 Pr(E i )
(17.2)
whenever pin ≥6ln n.
We now remove the conditioning by using the fact that

Pr(¬E i + 1 )=Pr(¬E i + 1 |E i )Pr(E i )+Pr(¬E i + 1 |¬E i )Pr(¬E i )
≤Pr(¬E i + 1 |E i )Pr(E i )+Pr(¬E i ); (17.3)
then, by Eqns. (17.2) and (17.3),

Pr(¬E i + 1 )≤Pr(¬E i )+
1
n^2
(17.4)
as long as pin ≥6ln n.
Hence, whenever pin ≥6ln n andE i holds with high probability, then so doesE i + 1.
To conclude we need two more steps. First, we need to show that pin <6ln n when
i is approximately ln ln n /ln d , since this is our desired bound on the maximum load.
Second, we must carefully handle the case where pin <6ln n separately, since the
Chernoff bound is no longer strong enough to give appropriate bounds once pi is this
small.

17.1the power of two choices
Let i ∗be the smallest value of i such that pi =β di / nd <6ln n / n. We show that i ∗is
ln ln n /ln d + O (1). To do this, we prove inductively the bound

β i + 4 =
n
22 di −
∑ i − 1
j = 0 dj
.
This holds true when i =0, and the induction argument follows:

β( i +1)+ 4 =
2 β id + 4
nd −^1
=
2
(
n
22 d
i −∑ ij −=^10 dj
) d
nd −^1
=
n
22 d
i + (^1) −∑ ij = 0 dj.
The first line is the definition ofβ i ; the second follows from the induction hypothesis.
It follows thatβ i + 4 ≤ n / 2 d
i
and hence that i ∗is ln ln n /ln d + O (1). By inductively
applying Eqn. (17.4), we find that
Pr(¬E i ∗)≤
i ∗
n^2

.
We now handle the case where pin <6ln n .Wehave
Pr(ν i ∗+ 1 >18 ln n |E i ∗)≤Pr(μ i ∗+ 1 >18 ln n |E i ∗)
≤
Pr( B ( n ,6ln n / n )≥18 ln n )
Pr(E i ∗)
≤
1
n^2 Pr(E i ∗)
,
where the last inequality again follows from the Chernoff bound. Removing the con-
ditioning as before then yields

Pr(ν i ∗+ 1 >18 ln n )≤Pr(¬E i ∗)+
1
n^2
≤
i ∗+ 1
n^2
. (17.5)
To wrap up, we note that
Pr(ν i ∗+ 3 ≥1)≤Pr(μ i ∗+ 3 ≥1)≤Pr(μ i ∗+ 2 ≥2)
and bound the latter quantity as follows:

Pr(μ i ∗+ 2 ≥ 2 |ν i ∗+ 1 ≤18 ln n )≤
Pr( B ( n ,(18 ln n / n ) d )≥2)
Pr(ν i ∗+ 1 ≤18 ln n )
≤
( n
2
)
(18 ln n / n )^2 d
Pr(ν i ∗+ 1 ≤18 ln n )
.
Here the last inequality comes from applying the crude union bound; there are

( n
2
)
ways
of choosing two balls, and for each pair the probability that both balls have height at
least i ∗+2 is at most (18 ln n / n )^2 d.

balanced allocations and cuckoo hashing
Removing the conditioning as before and then using Eqn. (17.5) yields
Pr(ν i ∗+ 3 ≥1)≤Pr(μ i ∗+ 2 ≥2)
≤Pr(μ i ∗+ 2 ≥ 2 |ν i ∗+ 1 ≤18 ln n )Pr(ν i ∗+ 1 ≤18 ln n )
+Pr(ν i ∗+ 1 >18 ln n )
≤
(18 ln n )^2 d
n^2 d −^2
+
i ∗+ 1
n^2
,
showing that Pr(ν i ∗+ 3 ≥1) is o (1/ n )for d ≥2 and hence that the probability the max-
imum bin load is more than i ∗+ 3 =ln ln n /ln d + O (1) is o (1/ n ).  /theta

Breaking ties randomly is convenient for the proof, but in practice any natural tie-
breaking scheme will suffice. For example, in Exercise17.1we show that if the bins
are numbered from 1 to n then breaking ties in favor of the smaller-numbered bin is
sufficient.
As an interesting variation, suppose that we split the n bins into two groups of equal
size. Think of half of the bins as being on the left and the other half on the right.
Each ball now chooses one bin independently and uniformly at random from each half.
Again, each ball is placed in the least loaded of the two bins – but now, if there is a
tie, the ball is placed in the bin on the left half. Surprisingly, by splitting the bins and
breaking ties in this fashion, we can obtain a slightly better bound on the maximum
load: ln ln n /(2 ln((1+

√
5)/2))+ O (1). One can generalize this approach by splitting
the bins into d ordered equal-sized groups; in case of a tie for the least-loaded bin,
the bin in the lowest-ranked group obtains the ball. This variation is the subject of
Exercise17.13.

17.2 Two Choices: The Lower Bound
In this section we demonstrate that the result of Theorem17.1is essentially tight by
proving a corresponding lower bound.

Theorem 17.4: Suppose that n balls are sequentially placed into n bins in the fol-
lowing manner. For each ball, d ≥ 2 bins are chosen independently and uniformly at
random (with replacement). Each ball is placed in the least full of the d bins at the
time of the placement, with ties broken randomly. After all the balls are placed, the
maximum load of any bin is at least ln ln n /ln d − O (1) with probability 1 − o (1/ n ).

The proof is similar in spirit to the upper bound, but there are some key differences. As
with the upper bound, we wish to find a sequence of valuesγ i such that the number of
bins with load at least i is bounded below byγ i with high probability. In deriving the
upper bound, we used the number of balls with height at least i as an upper bound on
the number of bins with height at least i. We cannot do this in proving a lower bound,
however. Instead, we find a lower bound on the number of balls with height exactly i
and then use this as a lower bound on the number of bins with height at least i.
In a similar vein, for the proof of the upper bound we used that the number of bins
with at least i balls at time n was at leastν i ( t ) for any time t ≤ n. This is not helpful

17.2two choices: the lower bound
now that we are proving a lower bound; we need a lower bound onν i ( t ), not an upper
bound, to determine the probability that the t th ball has height i +1. To cope with this,
we determine a lower boundγ i on the number of bins with load at least i that exist at
time n (1− 1 / 2 i ) and then bound the number of balls of height i +1 that arise over the
interval ( n (1− 1 / 2 i ), n (1− 1 / 2 i +^1 )]. This guarantees that appropriate lower bounds
hold when we need them in the induction, as we shall clarify in the proof.
We state the lemmas that we need, which are similar to those for the upper bound.

Lemma 17.5:

Pr( B ( n , p )≤ np /2)≤e− np /^8. (17.6)
Lemma 17.6: Let X 1 , X 2 ,..., Xnbe a sequence of random variables in an arbitrary
domain, and letY 1 , Y 2 ,..., Ynbe a sequence of binary random variables with the prop-
erty that Yi = Yi ( X 1 ,..., Xi ) .If

Pr( Yi = 1 | X 1 ,..., Xi − 1 )≥ p ,
then

Pr
( n
∑
i = 1
Yi > k
)
≥Pr( B ( n , p )> k ).
Proof of Theorem17.4: LetF i be the event thatν i ( n (1− 1 / 2 i ))≥γ i , whereγ i is
given by:

γ 0 = n ;
γ i + 1 =
n
2 i +^3
(
γ i
n
) d
.
ClearlyF 0 holds with probability 1. We now show inductively that successiveF i hold
with sufficiently high probability to obtain the desired lower bound.
We want to compute

Pr(¬F i + 1 |F i ).
With this in mind, for t in the range R =[ n (1− 1 / 2 i ), n (1− 1 / 2 i +^1 )], define the binary
random variable by

Zt =1 if and only if h ( t )= i +1orν i + 1 ( t −1)≥γ i + 1.
Hence Zt is always 1 ifν i + 1 ( t −1)≥γ i + 1.
The probability that the t th ball has height exactly i +1is
(
ν i ( t −1)
n

) d
−
(
ν i + 1 ( t −1)
n
) d
.
The first term is the probability that all the d bins chosen by the t th ball have load at
least i. This is necessary for the height of the t th ball to have height exactly i +1.
However, we must subtract out the probability that all d choices have at least i + 1
balls, because in this case the height of the ball will be larger than i +1.

balanced allocations and cuckoo hashing
Again lettingω j represent the bins selected by the j th ball, we conclude that
Pr( Zt = 1 |ω 1 ,...,ω t − 1 ,F i )≥
(
γ i
n
) d
−
(
γ i + 1
n
) d
.
This is because Zt is automatically 1 ifν i + 1 ( t −1)≥γ i + 1 ;hence we can consider the
probability in the case whereν i + 1 ( t −1)≤γ i + 1. Also, conditioned onF i ,wehave
ν i ( t −1)≥γ i.
From the definition of theγ i we can further conclude that

Pr( Zt = 1 |ω 1 ,...,ω t − 1 ,F i )≥
(
γ i
n
) d
−
(
γ i + 1
n
) d
≥
1
2
(
γ i
n
) d
.
Let pi =^12 (γ i / n ) d.
Applying Lemma 14.6 yields

Pr
(
∑
t ∈ R
Zt < k
∣∣
F i
)
≤Pr
(
B
(
n
2 i +^1
, pi
)
< k
)
.
Now our choice ofγ i nicely satisfies

γ i + 1 =
1
2
n
2 i +^1
pi.
By the Chernoff bound,

Pr
(
B
(
n
2 i +^1
, pi
)
<γ i + 1
)
≤e− npi /(8·^2
i + (^1) )
,
which is o (1/ n^2 ) provided that pin / 2 i +^1 ≥17 ln n. Let i ∗be a lower bound on the
largest integer for which this holds. We subsequently show that i ∗can be chosen to
be ln ln n /ln d − O (1);for now let us assume that this is the case. Then, for i ≤ i ∗,we
have shown that
Pr

(
∑
t ∈ R
Zt <γ i + 1
∣∣
∣
∣F i
)
≤Pr
(
B
( n
2 i +^1
, pi
)
<γ i + 1
)
= o
(
1
n^2
)
.
Further, by definition we have that

∑
t ∈ RZt <γ i +^1 implies¬F i +^1. Hence, for i ≤ i
∗,
Pr(¬F i + 1 |F i )≤Pr
(
∑
t ∈ R
Zt <γ i + 1
∣∣
F i
)
= o
(
1
n^2
)
.
Therefore, for sufficiently large n ,

Pr(F i ∗)≥Pr(F i ∗|F i ∗− 1 )·Pr(F i ∗− 1 |F i ∗− 2 )···Pr(F 1 |F 0 )·Pr(F 0 )
≥(1− 1 / n^2 ) i
∗
= 1 − o (1/ n ).
All that remains is to demonstrate that ln ln n /ln d − O (1) is indeed an appropriate
choice for i ∗. It suffices to show thatγ i ≥17 ln n when i is ln ln n /ln d − O (1). From
the recursionsγ i + 1 =γ id /(2 i +^3 nd −^1 ), we find by a simple induction that

γ i =
n
2
∑ i − 1
k = 0 ( i +^2 − k ) dk
.
17.3 Applications of the Power of Two Choices
A very rough bound gives

γ i ≥
n
210 di −^1
.
We therefore look for the maximum i such that
n
210 di −^1

≥17 ln n.
For n sufficiently large, we find that we can take i as large as ln ln n /ln d − O (1) by
using the following chain of inequalities:

n
210 di −^1
≥17 ln n ,
210 d
i − 1
≤
n
17 ln n
,
10 di −^1 ≤log 2 n −log 2 (17 ln n ),
di −^1 ≤
1
20
ln n ,
i ≤
ln ln n
ln d
− O (1).  /theta
17.3. Applications of the Power of Two Choices
The balanced allocation paradigm has a number of interesting applications to comput-
ing problems. We elaborate here on two simple applications. When considering these
applications, keep in mind that the ln ln n /ln d + O (1) bound we obtain for the bal-
anced allocation paradigm generally corresponds to a maximum load of at most 5 in
practice.

17.3.1 Hashing
When we considered hashing in Chapter 5 , we related it to the balls-and-bins paradigm
by assuming that the hash function maps the items being hashed to random entries in the
hash table. Subject to this assumption, we proved that (a) when O ( n ) items are hashed
to a table with n entries, the expected number of items hashed to each individual entry in
the table is O (1), and (b) with high probability, the maximum number of items hashed
to any entry in the table is /eta(ln n /ln ln n ).
These results are satisfactory for most applications, but for some they are not, since
the expected value of the worst-case lookup time over all items is /eta(ln n /ln ln n ). For
example, when storing a routing table in a router, the worst-case time for a lookup in
a hash table can be an important performance criterion, and the /eta(ln n /ln ln n ) result
is too large. Another potential problem is wasted memory. For example, suppose that
we design a hash table where each bin should fit in a single fixed-size cache line of
memory. Because the maximum load is so much larger than the average, we will have
to use a large number of cache lines and many of them will be completely empty. For
some applications, such as routers, this waste of memory is undesirable.

balanced allocations and cuckoo hashing
Applying the balanced allocation paradigm, we obtain a hashing scheme with O (1)
expected and O (ln ln n ) maximum access time. The 2- way chaining technique uses two
random hash functions. The two hash functions define two possible entries in the table
for each item. The item is inserted to the location that is least full at the time of insertion.
Items in each entry of the table are stored in a linked list. If n items are sequentially
inserted into a table of size n , the expected insertion and lookup time is still O (1). (See
Exercise17.3.) Theorem17.1implies that with high probability the maximum time to
find an item is O (ln ln n ), versus the /eta(ln n /ln ln n ) time when a single random hash
function is used. This improvement does not come without cost. Since a search for an
item now involves a search in two bins instead of one, the improvement in the expected
maximum search time comes at the cost of roughly doubling the average search time.
This cost can be mitigated if the two bins can be searched in parallel.

17.3.2 Dynamic Resource Allocation
Suppose a user or a process has to choose on-line between a number of identical
resources (choosing a server to use among servers in a network; choosing a disk to
store a directory; choosing a printer; etc.). To find the least loaded resource, users may
check the load on all resources before placing their requests. This process is expensive,
since it requires sending an interrupt to each of the resources. A second approach is
to send the task to a random resource. This approach has minimal overhead, but if all
users follow it then loads will vary significantly among servers. The balanced alloca-
tion paradigm suggests a more efficient solution. If each user samples the load of two
resources and sends his request to the least loaded one, then the total overhead remains
small while the load on the n resources varies much less.

17.4 Cuckoo Hashing
So far in this chapter we have considered balls-and-bins processes that correspond to
multiple choice hashing schemes, where each item can be placed in one of d choices
out of n total bins. Cuckoo hashing is a further variation on multiple choice hashing
schemes that uses the following idea: suppose items not only have multiple choices of
where they can be located, but even after we place an item, we can move it from one
of its choices to another at a later point if needed. This should give us more power to
place items in a way that balances the load. What can we say about cuckoo hashing
schemes?
Let us start our investigation of cuckoo hashing by considering the setting where
d =2, so an item can be placed in one of two possible bins. We also start by assuming
that each bin can hold only one item, so that our hash table can be kept as a simple
array of m locations. How many items can be placed into such a table?
As a baseline, let us consider what happens when we cannot move items. Each
item looks at its two choices, and is placed in the first if it is empty, or is placed in the
second if it is empty and the first is not. (If the two choices are always made uniformly
at random, and both choices are empty, it does not matter where the item is placed.)

17.4 cuckoo hashing
u v w x y z
u v w y z
x
Figure 17.1: An example of a cuckoo hash table. For each placed item, the directed arrow shows
the other location where that item can be moved to. In the initial configuration (top image), item x is
inserted, but its choices contain itemswand y .If x causes y to move and y causes z to move, then the
resulting configuration (bottom image) can hold all items. In the original configuration, if the choices
for x had been the locations containing items u andw,then x could not have been successfully placed.

As an exercise, you can check that without moves, when there are n bins, with high
probability you can place only O ( n^2 /^3 ) items before a new item being placed finds
both its choices already hold another item. This is a simple variation of the birthday
paradox.
Now let us consider the power that comes from moving items. If, on inserting an
item x , there is no room for an item at either of its two choices, we instead move the
item y in one of those bins to the other of its two choices. If the other bin for y is empty,
then we are done – every item has a suitable place. However, there may be another item
z in y ’s other location, in which case we may have to move z , and so on, until either we
find an empty space, or we realize that there is no empty space to be found, which is a
possibility. See Figure17.1for an example.
This approach is referred to as cuckoo hashing, taking the name from the cuckoo
bird, which lays its eggs in the nests of other birds and whose young kick out the eggs
or other young residing in the nest. We would like to understand various things about
cuckoo hashing, namely:

 /thetaHow many items can be successfully placed before an item cannot be placed?
 /thetaHow long do we expect it to take to insert a new item?
 /thetaHow can we know if we are in a situation where an item cannot be placed?
We address these issues by relating the cuckoo hashing process to a random graph
process. Let us treat the bins as vertices, and the items being hashed as edges. That

balanced allocations and cuckoo hashing
uv
yz
x
Figure 17.2: Items u ,v, y ,and z all reside in a bin, but their choices create a cycle in the cuckoo
graph. Adding item x would create a component with two cycles (when considering the edges as
undirected), which cannot be done. In simpler terms, a cuckoo hash table cannot store five items if
all of their choices fall into four bins.

is, since each item hashes to two possible hash locations, we can view it as an edge
connecting those two bins, or vertices, to which it hashes. As usual, we assume our
hash values are completely random. In that case the resulting graph may have parallel
edges, which are pairs of nodes connected by more than one edge, which occur when
different items hash to the same two locations (vertices). The graph may also have
self-loops, which are edges connecting a vertex to itself, which occur when both
locations (vertices) chosen for an item are the same. We call this graph the cuckoo
graph. We model the cuckoo graph corresponding to m items hashed into a table
of n entries by a random graph with n nodes and m edges, where each of the two
vertices of an edge is chosen independently and uniformly at random from the set of
n nodes.
We remark that self-loops can be eliminated by partitioning the table into two sub-
tables of equal size, and assigning each item a bin at random from each subtable. In that
case we have a random bipartite graph with n /2 vertices on each side and m random
edges, with each edge connecting two nodes, one chosen uniformly at random from
each side. The differences arising from these variations is minimal.
The load of our cuckoo hash table will be m / n , the ratio of the number of items
to the number of locations. Our main result is that if a cuckoo hash table with two
choices has load less than and bounded away from 1/2, placement will succeed with
high probability.
A key approach in studying cuckoo hashing is to look at the connected components
of the cuckoo graph. Recall that a connected component is simply a maximal group
of vertices that are all connected, or reachable, by traversing edges in the graph. We
show that as long as m / n ≤(1− /theta)/2 for some constant /theta>0, the maximum-sized
connected component in the cuckoo graph has only O (log n ) vertices with high proba-
bility, the expected number of vertices in a connected component for a given vertexvis
constant, and all components are trees or contain a single cycle with high probability.
(Here, a self-loop is considered a cycle on one vertex.) These facts about the cuckoo
graph translate directly into answers to our questions about cuckoo hashing.
It should be clear that an item cannot be placed if it falls into a component that,
after its placement, will have more items than bins to hold them, as shown by exam-
ple in Figure17.2. On the other hand, when all components are trees or have just a

17.4 cuckoo hashing
single cycle, every item can be placed successfully and efficiently. In fact we have the
following lemma:

Lemma 17.7: If an item is placed by cuckoo hashing so the resulting component is
a tree or has a single cycle, the placement will occur successfully, and can be done in
time proportional to the size of the component. If the resulting component has two or
more cycles, the placement fails.

Proof: If the number of edges, or items, exceeds the number of vertices, or bins, in
the component, as is the case if there are two or more cycles, then an item cannot be
placed.
To analyze the allocations of items to locations, it can help to think of each edge,
or item, as being directed away from the vertex, or bin, in which it currently resides.
Since each bin can store only one item, a proper allocation of items to bins must
have no more than one edge directed out of each vertex. Keep in mind, however, that
when we discuss cycles in components in our analysis, we are considering the undi-
rected edges; the directed edges are just to help us keep track of how items can be
moved.
It is clear that as long as all components are trees or have just a single cycle, the
items can be placed successfully. For a tree, one can simply choose one vertex as a
root, and orient all edges toward that root. That assignment has only one edge directed
out of each vertex and the root of the tree is assigned no element. For a component with
a cycle, the edges around the cycle have to be oriented consistently, and all other edges
have to be directed toward the cycle.
Cuckoo hashing will place items if they can be placed, and each bin is visited at most
twice during an insertion. There are three main cases to consider. When the item, or
edge, is placed into a component that joins two existing tree components (one of which
might be just a single vertex) so that the resulting component remains a tree, directed
edges will be followed until we reach a vertex with no outgoing edge. When a directed
edge is followed, it is reversed, corresponding to the replacement of the old item with
the new item. (See Figure17.3.)
When the item is placed so that both possible vertices already lie in the same compo-
nent, the behavior is similar to the first case. There is a unique path to an empty vertex,
corresponding to a bin that holds no item, and directed edges are followed and reversed
until that vertex is reached.
The last case is when the item to be placed joins a component that has a cycle with
a component that is a tree. It is possible in that case that placing the item will cause
the process to follow edges around the cycle, reversing the cycle orientation as it goes.
After going around the cycle and returning to the node where the new item was initially
placed, the new item will be kicked out, and then a path is followed to the empty location
in the tree component. It is important to see that while we can return once to the node
the insertion started at, we traverse each edge at most twice, once in each direction. We
never follow the same edge twice back into the cycle, because the edge will be flipped
to point away from the cycle. (See Figure17.4.)
In each case, placement takes time proportional to the component size.  /theta

balanced allocations and cuckoo hashing
x
Figure 17.3: Item x is inserted into the cuckoo graph, and placed in the bin (or vertex) on the left
(top image). It kicks out the item already there, moving the item to the neighboring bin in the graph.
In terms of the graph, the vertex can only have one outgoing edge, so the other adjacent edge must
reverse, and so on until the process terminates. In the bottom image, the reversed edges are shown as
dashed.

Lemma17.7tells us that to understand how cuckoo hashing performs, we simply
need to understand the component structure of the cuckoo graph. When we place a
new item in the cuckoo hash table, we add a new edge to the graph, which lies in an
existing component or joins two components. If we show the maximum component
size is O (log n ) with high probability, then we know from Lemma17.7that the max-
imum work needed to insert an item is O (log n ) with high probability. Similarly, if
we show that the expected size of a component is constant, then since the insertion
of a new item joins two components, the expected time to insert an item is bounded
by a constant. Of course, it is important to keep in mind that while insertion of a new
item can take a logarithmic number of steps, a lookup of an item always takes constant
time, since it is in one of two locations; this feature remains the key benefit of cuckoo
hashing.
Finally, as we try to place an item by moving other items in the hash table, keeping
track of the corresponding vertices visited in the cuckoo graph allows one to tell if the
graph has a bad component with two cycles, in which case placement of a new item
fails. Alternatively, because the maximum component size is O (log n ) with high prob-
ability, in practice in implementations one often allows at most c log n replacements
of items for a suitable constant c before declaring a failure. With this approach, one
does not have to keep track of the vertices seen, avoiding the use of memory during
placement.
We turn now to analyzing the connected component structure of a random cuckoo
graph with n nodes and m =(1− /theta) n /2 edges, for any constant /theta>0. Based on our

17.4 cuckoo hashing
x
Figure 17.4: Item x is inserted into the cuckoo graph, and placed in the top bin (or vertex) of its two
choices (top image). It kicks out the item already there, moving the item to the neighboring bin in the
graph. In terms of the graph, the vertex can only have one outgoing edge, so the other adjacent edge
must reverse, and so on. In this case, the process goes around the cycle and returns back to the original
vertex where x was placed (middle image). The item x is itself kicked out to its other location, and
the process terminates. Edges in the original graph that changed direction at least once are shown as
dashed; an edge can only change direction at most twice.

analysis thus far, our task now is to analyze the maximum size and the expected size
of connected components in the graph. Here size refers to the number of vertices in the
component. Our proof is based on a branching processes technique.

Lemma 17.8: Consider a cuckoo graph with n nodes and m =(1− /theta) n / 2 edges for
some constant  /theta> 0_._

(1) With high probability the largest connected component in the cuckoo graph has
size O (log n ).
(2) The expected size of a connected component cuckoo in the graph is O (1).

Proof: We first focus on bounding the largest connected component. We observe
that parallel edges and self-loops do not increase the size of a connected component.
Thus, assuming a model in which the m edges are chosen uniformly at random with

balanced allocations and cuckoo hashing
no parallel edges or self-loops can only increase the probability of having a large
connected component in the graph. This random graph model was introduced in
Section5.6as the Gn , N model. In our case the number of edges is N = m , and we refer
to graphs with m uniformly chosen edges as being chosen from Gn , m.
Our second observation transforms the analysis to the related random graph model
Gn , p , which we recall from Section5.6consists of graphs on n nodes with each of
the possible

( n
2
)
edges included in the graph independently with probability p .We
recall that having a connected component of size at least k for any value of k is a
monotone increasing graph property; if a graph G ( V , E ) has that property, then any
graph G ′=( V , E ′) with E ⊆ E ′also has that property.
Since having a connected component of a given size is a monotone increasing graph
property, we can use Lemma5.14. In particular, for any 0< /theta′<1, Lemma5.14allows
us to conclude that the probability that a graph drawn from Gn , m has a connected com-
ponent of size at least k is within e− O ( m )of the probability that a graph drawn from Gn , p
has a connected component of that size, where

p =(1+ /theta′)
m
( n
2
)=
(1+ /theta′)(1− /theta)
n − 1
=
1 −γ
n − 1
.
This holds for any constantγwith 0<γ < /theta, by choosing a suitably small /theta′. Thus,
our problem is reduced to bounding the maximum size of a connected component in a
graph drawn from Gn , p , with p =(1−γ)/ n.
Fix a vertexv. We explore the connected component containing vertexvby execut-
ing a breadth first search fromv. We start by placing nodevin a queue and look at
the neighbors ofv. We add these neighbors into a queue and look at the neighbors of
these neighbors, adding any new vertices to the queue, and so on. More formally, after
adding all the nodes at distance nfrom the rootvto the queue, we sequentially look at
the neighbors of each of these nodes, adding to the queue neighbors at distance n+ 1
from the root that are not yet in the queue. The process ends when there are no new
neighbors to add to the queue. Clearly, when the process ends, the queue stores all the
nodes in the connected component that includesv. Letv=v 1 ,v 2 ,...,v k be the nodes
in the queue at the termination of the process, in the order in which they entered the
queue.
Let Zi be the number of nodes added to the queue while looking at neighbors of
v i , i.e., Zi counts the neighbors ofv i that are not neighbors of any nodev j , j < i. The
key point in the analysis is that conditioning on the neighborhoods ofv 1 ,...,v i − 1 , the
distribution of Zi is stochastically dominated by a binomial random variable distributed
B ( n − 1 ,(1−γ)/( n −1)).

Definition 17.1: A random variable X stochastically dominates a random variable Y
if for all a,

Pr( X ≥ a )≥Pr( Y ≥ a ).
Equivalently, X stochastically dominates Y if for all a

FX ( a )≤ FY ( a ),
where FXand FYare the cumulative distribution functions of X and Y, respectively.

17.4 cuckoo hashing
To show this, we consider first the distribution of Z 1 , the number of neighbors of
nodev. There are n −1 other nodes, each connected tovwith probability p , thus, Z 1
is distributed B ( n − 1 ,(1−γ)/( n −1)). Consider now the distribution of Zi , i >1,
where Zi counts the neighbors ofv i that are not neighbors of any nodev j , j < i. Con-
ditioned on the nodes that were already discovered by the breath first search process,
there are no more than n − i possible new nodes connected tov i , and each of them is
connected tov i with probability p independent of any other edges. Thus, the distribu-
tion of Zi , conditioned on the values of Z 1 ,..., Zi − 1 , is stochastically dominated by a
random variable Bi with distribution B ( n − 1 ,(1−γ)/( n −1)).
We bound the probability that our breadth first search found a component of size k.
The breadth first search would have stopped with fewer than k vertices if

∑ k −^1
j = 1
Zi < k − 1 ,
because then we would have found fewer than k −1 additional vertices in exploring
the first k −1 vertices. So we must have

∑ k −^1
j = 1
Zi ≥ k − 1
for the breadth first search to reach k vertices. From our domination argument, the
probability that our breadth first search reaches k vertices is bounded above by

Pr
⎛
⎝
∑ k −^1
j = 1
Zi ≥ k − 1
⎞
⎠≤Pr
⎛
⎝
∑ k −^1
j = 1
Bi ≥ k − 1
⎞
⎠
=Pr( B (( k −1)( n −1),(1−γ)/( n −1))≥ k −1).
Here we have used that the sum of binomials is itself binomial. We are now ready
to apply the standard Chernoff bound (4.2). Let S be a binomial B (( k −1)( n −1),
(1−γ)/( n −1))ofmean E [ S ]=(1−γ)( k −1). Then

Pr( S ≥ k −1)=Pr
(
S ≥
E [ S ]
1 −γ
)
≤Pr( S ≥ E [ S ](1+γ))
≤e−( k −1)(1−γ)γ
(^2) / 3
.
Here we have used that 1/(1−γ)>(1+γ). Setting k ≥ 1 +γ (^2) (1^9 −γ)ln n ,wehave
that the probability thatv 1 is part of a connected component of size at least k is
bounded above by 1/ n^3 , and by a union bound the probability that any vertex is part
of a connected component of size at least k is bounded above by 1/ n^2. Now applying
Lemma5.14, we can conclude that in the cuckoo graph with n nodes and m edges, the
probability that there is connected component of size at least k is bounded above by
1 / n^2 +e− O ( m )≤ 2 / n^2 for large enough n.

balanced allocations and cuckoo hashing
Next we bound the expected size of a connected component that includes a given
nodev. Consider first a graph chosen from Gn , p with p =(1−γ)/( n −1). Let X the
size of the component that includes vertexvin that graph. As we have seen, for a graph
chosen from Gn , p , we can view the breadth first search process as a branching process
where the number of offspring of nodev i is Zi , which is stochastically dominated by
a random variable Bi distributed as B ( n − 1 ,(1−γ)/( n −1)) and with expectation
1 −γ. As we showed in Section 2.3, a branching process where the expected number
of offspring of a node is bounded above by 1−γhas an expected size of 1/γ. Hence,
in Gn , p , E [ X ]≤ 1 /γ.
Let Y be the size of the connected component that includesvin a graph chosen from
Gn , m. Then, for anyv,

E [ Y ]=
∑ n
i = 1
Pr( Y ≥ k )≤
∑ n
k = 1
Pr( X ≥ k )+ n e− O ( m )≤
1
γ
+ n e− O ( m )= O (1),
where in the first inequality we applied Lemma5.14and in the second inequality we
used the bound on E [ X ].  /theta

Next we need to show that all connected components with more than one node in
the cuckoo graph are either trees or have a single cycle.

Lemma 17.9: Consider a cuckoo hashing graph with n nodes and m =(1− /theta) n / 2
edges. For any constant  /theta> 0 , with high probability all the connected components in
the graph are either single vertices, trees, or unicyclic.

Proof: For the proof, we need a bound on the number of ways k vertices can be a
connected by a tree. We make use of the following combinatorial fact.

Lemma 17.10 [ Cayley’s Formula ] : The number of distinct labeled trees on k vertices
is kk −^2_._

Here a labeled tree on k vertices is one where each vertex is given a distinct num-
ber from 1 to k , and trees that are isomorphic when taking into account the labels are
considered the same. Hence there is one labeled tree on two vertices with one edge
between them – there are two ways of labeling the vertices, but they are isomorphic.
Similarly, there are three labeled trees on three vertices, with one tree for each assign-
ment of a number to the vertex of degree 2. There are many proofs to Cayley’s formula;
one approach is given in Exercise17.15.
A connected component that is not a tree or has more than one cycle must include
a tree plus at least two additional edges. Let Yk be a random variable denoting the
number of components with k vertices and at least k +1 edges. We determine a bound
on E [ Yk ] to bound the probability of the existence of such a component. We need only
worry about values of k where k = O (log n ) since we already proved that with high
probability the graph has no larger connected components.
Given a set of k vertices that form a component, the k vertices must be connected by
a tree. Suppose we choose a tree of k −1 edges connecting those vertices. We require

17.4 cuckoo hashing
all of the edges in the tree to be part of the graph, and because we allow self-loops
and multi-edges, each of the m =(1− /theta) n /2 possible random edges will be a given
specific edge of the tree with probability 2/ n^2. We then must have at least two additional
edges within that component. The two additional edges fall within the component with
probability k^2 / n^2. Finally, all the k ( n − k ) edges between vertices in the component
and vertices not in the component must not be in the graph, or we would not have a
component of size k. The following expression overcounts the number of components
somewhat, as the same component may be counted multiple times.

E [ Yk ]≤

(
n
k
)
kk −^2
(
m
k + 1
)(
k + 1
2
)
( k −1)!
(
2
n^2
) k − 1 ⎛
⎝
(
k
2
)
( n
2
)
⎞
⎠
(^2) (
1 −
2 k ( n − k )
n^2
) m − k − 1
.
That is, we first choose k vertices from the n vertices, we choose one of the kk −^2 trees
to connect these vertices, and we choose k +1 of the m edges to form this tree and add
two additional edges to the component, so there is more than one cycle.
E [ Yk ]≤

(
n
k
)
kk −^2
(
m
k + 1
)(
k + 1
2
)
( k −1)!
(
2
n^2
) k − 1 ⎛
⎝
(
k
2
)
( n
2
)
⎞
⎠
(^2) (
1 −
2 k ( n − k )
n^2
) m − k − 1

≤
nkmk +^1
2 k!
kk −^2
(
2
n^2
) k − 1 (
k^2
n^2
) 2
e−^2 k ( n − k )( m − k −1)/ n
2
≤
1
n
(1− /theta) k +^1
k^2 e k
8
e−^2 k ( n − k )( m − k −1)/ n
2
≤
1
n
(1− /theta) k +^1
k^2
8
e( kn
(^2) − 2 k ( n − k )( m − k −1))/ n 2

≤
k^2
8 n
(1− /theta) k e( kn
(^2) − 2 knm )/ n 2
e^4 k
(^2) / n

≤
k^2
8 n
(1− /theta) k e k  /thetae^4 k
(^2) / n

≤
k^2
8 n
e k ( /theta+ln(1− /theta))e^4 k
(^2) / n
.
To reach the second line in the equations above we have used that
( n
k

)
< nk / k! and
1 − x ≤e− x ; to reach the third line we have used kk / k !≤e k. Because, from our pre-
vious argument, we can assume that k = O (log n ), the final term e^4 k^2 / n in the final line
can be bounded above by 2 for large enough n. The key term in the final line is the
 /theta+ln(1− /theta), which is negative, as can be seen using the expansion ln(1− /theta)=
−

∑∞
i = 1  /theta
i / i ; the term /theta+ln(1− /theta) is therefore− /eta( /theta (^2) )as /thetagoes to 0. The final
expression therefore includes a term of the form e− /eta( k  /theta^2 )that is geometrically decreas-
ing in k. It follows that for any z = O (log n ),
∑ z
k = 1 E [ Yk ]is O (1/ n ), and hence the
probability that any component contains more than one cycle is O (1/ n ). We can con-
clude that cuckoo hashing successfully places every item with high probability.  /theta

balanced allocations and cuckoo hashing
One might wonder if we could do better. However, it is also easy to check that a cycle
component occurs with probability m(1/ n ); for example, there is an m(1/ n ) probabil-
ity that two items both choose the same bin for both its choices, or that three items
choose the same distinct pair of bins. We consider ways one might improve this failure
probability in Section17.5.
Similarly, one might wonder if we could handle loads larger than 1/2, or if the 1/ 2
is just an outcome of our analysis. In fact, for cuckoo hashing as we have described
it, 1/2 is the limit. With m =(1+ /theta)/2 edges, the cuckoo graph looks very different;
a constant fraction of the vertices become joined in a giant component of size m( n ),
and many of the vertices lie on cycles. We have seen similar threshold behaviors in
random graphs before in Section 6.5.1; the threshold here corresponds directly to the
load that can be handled by cuckoo hashing. However, higher loads are possible for
more complex variations of cuckoo hashing, as we describe in Section17.5.
Finally, it is worth mentioning that the analysis using Cayley’s formula that we used
to bound the number of components with two or more cycles could also be applied to
bound the expected number of components of each size. There are some subtleties in
the random graph model we have used here, but in Exercise17.15, we show how to use
this method rather than the branching process method to give an alternative proof that
the largest component size is O (log n ) in the Gn , p random graph model.

17.5 Extending Cuckoo Hashing
17.5.1 Cuckoo Hashing with Deletions
It is worth noting that our analysis of cuckoo hashing depended only on the proper-
ties of the corresponding random graph. Because of this, our analysis for random hash
functions holds even if we delete items, as long as the deletion process is also random,
so that which items are deleted does not depend on the outcomes from the hash func-
tions. For example, if items have lifetimes governed by a random distribution, then as
long as there are always at most m < n (1− /theta)/2 elements in the table, the probability
we will fail on any specific configuration is only O (1/ n ).
Indeed, we can actually show something slightly stronger; the addition of a new
element causes a failure with probability only O (1/ n^2 ). This means we can delete and
insert items for at least roughly a quadratic number of steps before a failure occurs. To
see this, if we have m random edges in our cuckoo graph, we can bound the probability
that a new item leads to the introduction of a double cycle. Suppose the new items cre-
ates a double cycle on exactly k vertices, where as before we can take k to be O (log n ),
as larger components are sufficiently unlikely that we can ignore them in our analysis.
For this to happen, there must already be k edges among these k vertices. There are

( n
k
)
ways of choosing these vertices, and

( m
k
)
ways of choosing the items that correspond
to the edges. After adding the new edge for the inserted item, the k +1 edges must
form a spanning tree, as well as two additional edges. Finally, there can be no other
edges among the k vertices, or between those k vertices and the other n − k vertices.
Following the same analysis as we have used previously, ifEis the event that the new

17.5extending cuckoo hashing
vertex introduces a failure, we have

Pr(E)≤
∑
k
(
n
k
)(
m
k
)
kk −^2
(
k + 1
2
)
( k −1)!
(
2
n^2
) k − 1 (
k^2
n^2
) 2
×
(
1 −
k^2 + 2 k ( n − k )
n^2
) m − k − 1
≤
∑
k
k^2 ( k +1)
4 n^2
kk
k!
(
2 m
n
) k
e−(2 k ( n − k )+ k
(^2) )( m − k +1)/ n 2

≤
∑
k
k^2 ( k +1)
4 n^2
e k (1− /theta) k e−(2 k ( n − k )+ k
(^2) )( m − k +1)/ n 2

≤
∑
k
k^2 ( k +1)
4 n^2
e k (1− /theta) k e( kn
(^2) − 2 knm )/ n 2
e^4 k
(^2) / n

≤
∑
k
k^2 ( k +1)
4 n^2
(1− /theta) k e k  /thetae^4 k
(^2) / n

≤
∑
k
k^2 ( k +1)
4 n^2
e k ( /theta+ln(1− /theta))e^4 k
(^2) / n
.
Again, we need only consider k = O (log n ). The exponential term grows like e− /eta( /theta^2 k ),
which gives that Pr(E)is O (1/ n^2 ).
Of course, deletion of an item in a cuckoo hash table, like insertion, only takes con-
stant time.

17.5.2. Handling Failures
We have shown the failure probability of cuckoo hashing is O (1/ n ) when inserting m
items into n > 2 m bins. Unfortunately, that result is easily shown to be tight. As a first
step, consider the following mode of failure: we find three balls with the same choices
for the two bins. (To ease the calculations, we suppose that all three balls land in two
distinct bins – no self-loops among the three.) The expected number of “triples” of balls
with this property is
(
m
3

)(
1 −
1
n
)(
2
n^2
) 2
.
This is because there are

( m
3
)
ways to choose the three balls. With probability
(
1 −^1 n
)
the
first ball did not choose a self-loop; the other two balls then each choose the same pair
of bins with probability 2/ n^2. We easily observe that when m = m( n ) this expectation
is m(1/ n ). A calculation of the variance readily yields the probability that there is such
a triple is also m(1/ n ), using the second moment method.
While the probability of failure for a cuckoo hash table is o (1), the fact that it is
 m(1/ n ) remains concerning; this could be very high for many practical situations. One
way to cope with this problem is to allow rehashing. If we ever reach a failure point,

balanced allocations and cuckoo hashing
where either we find that we can’t place an item because of cycles, or we simply find
that a component is too large (over c log n for a suitable constant c ), then we can choose
a new hash function and rehash all the items into a new cuckoo hash table. A question
is how much impact will rehashing have.
The amount of work to rehash using a new hash function for all items is O ( n ), and we
only have to do it with probability O (1/ n ). Even if we have to rehash multiple times
before we reach success, the expected work to hash m items can be bounded. Using
order notation rather loosely, we find the total number of operations is

O ( n )+
∑∞
k = 1
k · O ( n )·( O (1/ n )) k = O ( n ).
Hence the amortized amount of work per item due to rehashing is only constant in
expectation, and also with high probability, since the probability of rehashing k or more
times is O (1/ nk ). However, one can imagine that rehashing might not be a suitable
solution in some practical settings, because it would be undesirable for the system to
have to wait for a complete rehashing of the hash table.
An alternative approach to rehashing that generally does quite well is to set aside a
small amount of memory for a stash. If an item cannot be placed because it creates a
component with more than one cycle, it can be placed in the stash. Usually, the stash
will be empty; however, when it is not empty, it will need to be checked on every lookup.
(Further, if items are deleted, one should check whether an item in the stash can then
be put back into the cuckoo hash table.) We have seen that the use of the stash should
be rare, since the failure probability is only O (1/ n ). Extending the previous analysis,
one can show that failures behave “nearly independently”; the probability that j items
need to be held in a stash falls like O (1/ nj ). Hence, even a very small stash, such as one
that can hold four items, can greatly reduce the failure probability. The use of stashes
is considered further in Exercise17.17.

17.5.3. More Choices and Bigger Bins
There are many ways to vary or enhance cuckoo hashing. One way, as we have seen,
is by using a stash. One can also slightly improve the performance using the following
method: split the table into two subtables and make one choice randomly from each
subtable for each item; when inserting an item, just place the new element in the first
subtable, kicking out the element already there if needed; and finally, optimize the sub-
table sizes by making the first slightly larger, since now it will tend to hold more items.
The slight asymmetry that arises from favoring the first subtable, both in placement
and in size, can yield a small improvement in behavior, but the additional complexity
may not be worthwhile for many applications.
However, more important variations of cuckoo hashing allow more than one item per
bin, more than two choices per item, or both. Both of these approaches can significantly
enhance the load that can be achieved, as well as reduce the probability of failure, at
the expense of more complications in the insertion and lookup processes.

17.5extending cuckoo hashing
If each item has two choices but we allow b >1 items per bin, then we continue
to have a random graph problem, but now the question is whether cuckoo hashing
can effectively find an orientation with at most b edges pointing away from a vertex.
Allowing more than one item per bin can be very natural; for example, a bin may
correspond to a fixed amount of memory, such as a cache line, that might correspond
to the size of multiple items. One issue is how to choose which item to kick out of a
bin when it is necessary to place an item into a full bin. Natural possibilities include
breadth first search, or a “random walk” style search where at each step a random item
is selected from the bin to be kicked out to make room for the existing element.
If each item has d >2 choices but there is just one item per bin, then our problem
involves random hypergraphs, rather than random graphs, where each edge is a col-
lection of d vertices. When all choices for an item lead to a bin that already contains
an item, we again face the issue of how to choose which item to kick out. One could
again use approaches based on a breadth first search, or a “random walk” style search.
A further variation is to allow different items differing number of choices, according to
some distribution, where the number of choices is itself determined by a hash function
on the item.
Of course, one can also combine more than two choices per item and more than one
item per bin. With four choices and one item per bin, the maximum load that can be
achieved with no failures with high probability (as n grows large) is over 0.97, much
more than the 0.5 bound for two choices. Similarly, two choices with up to four items
per bin allow loads over 0.98. Combining multiple choices with multiple items per bin
yields even higher load factors.
The following theorem provides the form of the load threshold as the number of bin
choices and the number of items per bin varies. Its proof is quite complex and beyond
the scope of this book.

Theorem 17.11: Consider a cuckoo hash table with n items, m / n bins that each can
hold up to n items, and k choices per item. We consider a regime when n / m is held
fixed, but n , m →∞ .Let β( c ) denote the largest value of β so that

1
k
β
(Pr( Po (β)≥ n)) k −^1
= c ,
where Po ( x ) refers to a discrete Poisson random variable with mean x. Define ck , n to
be the unique value of c that satisfies

β( c )·Pr[ Po (β( c ))≥ n]
k ·Pr[ Po (β( c ))≥ n+1]
= n.
The following results hold for any constant values k ≥ 3 and n≥ 1 , or for k = 2 and
constant n≥ 2_. For every_  /theta> 0 , for large enough n, we have that if n / m < ck , n− /theta ,
there is a way of placing the items in the hash table that respects their choices and
the limits on the number of items per bin with probability 1 − o (1) .Ifn / m > ck , n+ /theta ,
then there is no way to place the items that respects their choices and the limits on the
number of items per bin with probability 1 − o (1).

balanced allocations and cuckoo hashing
17.6. Exercises
Exercise 17.1: (a) For Theorems17.1and17.4, the statement of the proof is for the
case that ties are broken randomly. Argue informally that, if the bins are numbered from
1to n and if ties are broken in favor of the lower-numbered bin, then the theorems still
hold.
(b) Argue informally that the theorems apply to any tie-breaking mechanism that
has no knowledge of the bin choices made by balls that have not yet been placed.

Exercise 17.2: Consider the following variant of the balanced allocation paradigm:
n balls are placed sequentially in n bins, with the bins labeled from 0 to n −1. Each
ball chooses a bin i uniformly at random, and the ball is placed in the least loaded of
bins i , i +1mod n , i +2mod n ,..., i + d −1mod n. Argue that, when d is a constant,
the maximum load grows as /eta(ln n /ln ln n ). That is, the balanced allocation paradigm
does not yield an O (ln ln n ) result in this case.

Exercise 17.3: Explain why, with 2-way chaining, the expected time to insert an item
and to search for an item in a hash table of size n with n items is O (1). Consider two
cases: the search is for an item that is in the table; and the search is for an item that is
not in the table.

Exercise 17.4: Consider the following variant of the balanced allocation paradigm:
n balls are placed sequentially in n bins. Each ball comes with d choices, chosen
independently and uniformly at random from the n bins. When a ball is placed, we
are also allowed to move balls among these d bins to equalize their load as much as
possible. Show that the maximum load is still at least ln ln n /ln d − O (1) with proba-
bility 1− o (1/ n ) in this case.

Exercise 17.5: Suppose that in the balanced allocation setup there are n bins, but the
bins are not chosen uniformly at random. Instead, the bins have two types: 1/3 of the
bins are type A and 2/3 of the bins are type B. When a bin is chosen at random, each
of the type-A bins is chosen with probability 2/ n and each of the type-B bins is chosen
with probability 1/ 2 n. Prove that the maximum load of any bin when each ball has d
bin choices is still at most ln ln n /ln d + O (1).

Exercise 17.6: Consider a parallel version of the balanced allocation paradigm in
which we have n / k rounds, where k new balls arrive in each round. Each ball is placed
in the least loaded of its d choices, where in this setting the load of each bin is the load
at the end of the previous round. Ties are broken randomly. Note that the k new balls
cannot affect each other’s placement. Give an upper bound on the maximum load as a
function of n , d , and k.

Exercise 17.7: We have shown that sequentially throwing n balls into n bins randomly,
using two bin choices for each ball, yields a maximum load of ln ln n /ln 2+ O (1)

17.6exercises
with high probability. Suppose that, instead of placing the balls sequentially, we had
access to all of the 2 n choices for the n balls, and suppose we wanted to place each ball
into one of its choices while minimizing the maximum load. In this setting, with high
probability, we can obtain a maximum load that is constant.
Write a program to explore this scenario. Your program should take as input a para-
meter k and implement the following greedy algorithm. At each step, some subset of the
balls are active; initially, all balls are active. Repeatedly find a bin that has at least one
but no more than k active balls that have chosen it, assign these active balls to that bin,
and then remove these balls from the set of active balls. The process stops either when
there are no active balls remaining or when there is no suitable bin. If the algorithm
stops with no active balls remaining, then every bin is assigned no more than k balls.
Try running your program with 10,000 balls and 10,000 bins. What is the smallest
value of k for which the program terminates with no active balls remaining at least four
out of five times? If your program is fast enough, try experimenting with more trials.
Also, if your program is fast enough, try answering the same question for 100,000 balls
and 100,000 bins.

Exercise 17.8: The following problem models a simple distributed system where
agents contend for resources and back off in the face of contention. As in Exercise5.12,
balls represent agents and bins represent resources.
The system evolves over rounds. In the first part of every round, balls are thrown
independently and uniformly at random into n bins. In the second part of each round,
each bin in which at least one ball has landed in that round serves exactly one ball from
that round. The remaining balls are thrown again in the next round. We begin with n
balls in the first round, and we finish when every ball is served.

(a) Show that, with probability 1− o (1/ n ), this approach takes at most log 2 log 2 n +
O (1) rounds. ( Hint: Let bk be the number of balls left after k rounds; show that
bk + 1 ≤ c ( bk )^2 / n , for a suitable constant c with high probability, as long as bk + 1 is
sufficiently large.)
(b) Suppose that we modify the system so that a bin accepts a ball in a round if and only
if that ball was the only ball to request that bin in that round. Show that, again with
probability 1− o (1/ n ), this approach takes at most log 2 log 2 n + O (1) rounds.

Exercise 17.9: The natural way to simulate experiments with balls and bins is to create
an array that stores the load at each bin. To simulate 1,000,000 balls being placed into
1,000,000 bins would require an array of 1,000,000 counters. An alternative approach
is to keep an array that records in the j th cell the number of bins with load j. Explain
how this could be used to simulate placing 1,000,000 balls into 1,000,000 bins using
the standard balls-and-bins paradigm and the balanced allocation paradigm with much
less space.

Exercise 17.10: Write a program to compare the performance of the standard balls-
and-bins paradigm and the balanced allocation paradigm. Run simulations placing n
balls into n bins, with each ball having d =1, 2, 3, and 4 random choices. You should

balanced allocations and cuckoo hashing
try n = 10 ,000, n = 100 ,000, and n = 1 , 000 ,000. Repeat each experiment at least 100
times and compute the expectation and variance of the maximum load for each value
of d based on your trials. You may wish to use the idea of Exercise17.9.

Exercise 17.11: Write a simulation showing how the balanced allocation paradigm
can improve performance for distributed queueing systems. Consider a bank of n FIFO
queues with a Poisson arrival stream of customers to the entire bank of rateλ n per sec-
ond, whereλ<1. Upon entry a customer chooses a queue for service, and the service
time for each customer is exponentially distributed with mean 1 second. You should
compare two settings: (i) where each customer chooses a queue independently and uni-
formly at random from the n queues for service; and (ii) where each customer chooses
two queues independently and uniformly at random from the n queues and waits at
the queue with fewer customers, breaking ties randomly. Notice that the first setting is
equivalent to having a bank of nM / M /1 FIFO queues, each with Poisson arrivals of rate
λ<1 per second. You may find the discussion in Exercise8.27helpful in constructing
your simulation.
Your simulation should run for t seconds, and it should return the average (over all
customers that have completed service) of the time spent in the system as well as the
average (over all customers that have arrived) of the number of customers found waiting
in the queue they selected for service. You should present results for your simulations
for n =100 and for t = 10 ,000 seconds, withλ= 0. 5 , 0. 8 , 0 .9, and 0.99.

Exercise 17.12: Write a program to compare the performance of the following vari-
ation of the standard balls-and-bins paradigm and the balanced allocation paradigm.
Initially n points are placed uniformly at random on the boundary of a circle of circum-
ference 1. These n points divide the circle into n arcs, which correspond to bins. We
now place n balls into the bins as follows: each ball chooses d points on the boundary of
the circle, uniformly at random. These d points correspond to the arcs (or, equivalently,
bins) that they lie on. The ball is placed in the least loaded of the d bins, breaking ties
in favor of the smallest arc.
Run simulations placing n balls into n bins for the cases d =1 and d =2. You
should try n = 1 ,000, n = 10 ,000, and n = 100 ,000. Repeat each experiment at least
100 times; for each run, the n initial points should be re-chosen. Give a chart showing
the number of times the maximum load was k , based on your trials for each value of d.
You may note that some arcs are much larger than others, and therefore when d = 1
the maximum load can be rather high. Also, to find which bin each ball is placed in
may require implementing a binary search or some other additional data structure to
quickly map points on the circle boundary to the appropriate bin.

Exercise 17.13: There is a small but interesting improvement that can be made to the
balanced allocation scheme we have described. Again we will place n balls into n bins.
We assume here than n is even. Suppose that we divide the n bins into two groups of
size n /2. We call the two groups the left group and the right group. For each ball, we
independently choose one bin uniformly at random from the left and one bin uniformly
at random from the right. We put the ball in the least loaded bin, but if there is a tie we

17.6exercises
always put the ball in the bin from the left group. With this scheme, the maximum load
is reduced to ln ln n /2lnφ+ O (1), whereφ=

(
1 +
√
5
)
/2 is the golden ratio. This
improves the result of Theorem17.1by a constant factor. (Note the two changes to our
original scheme: the bins are split into two groups, and ties are broken in a consistent
way; both changes are necessary to obtain the improvement we describe.)

(a) Write a program to compare the performance of the original balanced allocation
paradigm with this variation. Run simulations placing n balls into n bins, with
each ball having d =2 choices. You should try n = 10 ,000, n = 100 ,000, and n =
1 , 000 ,000. Repeat each experiment at least 100 times and compute the expectation
and variance of the maximum load based on your trials. Describe the extent of the
improvement of the new variation.
(b) Adapt Theorem17.1to prove this result. The key idea in how the theorem’s proof
must change is that we now require two sequences,β i andγ i. Similar to Theo-
rem17.1,β i represents a desired upper bound on the number of bins on the left
with load at least i , andγ i is a desired upper bound on the number of bins on the
right with load at least i. Argue that choosing

β i + 1 =
c 1 β i γ i
n^2
and γ i + 1 =
c 2 β i + 1 γ i
n^2
for some constants c 1 and c 2 is suitable (as long asβ i andγ i are large enough that
Chernoff bounds may apply).
Now let Fk be the k th Fibonacci number. Apply induction to show that, for suf-
ficiently large i ,β i ≤ nc 3 cF 42 i andγ i ≤ nc 3 cF 42 i +^1 for some constants c 3 and c 4. Fol-
lowing Theorem17.1, use this to prove the ln ln n /2lnφ+ O (1) upper bound.
(c) This variation can easily be extended to the case of d >2 choices by splitting the
n bins into d ordered groups, choosing one bin uniformly at random from each
group, and breaking ties in favor of the group that comes first in the ordering.
Suggest what would be the appropriate upper bound on the maximum load for this
case, and give an argument backing your suggestion. (You need not give a complete
formal proof.)
Exercise 17.14: The birthday paradox (discussed in Section 5.1) shows that if balls
are sequentially thrown randomly into n bins, with constant probability there will be a
collision after /eta(

√
n ) balls are thrown.
(a) Suppose that balls are placed sequentially, each ball has two choices of where to be
placed, and a ball will choose a bin that avoids a collision if that is possible. Show
that there are constants c 1 and c 2 so that after c 1 n^2 /^3 − o ( n^2 /^3 ) balls are thrown no
collision has occurred with probability at least 1/2, and after c 2 n^2 /^3 + o ( n^2 /^3 ) balls
are thrown at least one collision has occurred with probability at least 1/2.
(b) How close can you make the constants c 1 and c 2?
(c) Extend your analyis for more than two choices. Specifically, show that if each ball
has k choices for some constant k , there are constants c 1 , k and c 2 , k so that after
c 1 , kn^1 −^1 / k − o ( n^1 −^1 / k ) balls are thrown no collision has occurred with probability

balanced allocations and cuckoo hashing
at least 1/2, and after c 2 , kn^1 −^1 / k + o ( n^1 −^1 / k ) balls are thrown at least one collision
has occurred with probability at least 1/2.
(d) How close can you make the constants c 1 , k and c 2 , k?

Exercise 17.15: In our analysis for cuckoo hash tables we showed that the largest com-
ponent size was O (log n ) with high probability. Here we provide part of an alternative
proof of this result in the Gn , p model, using an analysis that makes use of Cayley’s
formula. Consider a random graph G chosen from Gn , p , with p = c / n for a constant
c <1.

(a) Let Xk be the expected number of tree components on exactly k vertices for a ran-
dom graph from Gn , p with p = c / n for a constant c <1. A tree component on k
vertices will be connected with k −1 edges, and will have no edges to the other
n − k vertices. Show that
E [ Xk ]=
(
n
k
)
kk −^2
(
c
n
) k − (^1) (
1 −
c
n
) kn − k ( k +3)/ 2 + 1
.
(b) Show that for 1≤ k ≤

√
n ,
E [ Xk ]≤ C
n
ck^2
e(1− c +ln c ) k
for some constant C for large enough n.
(c) Using the expression for E [ Xk ], show that
E [ Xk + 1 ]
E [ Xk ]
=( n − k )
(
1 +
1
k
) k − 1
c
n
(
1 −
c
n
) n − k − 2
,
and in turn
E [ Xk + 1 ]
E [ Xk ]
≤
(
1 −
k
n
)
c e^1 − c (1− k / n )
(
1 −
c
n
)− 2
.
(d) Show that x e^1 − x ≤1for x >0, and conclude that

E [ Xk + 1 ]
E [ Xk ]
≤
(
1 −
c
n
)− 2
.
(e) Using the above, argue that the probability that there is any tree component with
more than
√
n vertices in G is o (1/ n ), and that therefore the maximum size of a
tree component of G is O (log n ) with probability 1− o (1/ n ).
Exercise 17.16: Complete the argument from Section17.5.2to show that the failure
probability for standard cuckoo hashing is at least m(1/ n ).

Exercise 17.17: Write code to implement the following experiment. You will build a
cuckoo hash table using two choices per item and one item per bin, with an array of
size 2^20 , and you will insert 514,000 items into it. (This is a bit more than than 49%
of 2^20 .) You have to decide how many moves you will allow before deciding a failure
has occurred; 200 should be sufficient. If during the insertion process an item cannot
be inserted, place the item in a stash and continue inserting the remaining items.

17.6exercises
Perform 100,000 trials. How often would you need a stash to hold an item? How
often would a stash that can hold one item suffice? Two items?

Exercise 17.18: We show here one way to derive Cayley’s formula. A directed rooted
tree is a tree with a special root vertex, and all the edges in the tree are assigned a
direction, with all edges directed away from the root. We count the number of sequences
of directed edges that can lead to a directed rooted tree in two different ways, and use
it to calculate an expression for T ( k ), the number of distinct labeled trees on k vertices.

(a) We create an ordered triple as follows. We first choose a labeled but undirected
tree. We next choose a vertex as a root, and now we can think of the tree as being a
directed rooted tree. Finally, we choose one of the ( k −1)! possible permutations
of the directed edges. We can think of our choices of labeled tree, root vertex, and
edge permutation as an ordered triple.
Show that there is a one-to-one correspondence between these ordered triples
and sequences of directed edges on k vertices that lead to a directed rooted tree.
Explain why this shows that the number of sequences of directed edges that can
lead to a directed rooted tree on k vertices is ( k !)· T ( k ).
(b) Now suppose instead we start with an empty graph, where we think of each vertex
as initially its own rooted tree (with no edges), and add directed edges one at a
time. At each step we will have a forest of directed edges. After nsteps the forest
will have k − nroots, so that after k −1 edges are added, we will have a directed
rooted tree. At each step we choose an edge to add by first choosing any of the
k vertices in the graph. This vertex will be in one of the trees in the forest. We
then choose a root from another tree to connect to, with the edge directed from
the first vertex to the second. This removes one of the roots from consideration,
so each step reduces the number of roots by one. Show that there is a one-to-one
correspondence between sequences of directed edges that lead to a directed rooted
tree and the sequences of edges that can be chosen in this manner, and show that
there are kk −^1 ( k −1)! ways of choosing the sequences of edges as above.
(c) Argue from the above steps that T ( k )= kk −^2.

Exercise 17.19: Suppose we consider the effects of adding a stash that can hold a
single item, with standard cuckoo hashing using two choices and one item per bin.
In this case, we can consider two ways to fail; we might have a single component of
k vertices with at least k +2 edges, or we might have two disjoint components, one
with k 1 vertices with at least k 1 +1 edges and one with k 2 vertices with at least k 2 + 1
edges. By extending our previous analysis regarding components and edges, show that
the probability of having a failure with cuckoo hashing when using a stash that can
hold one item is O (1/ n^2 ).

Exercise 17.20: Write code to implement the following experiment. You will build a
cuckoo hash table using four choices per item and one item per bin, with an array of
size 2^20. If all choices are full, choose one of the items to kick out randomly. (You may,
if you like, optimize after the first move on an insertion by not allowing yourself to

balanced allocations and cuckoo hashing
choose to place an item in a bin that it has just been kicked out of at the last step.) You
have to decide how many moves you will allow before deciding a failure has occurred;
200 should be sufficient. Load the table until you reach an item that cannot be placed.
Record the load, or the fraction of the array that has been filled; that is, the number
of items divided by 2^20. Repeat the experiment 1000 times. What load level seems
safe with four choices? How does this compare to Theorem17.11? (Theorem17.11is
about the existence of a valid assignment, not about this placement algorithm, and is
an asymptotic result. It is therefore not necessarily expected that the experiment should
achieve the performance suggested by the theorem.)

Exercise 17.21: Modify your code above so that you can experiment with varying
numbers of choices per item and varying numbers of items per bin. For different values
of these parameters, determine (approximately) the load where the failure probability
appears to be nontrivial and compare to Theorem17.11.

Further Reading

N. Alon and J. Spencer, The Probabilistic Method, 2nd edn. Wiley, New York, 2000.
B. Bollobás, Random Graphs, 2nd edn. Academic Press, Orlando, FL, 1999.
T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein, Introduction to Algorithms, 2nd edn. MIT
Press / McGraw-Hill, Cambridge / New York, 2001.
T. M. Cover and J. A. Thomas, Elements of Information Theory, Wiley, New York, 1991.
W. Feller, An Introduction to the Probability Theory, Vol. 1, 3rd edn. Wiley, New York, 1968.
W. Feller, An Introduction to the Probability Theory, Vol. 2. Wiley, New York, 1966.
S. Har-Peled, Geometric Approximation Algorithms , AMS, Providence, RI, 2011.
M. Jerrum, Counting , Sampling and Integrating: Algorithms and Complexity (Lectures in
Mathematics. ETH Zürich), Birkhäuser, Berlin, 2003.
S. Karlin and H. M. Taylor, A First Course in Stochastic Processes, 2nd edn. Academic Press,
New York, 1975.
S. Karlin and H. M. Taylor, A Second Course in Stochastic Processes. Academic Press, New York,